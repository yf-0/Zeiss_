% Encoding: UTF-8

@Article{,
  author      = {Diederik P. Kingma and Jimmy Ba},
  title       = {Adam: A Method for Stochastic Optimization},
  abstract    = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  date        = {2014-12-22},
  eprint      = {1412.6980v9},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1412.6980v9:PDF},
  keywords    = {cs.LG},
}

@InProceedings{6180909,
  author    = {T. Blum and V. Kleeberger and C. Bichlmeier and N. Navab},
  title     = {mirracle: An augmented reality magic mirror system for anatomy education},
  booktitle = {2012 IEEE Virtual Reality Workshops (VRW)},
  year      = {2012},
  pages     = {115-116},
  month     = {March},
  abstract  = {We present an augmented reality magic mirror for teaching anatomy. The system uses a depth camera to track the pose of a user standing in front of a large display. A volume visualization of a CT dataset is augmented onto the user, creating the illusion that the user can look into his body. Using gestures, different slices from the CT and a photographic dataset can be selected for visualization. In addition, the system can show 3D models of organs, text information and images about anatomy. For interaction with this data we present a new interaction metaphor that makes use of the depth camera. The visibility of hands and body is modified based on the distance to a virtual interaction plane. This helps the user to understand the spatial relations between his body and the virtual interaction plane.},
  doi       = {10.1109/VR.2012.6180909},
  issn      = {1087-8270},
  keywords  = {augmented reality;biomedical education;cameras;computer aided instruction;computerised tomography;data visualisation;3D models;CT dataset;CT slices;anatomy education;augmented reality magic mirror system;depth camera;interaction metaphor;large display;mirracle;organs;photographic dataset;pose tracking;text information;volume visualization;Augmented reality;Cameras;Computed tomography;Education;Glass;Mirrors;Visualization;H.5.1 [Information Interfaces and Presentation]: Artificial, augmented, and virtual realities},
}

@InProceedings{7328051,
  author    = {H. Roodaki and K. Filippatos and A. Eslami and N. Navab},
  title     = {Introducing Augmented Reality to Optical Coherence Tomography in Ophthalmic Microsurgery},
  booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
  year      = {2015},
  pages     = {1-6},
  month     = {Sept},
  abstract  = {Augmented Reality (AR) in microscopic surgery has been subject of several studies in the past two decades. Nevertheless, AR has not found its way into everyday microsurgical workflows. The introduction of new surgical microscopes equipped with Optical Coherence Tomography (OCT) enables the surgeons to perform multimodal (optical and OCT) imaging in the operating room. Taking full advantage of such elaborate source of information requires sophisticated intraoperative image fusion, information extraction, guidance and visualization methods. Medical AR is a unique approach to facilitate utilization of multimodal medical imaging devices. Here we propose a novel medical AR solution to the long-known problem of determining the distance between the surgical instrument tip and the underlying tissue in ophthalmic surgery to further pave the way of AR into the surgical theater. Our method brings augmented reality to OCT for the first time by augmenting the surgeon's view of the OCT images with an estimated instrument cross-section shape and distance to the retinal surface using only information from the shadow of the instrument in intraoperative OCT images. We demonstrate the applicability of our method in retinal surgery using a phantom eye and evaluate the accuracy of the augmented information using a micromanipulator.},
  doi       = {10.1109/ISMAR.2015.15},
  keywords  = {augmented reality;data visualisation;medical image processing;micromanipulators;optical tomography;surgery;augmented reality;image fusion;information extraction;intraoperative OCT image;medical AR solution;micromanipulator;microscopic surgery;multimodal medical imaging device;ophthalmic microsurgery;ophthalmic surgery;optical coherence tomography;retinal surgery;surgical instrument tip;surgical microscope;Biomedical optical imaging;Estimation;Microscopy;Optical microscopy;Retina;Surgery;Augmented reality;instrument cross-section;ophthalmic surgery;optical coherence tomography},
}

@InProceedings{6862231,
  author    = {L. Schreiter and D. Bresolin and M. Capiluppi and J. Raczkowsky and P. Fiorini and H. Woern},
  title     = {Application of contract-based verification techniques for hybrid automata to surgical robotic systems},
  booktitle = {2014 European Control Conference (ECC)},
  year      = {2014},
  pages     = {2310-2315},
  month     = {June},
  abstract  = {Surgical robotic systems have to deliver a high quality of safety, since they deal with human lives. Their safety specifications must ensure the absence of risks for the patient and the operating room staff. To respect the modular nature of a surgical system, we propose a contract based verification approach for safety. We introduce a case study based on a typical surgical robotic operation scenario and model its components by using hybrid automata. We exploit the theory of parallel composition of contracts to verify properties on each component and prove that the property of the overall system can be obtained by composition.},
  doi       = {10.1109/ECC.2014.6862231},
  keywords  = {automata theory;medical robotics;surgery;contract based verification approach;contract parallel composition;contract-based verification techniques;hybrid automata;surgical robotic systems;Automata;Contracts;Manipulators;Robot kinematics;Safety;Target tracking;Hybrid Systems;Robotics;Safety Critical Systems},
}

@InProceedings{1196913,
  author    = {M. Aschke and C. R. Wirtz and J. Raczkowsky and H. Worn and S. Kunze},
  title     = {Augmented reality in operating microscopes for neurosurgical interventions},
  booktitle = {First International IEEE EMBS Conference on Neural Engineering, 2003. Conference Proceedings.},
  year      = {2003},
  pages     = {652-655},
  month     = {March},
  abstract  = {Currently, there are no commercial systems available which provide the surgeon with a real three dimensional, stereoscopic overlay of the operating field in an operating microscope. We introduce a concept for a stereoscopic overlay of the operating field using the optics of the operating microscope. Preoperatively achieved data are inserted into the optics of the operating microscope. The goal is to enhance the surgeon's ability for a better intraoperative orientation by giving him the three-dimensional information he needs. To achieve this we developed a concept using virtual cameras for the collimation. We also analyzed current display technologies to determine the best technology which meets our demands. Another advantage of this approach is to prevent the surgeon from the need to take his view from the operating field towards the screen of the navigation system and back. We propose a general process model, a prototype for injecting images into the operating microscope and a visualization concept. In the frame of this work a brain phantom used as trial object for our prototype is developed. The work is supported by the Deutsche Forschungsgemeinschaft (DFG), as a part of the Priority Programme 1124: "Medical Navigation and Robotics".},
  doi       = {10.1109/CNE.2003.1196913},
  keywords  = {augmented reality;medical computing;neurophysiology;optical microscopes;stereo image processing;surgery;Deutsche Forschungsgemeinschaft;augmented reality;brain phantom;collimation;display technologies;general process model;intraoperative orientation;navigation system;neurosurgical interventions;operating microscope;preoperatively achieved data;real three dimensional stereoscopic overlay;stereoscopic overlay;three-dimensional information;trial object;virtual cameras;visualization concept;Augmented reality;Biomedical optical imaging;Cameras;Displays;Navigation;Neurosurgery;Optical collimators;Optical microscopy;Prototypes;Surges},
}

@InProceedings{6181327,
  author    = {H. Mönnich and P. Nicolai and T. Beyl and J. Raczkowsky and H. Wörn},
  title     = {A supervision system for the intuitive usage of a telemanipulated surgical robotic setup},
  booktitle = {2011 IEEE International Conference on Robotics and Biomimetics},
  year      = {2011},
  pages     = {449-454},
  month     = {Dec},
  abstract  = {This paper introduces the OP:Sense system that is able to track objects and humans and. To reach this goal a complete surgical robotic system is built up that can be used for telemanipulation as well as for autonomous tasks, e.g. cutting or needle-insertion. Two KUKA lightweight robots that feature seven DOF and allow variable stiffness and damping due to an integrated impedance controller are used as actuators. The system includes two haptic input devices for providing haptic feedback in telemanipulation mode as well as including virtual fixtures to guide the surgeon even during telemanipulation mode. The supervision system consists of a marker-based optical tracking system, Photonic Mixer Device cameras (PMD) and rgb-d cameras (Microsoft Kinect). A simulation environment is constantly updated with the model of the environment, the model of the robots and tracked objects, the occupied space as well as tracked models of humans.},
  doi       = {10.1109/ROBIO.2011.6181327},
  keywords  = {cameras;haptic interfaces;medical robotics;object tracking;optical tracking;robot vision;surgery;telerobotics;KUKA lightweight robot;Microsoft Kinect;OP sense system;PMD;actuator;autonomous task;damping;haptic feedback;haptic input device;human tracking;impedance controller;marker-based optical tracking system;object tracking;photonic mixer device camera;rgb-d camera;supervision system;surgical robotic system;telemanipulated surgical robotic setup;telemanipulation mode;variable stiffness;virtual fixture;Cameras;Collision avoidance;Haptic interfaces;Humans;Robot vision systems;Surgery},
}

@InProceedings{5398519,
  author    = {H. Mönnich and J. Raczkowsky and D. Stein and H. Wörn},
  title     = {Increasing the accuracy with a rich sensor system for robotic laser osteotomy},
  booktitle = {2009 IEEE Sensors},
  year      = {2009},
  pages     = {1684-1688},
  month     = {Oct},
  abstract  = {In laser osteotomy bone is cut precisely with a laser. A robot is used to position an end-effector with a scanhead, which deflects the cutting laser. An optical tracking system (OTS) tracks the position of the bone, in our case a skull, and the end effector. The skull is registered to its model, based on a segmented CT data, as well as the robot to the OTS. With this a complete transformation chain between the model and the robot coordinate system is given. Using the transformation chain the positions, where the laser beam is emitted and where it has to incise the skull, can be computed for the robot coordinate system. The complete registration processes introduce different errors in every step. To achieve clean cuts the correct distance between scanhead and ablated bone segment is crucial. Therefore a distance sensor is attached to the end-effector, measuring the distance to the skull with an accuracy of 100 ¿¿m. The value is used to adjust the positioning to the correct distance.},
  doi       = {10.1109/ICSENS.2009.5398519},
  issn      = {1930-0395},
  keywords  = {biomedical measurement;biosensors;bone;computerised tomography;distance measurement;image registration;image segmentation;laser ablation;laser applications in medicine;laser beam cutting;medical image processing;medical robotics;optical tracking;orthopaedics;position control;remote sensing by laser beam;surgery;ablated bone segment;biosensors;bone;distance sensor;end effector;laser cutting;laser osteotomy;optical tracking system;positioning;registration processes;robotic laser osteotomy;scanhead;segmented CT data;skull;Bones;End effectors;Laser beam cutting;Laser beams;Laser modes;Optical sensors;Robot kinematics;Robot sensing systems;Sensor systems;Skull},
}

@InProceedings{343843,
  author    = {J. Zhang and J. Raczkowsky and A. Herp},
  title     = {Emulation of spline curves and its applications in robot motion control},
  booktitle = {Proceedings of 1994 IEEE 3rd International Fuzzy Systems Conference},
  year      = {1994},
  pages     = {831-836 vol.2},
  month     = {Jun},
  abstract  = {The paper presents the design of a fuzzy controller to emulate spline curves for generating smooth motion trajectories. Spline models and a fuzzy control model are compared. Based on this comparison, a fuzzy control model is devised to make the robot continuously pass through the planned subgoals guiding the robot motion. By analyzing primitive movements along subgoals, the rule base can be developed. System structures are proposed for applying this fuzzy controller in the hierarchical motion control of robot systems. Its applications in robot arms and mobile robots are also presented},
  doi       = {10.1109/FUZZY.1994.343843},
  keywords  = {fuzzy control;intelligent control;mobile robots;motion control;robot kinematics;splines (mathematics);fuzzy control model;fuzzy controller;hierarchical motion control;mobile robots;planned subgoals;primitive movements;robot arms;robot motion control;rule base;smooth motion trajectories;spline curve emulation;spline models;system structures;Control systems;Emulation;Fuzzy control;Fuzzy systems;Intelligent robots;Motion control;Robot control;Robot motion;Robot sensing systems;Spline},
}

@InProceedings{5174713,
  author    = {J. Burgner and M. Mueller and J. Raczkowsky and H. Woern},
  title     = {Robot assisted laser bone processing: Marking and cutting experiments},
  booktitle = {2009 International Conference on Advanced Robotics},
  year      = {2009},
  pages     = {1-6},
  month     = {June},
  abstract  = {Processing bony tissue using robot assisted laser ablation is a promising new method, which facilitates to manipulate bone in mum range. Developing the first setup for robot assisted laser processing of bone necessitates on the one hand suitable planning and simulation techniques. Especially planning of the ablation pattern, i.e. the placement of single laser pulses is a crucial step. On the other hand the maximal achievable accuracy performing laser cutting is in the range of the single spot size of a laser pulse. Therefore the assembly of all components and the methods applied for the execution is of high importance. We established the first setup for robot assisted laser ablation of bone. The workflow starts with the planning of the marking or cutting geometry on a surface model of the patient. Afterwards a simulation allows to verify the expected of the procedure. In order to evaluate our system we performed marking and cutting experiments, which are presented in this contribution. For the first time an osteotomy of cadaver femoral cow bone was performed using robot assisted laser ablation.},
  keywords  = {bone;cutting;laser ablation;medical robotics;ablation pattern planning;cadaver femoral cow bone;cutting experiments;laser cutting;marking experiments;osteotomy;robot assisted laser ablation;robot assisted laser bone processing;simulation technique;Bones;Computational geometry;Geometrical optics;Laser ablation;Laser beam cutting;Laser modes;Optical pulses;Process planning;Robotic assembly;Robots},
}

@InProceedings{5410620,
  author    = {D. Stein and H. Mönnich and J. Raczkowsky and H. Wörn},
  title     = {Visual servoing with an optical tracking system and a lightweight robot for laser osteotomy},
  booktitle = {2009 IEEE International Conference on Control and Automation},
  year      = {2009},
  pages     = {1896-1900},
  month     = {Dec},
  abstract  = {This paper presents a visual servoing application with a fixed camera configuration for CO2 laser osteotomy. A multi camera system from ART is used to track the position of the robot and a skull via marker spheres that are attached to both. A CT scan from the skull is performed and segmented to acquire a 3D model. Inside the model the position for the robot for the laser ablation is planned. A 60 Hz update rate for the tracking system a and a 80 HZ update rate for the robot is sufficient to reposition the robot relative to the skull for CO2 laser ablation scenarios to ablate during motion, e.g. breathing motion. Also the accuracy of the lightweight robot is increased with the additional supervision of an optical tracking system. Accuracy improvement was measured with an FARO measurement arm. A visual servoing control schema is presented. The demonstrator shows a working visual servoing application for laser.},
  doi       = {10.1109/ICCA.2009.5410620},
  issn      = {1948-3449},
  keywords  = {computerised tomography;laser ablation;medical image processing;medical robotics;visual servoing;CT scan;laser ablation;laser osteotomy;lightweight robot;multi camera system;optical tracking system;visual servoing;Cameras;Computed tomography;Laser ablation;Laser applications;Laser modes;Robot vision systems;Skull;Subspace constraints;Tracking;Visual servoing},
}

@InProceedings{5068864,
  author    = {M. Ciucci and L. A. Kahrs and J. Raczkowsky and H. Worn and M. E. Halatsch},
  title     = {The NEAR project: Active endoscopes in the operating room},
  booktitle = {2009 IEEE International Conference on Virtual Environments, Human-Computer Interfaces and Measurements Systems},
  year      = {2009},
  pages     = {47-52},
  month     = {May},
  abstract  = {The following work describes in the field of neuro endoscopy the possibilities offered by active endoscopes equipped with augmented reality features and able to perform quantitative measurements. In the NEAR project, a practical implementation targeted towards endoscopic third ventriculostomy (ETV) and realized with standard optical systems and endoscopes is used as an example to discuss the limits and the possibilities offered by computer vision and navigations techniques.},
  doi       = {10.1109/VECIMS.2009.5068864},
  issn      = {1944-9410},
  keywords  = {augmented reality;endoscopes;medical computing;NEAR project;active endoscopes;augmented reality features;endoscopic third ventriculostomy;neuroendoscopy;operating room;quantitative measurement;standard optical systems;Augmented reality;Brain modeling;Computer vision;Endoscopes;Image reconstruction;Layout;Minimally invasive surgery;Navigation;Neurosurgery;Robotics and automation;augmented reality;neuroendoscopy;quantitative endoscopy;triangulation},
}

@InProceedings{6181667,
  author    = {P. Nicolai and T. Beyl and H. Mönnich and J. Raczkowsky and H. Wörn},
  title     = {OP:Sense #x2014; An integrated rapid development environment in the context of robot assisted surgery and operation room sensing},
  booktitle = {2011 IEEE International Conference on Robotics and Biomimetics},
  year      = {2011},
  pages     = {2421-2422},
  month     = {Dec},
  abstract  = {In this video we show the capabilities of the OP:Sense system. OP:Sense is an integrated rapid application development environment for robot assisted surgery. It mainly aims on MIRS and on open head neurosurgery as OP:Sense is developed for the EU Projects FP7 SAFROS and FP7 ACTIVE that aim on these use-cases. Besides the framework, OP:Sense also integrates applications. Thus it is not only the framework itself but also a system that demonstrates how robots can be used for surgical interventions. Core of the system is the ACE TAO framework [1] [2] that implements realtime CORBA for communication between distributed systems. We built interfaces based on CORBA for use in Matlab and Simulink. Also there are modules for 3D Slicer and applications for the control of devices like robots, or surgical tools. As Matlab is a mighty tool for rapid application development it can be used to develop applications in a faster way compared to using C++ or similar programming languages. We use Matlab for setting up our environment and for tasks and computations that does not need to run in realtime. For Realtime tasks like telemanipulation we use Simulink models.},
  doi       = {10.1109/ROBIO.2011.6181667},
  keywords  = {distributed object management;manipulators;medical computing;medical robotics;surgery;telerobotics;3D slicer;ACE TAO framework;FP7 ACTIVE project;FP7 SAFROS project;Matlab;OP:Sense system;Simulink model;distributed system;integrated rapid application development environment;integrated rapid development environment;open head neurosurgery;operation room sensing;realtime CORBA;robot assisted surgery;surgical intervention;telemanipulation;Application software;Haptic interfaces;Robot kinematics;Robot sensing systems;Surgery;Three dimensional displays},
}

@InProceedings{5509881,
  author    = {G. Kane and R. Boesecke and J. Raczkowsky and H. Wörn},
  title     = {Kinematic path-following control of a mobile robot on arbitrary surface},
  booktitle = {2010 IEEE International Conference on Robotics and Automation},
  year      = {2010},
  pages     = {1562-1567},
  month     = {May},
  abstract  = {This paper outlines a method for applying a kinematic path following control of a mobile robot without any regard for surface structure. Background. A great deal of mobile robotics kinematics analysis is based on the movement of the robot on a two dimensional flat surface. Our application for precision surgery required a new approach to a system that would operate on a highly non-linear surface; this specific system was a surgical robot that would conduct craniotomies while moving over highly irregular and often deformed skulls. Methods. The approach used an abstract view of the operating environment that would totally ignore the surface, instead determining the control parameters based only on the robot and the desired cutting trajectory. The approach was then evaluated in a 3D environment using a series of predefined surfaces to determine bounding limits in the control theory. These limits were then tested in a second series of tests using real data from the CT preoperative imagery of previous patients and phantoms. The simulation results were then compared with the actual performance of the robot on phantoms and cadavers. Results. The approach has been successfully implemented on the first medical robot to position itself through spiked wheels on the surface of the skull. Testing has to date been successful in both a simulation environment, and on initial phantom and cadiever trials, with accuracies equal to that of the larger industrial modified surgical robots.},
  doi       = {10.1109/ROBOT.2010.5509881},
  issn      = {1050-4729},
  keywords  = {computerised tomography;medical robotics;mobile robots;path planning;robot kinematics;3D environment;CT preoperative imagery;arbitrary surface;control theory;craniotomies;kinematic path-following control;medical robot;mobile robotics kinematics analysis;precision surgery;spiked wheels;surgical robot;Computed tomography;Control theory;Imaging phantoms;Kinematics;Medical robotics;Medical simulation;Mobile robots;Surface structures;Surgery;Testing},
}

@InProceedings{1545553,
  author    = {H. Peters and J. Raczkowsky and H. Woern},
  title     = {Approach to an architecture for a generic computer integrated surgery system},
  booktitle = {2005 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  year      = {2005},
  pages     = {2455-2460},
  month     = {Aug},
  abstract  = {This article describes the proposal for an architecture that allows to easily connect different devices or modules to a system for computer integrated surgery (CIS). This would allow the reuse of expensive CIS devices like robots. Yet, in contrast to other surgical robot systems, this architecture does not require a robot - it just supports its use. Therefore, a robot and its attached tool are distinguished as two separate devices. All devices in this architecture have knowledge about themselves and thus hardly depend on other devices. The article motivates and describes this architecture and explains an example implementation.},
  doi       = {10.1109/IROS.2005.1545553},
  issn      = {2153-0858},
  keywords  = {medical computing;medical robotics;software architecture;surgery;generic computer integrated surgery system;medical robot;surgical instrument;surgical robot system;system architecture;Computational Intelligence Society;Computer architecture;Medical robotics;Proposals;Robots;Surgery;architecture;medical robots;surgical instruments},
}

@InProceedings{5174813,
  author    = {D. Stein and H. Monnich and J. Raczkowsky and H. Worn},
  title     = {Automatic and hand guided self-registration between a robot and an optical tracking system},
  booktitle = {2009 International Conference on Advanced Robotics},
  year      = {2009},
  pages     = {1-5},
  month     = {June},
  abstract  = {In this paper an self-registration method for a robot with an optical tracking system (OTS) is presented. The ability of the used KUKA lightweight robot allows to perform the registration in hand guided mode or completely automatically. The hand guidance allows the user to register the robot with the tracking system in a very fast and intuitive way. He can take the robot by hand and move it and the registration is performed automatically. The mathematical goal is to find a rotation and translation to compute the pose of the robot base and end effector in terms of the coordinate system of the OTS. At each position the pose of the rigid body as well as the TCP of the robot is determined. Since the origin of the rigid body usually is different from the TCP a simple point-to-point transformation as e.g. provided by Horn is not suitable. The problem was reformulated to avoid this and solved with a modified Gauss Newton algorithm.},
  keywords  = {Gaussian processes;Newton method;medical robotics;optical tracking;pose estimation;surgery;Gauss Newton algorithm;KUKA lightweight robot;automatic self-registration;hand guided self-registration;optical tracking system;pose estimation;Biomedical optical imaging;Calibration;End effectors;Fasteners;Gaussian processes;Laser surgery;Medical robotics;Robot kinematics;Robotics and automation;Skull;Surgical robotics;laser osteotomy;optical tracking;self calibration},
}

@InProceedings{6669654,
  author    = {M. Capiluppi and L. Schreiter and P. Fiorini and J. Raczkowsky and H. Woern},
  title     = {Modeling and verification of a robotic surgical system using Hybrid Input/Output Automata},
  booktitle = {2013 European Control Conference (ECC)},
  year      = {2013},
  pages     = {4238-4243},
  month     = {July},
  abstract  = {The area of robotic surgical systems has to deal with several important safety aspects to ensure that the patient and the Operating Room staff are safe. A robotic surgical system has to fulfill specific safety requirements and to ensure that the system reacts like its specification. To this end, a verification process is necessary. In this paper an architecture for robotic surgery is modeled using the framework of Hybrid Input/Output Automata (HIOAs). A case study based on a surgical robotic operation scenario is presented and modeled using HIOAs. Exploiting the modularity and compositionality theory of HIOAs, the verification of the system is performed.},
  keywords  = {automata theory;medical robotics;surgery;HIOA;compositionality theory;hybrid input-output automata;modularity theory;operating room staff;robotic surgery architecture;robotic surgical system verification;safety requirements;surgical robotic operation scenario;Automata;End effectors;Robot kinematics;Safety;Tracking;Trajectory;Hybrid I/O Automaton;Hybrid systems;Robotic Surgery;Verification},
}

@InProceedings{4347528,
  author    = {Lu Shaofang and L. A. Kahrs and M. Werner and F. B. Knapp and J. Raczkowsky and J. Schipper and M. Ivanenko and H. Worn and P. Hering and T. Klenzner},
  title     = {First Study on Laser Bone Ablation System at the Skull Base for Micro Surgery Based on Vision Navigation},
  booktitle = {2007 Chinese Control Conference},
  year      = {2007},
  pages     = {602-604},
  month     = {July},
  abstract  = {As the anatomic structures, for example bone thickness, are different and the destruction of the membrane lining the inner ear can lead to a damage of organ functions, for example deafness or vertigo, the protection of soft tissue structures behind the ablated bone in skull base surgery is mandatory. Consequently, a safer and more accurate Cochlear Implantation technology need to be developed urgently. For the detection of the boundary between soft tissue and bone the laser bone ablation system which was based on the combination of laser, robotics, coaxial monitoring and vision navigation was developed for a micro surgery at the skull base. Through this the laser is guided across the ablation area by vision navigation technologies. In this paper our laser bone ablation system and the first results of the boundary detection are described.},
  doi       = {10.1109/CHICC.2006.4347528},
  issn      = {1934-1768},
  keywords  = {bone;ear;laser applications in medicine;medical robotics;navigation;surgery;anatomic structure;coaxial monitoring;cochlear implantation technology;inner ear membrane lining;laser bone ablation system;micro surgery;organ function;robotics;skull base;soft tissue;vision navigation technology;Biological tissues;Biomembranes;Bones;Deafness;Ear;Laser ablation;Laser surgery;Navigation;Protection;Skull;Coaxial Process Control;Interface Detection;Laser Bone Ablation;Vision Navigation},
}

@InProceedings{932904,
  author    = {D. Engel and J. Raczkowsky and H. Worn},
  title     = {A safe robot system for craniofacial surgery},
  booktitle = {Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No.01CH37164)},
  year      = {2001},
  volume    = {2},
  pages     = {2020-2024 vol.2},
  abstract  = {The ultimate ambition of a robotic system in surgical theater has to be the safety of the involved humans: patient, physicians, nurses. In order to provide that claim the criteria of ergonomics, redundancy, short reaction time, and accuracy must be met. In contrast to many other surgical robotic applications we are studying complex bone cut trajectories which require several distinct orientations of the robot tool and milling cutter, respectively. Exact bone cuts are especially needed for bone repositionings placed at the human skull in craniofacial surgery. Therefore, the system has to be flexible enough to keep the capability of changing the patient's position during the intra-operative phase. This paper introduces the overall concept and realization of the system.},
  doi       = {10.1109/ROBOT.2001.932904},
  issn      = {1050-4729},
  keywords  = {ergonomics;medical robotics;redundant manipulators;surgery;bone repositionings;complex bone cut trajectories;craniofacial surgery;ergonomics;human skull;milling cutter;redundancy;robot tool;safe robot system;surgical robotic applications;Bones;Ergonomics;Fasteners;Robot kinematics;Robot sensing systems;Safety;Service robots;Skull;Surgery;Surges},
}

@InProceedings{894661,
  author    = {J. Munchenberg and J. Brief and J. Raczkowsky and H. Worn and S. Hassfeld and J. Muhling},
  title     = {Operation planning of robot supported surgical interventions},
  booktitle = {Proceedings. 2000 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2000) (Cat. No.00CH37113)},
  year      = {2000},
  volume    = {1},
  pages     = {547-552 vol.1},
  abstract  = {Presents a new operation planning system which enables a surgeon to plan and perform complex robot-supported surgical interventions. The system has been evaluated in the Clinic for Cranio-Maxillo-Facial Surgery at the University of Heidelberg. In contrast to commercial systems, our goal was that the system should consider the complete surgical intervention and not just a single procedure of it. A second goal was that the system should enable the management of complex operations, independent of which way the intervention is intra-operatively performed (without technical support, with passive navigation support or with active support by robots). Our system supports the surgeon during pre-operative planning as well as during the intra-operative execution phase. Therefore, we developed a model of the course of operation by which the management of surgical interventions is made possible. The focus of this paper is on this course model. First, we introduce instruction graphs and describe the structure of each activity, observing its attributes and their context. Additionally, various surgical scopes are presented which enable the surgeon to select one view among different ones in the individual operation procedures, in accordance with medical and technical knowledge as well as in accordance with different degrees of abstraction. Finally, we demonstrate the realisation of the introduced course model in our operation planning system and present first results in clinical practice},
  doi       = {10.1109/IROS.2000.894661},
  keywords  = {control engineering computing;graphs;medical computing;medical robotics;planning;surgery;abstraction;active robotic support;clinical practice;complex operation management;instruction graphs;intraoperative execution phase;medical knowledge;operation course model;passive navigation support;preoperative planning;robot-supported surgical interventions;surgical operation planning system;surgical scope;technical knowledge;technical support;Computer science;Costs;Head;Navigation;Process control;Process planning;Robots;Surgery;Surges;Surgical instruments},
}

@InProceedings{637958,
  author    = {J. Raczkowsky and U. Rembold},
  title     = {Sensor Data Integration For The Control Of An Autonomous Robot},
  booktitle = {Intelligent Robots and Systems '89. The Autonomous Mobile Robots and Its Applications. IROS '89. Proceedings., IEEE/RSJ International Workshop on},
  year      = {1989},
  pages     = {557-562},
  month     = {Sept},
  abstract  = {Not Available},
  doi       = {10.1109/IROS.1989.637958},
  keywords  = {Control systems;Data processing;Intelligent sensors;Mobile robots;Navigation;Robot sensing systems;Robotic assembly;Robotics and automation;Sensor fusion;Sensor systems},
}

@InProceedings{6609707,
  author    = {N. Jansen and T. Brennecke and J. Hirschfeld and L. Colter and J. Raczkowsky and H. Woern and J. Schipper},
  title     = {Rotatable flexible neck-model for the evaluation of minimally invasive operation procedures with the help of an ultrasound-based navigation system},
  booktitle = {2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
  year      = {2013},
  pages     = {1140-1143},
  month     = {July},
  abstract  = {Future minimally invasive neck surgery requires a navigation system adapted to the actual intraoperative bedding of the patient. The detection of the bedding-caused tissue shift is essential for a safe orientation for the surgeons' new endoscopic operation procedures in neck surgery. It is essential to visualize the relation between important anatomic landmarks and operation instruments at any time. Within the scientific project SACAS we focus on developing an ultrasound supported navigation system based on preoperative imaging which considers the intraoperative tissue shift. A rotatable, flexible neck-model provides the basis for our analyses to evaluate the tissue shift and to invent the new navigation system for endoscopic neck surgery. The total registration error of the system was 2 mm.},
  doi       = {10.1109/EMBC.2013.6609707},
  issn      = {1094-687X},
  keywords  = {biological tissues;biomedical ultrasonics;image registration;medical image processing;physiological models;surgery;actual intraoperative bedding;anatomic landmark;bedding-caused tissue shift detection;endoscopic neck surgery;intraoperative tissue shift;minimally invasive neck surgery;operation instrument;preoperative imaging;rotatable flexible neck-model;safe orientation;scientific project SACAS;total registration error;ultrasound supported navigation system;ultrasound-based navigation system;Imaging;Materials;Navigation;Neck;Sockets;Surgery;Ultrasonic imaging;Carotid Arteries;Evaluation Studies as Topic;Humans;Magnetic Resonance Imaging;Neck;Rotation;Surgery, Computer-Assisted;Surgical Procedures, Minimally Invasive;Ultrasonics},
}

@InProceedings{7347758,
  author    = {P. Nicolai and J. Raczkowsky and H. Wörn},
  title     = {Continuous pre-calculation of human tracking with time-delayed ground-truth: A hybrid approach to minimizing tracking latency by combination of different 3D cameras},
  booktitle = {2015 12th International Conference on Informatics in Control, Automation and Robotics (ICINCO)},
  year      = {2015},
  volume    = {02},
  pages     = {121-130},
  month     = {July},
  abstract  = {We present an approach to track a point cloud with a 3D camera system with low latency and/or high frame rate, based on ground truth provided by a second 3D camera system with higher latency and/or lower frame rate. In particular, we employ human tracking based on Kinect cameras and combine it with higher frame-rate/lower latency of Time-of-Flight (ToF) cameras. We present the system setup, methods used and evaluation results showing a very high accuracy in combination with a latency reduction of up to factor 30.},
  keywords  = {cameras;human-robot interaction;image sensors;minimisation;object tracking;robot vision;3D camera system;Kinect cameras;ToF cameras;continuous precalculation;frame rate;human tracking;human-robot interaction;hybrid tracking latency minimization approach;latency reduction;time-delayed ground-truth;time-of-flight cameras;Cameras;Computer vision;Image motion analysis;Optical imaging;Optical sensors;Target tracking;Three-dimensional displays;3D Camera;Data Fusion;Probability Propagation;Tracking},
}

@InProceedings{4811031,
  author    = {A. de Mauro and J. Raczkowsky and M. E. Halatsch and H. Worn},
  title     = {Virtual Reality Training Embedded in Neurosurgical Microscope},
  booktitle = {2009 IEEE Virtual Reality Conference},
  year      = {2009},
  pages     = {233-234},
  month     = {March},
  abstract  = {In this paper, we present the very first virtual reality training system for neurosurgical interventions based on a real surgical microscope and on a haptic interface for a better visual and ergonomic realism. Its main purpose is the realistic simulation of the palpation of low-grade glioma. The ability of a surgeon to feel the difference in consistency between tumors cells and normal brain parenchyma requires considerable experience and it is a key factor for a successful intervention. The simulation takes advantage of an accurate tissue modeling, a force feedback device and the rendering of the virtual scene directly to the oculars of the operating microscope.},
  doi       = {10.1109/VR.2009.4811031},
  issn      = {1087-8270},
  keywords  = {force feedback;haptic interfaces;medical computing;rendering (computer graphics);virtual reality;brain parenchyma;ergonomic realism;force feedback device;haptic interface;low-grade glioma;neurosurgical intervention;neurosurgical microscope;palpation;rendering;tissue modeling;virtual reality training system;virtual scene;Brain modeling;Ergonomics;Force feedback;Haptic interfaces;Microscopy;Neurosurgery;Surgery;Surges;Tumors;Virtual reality;Virtual reality;haptic feedback;neurosurgery;physical modeling},
}

@InProceedings{724068,
  author    = {C. Burghart and C. Wurll and D. Henrich and J. Raczkowsky and U. Rembold and H. Worn},
  title     = {On-line motion planning for medical applications},
  booktitle = {Industrial Electronics Society, 1998. IECON '98. Proceedings of the 24th Annual Conference of the IEEE},
  year      = {1998},
  volume    = {4},
  pages     = {2233-2238 vol.4},
  month     = {Aug},
  abstract  = {Enhancing the quality of surgical interventions is one of the main goals of surgical robotics. Thus we have devised a surgical robotic system for maxillofacial surgery which can be used as an intelligent intraoperative surgical tool. Up to now a surgeon preoperatively plans an intervention by studying two dimensional X-rays, thus neglecting the third dimension. In the course of the special research programme “Computer and Sensor Aided Surgery” a planning system has been developed at our institute, which allows the surgeon to plan an operation on a three dimensional computer model of the patient. Transposing the preoperatively planned bone cuts, bore holes, cavities, and milled surfaces during surgery still proves to be a problem, as no adequate means are at hand: the actual performance of the surgical intervention and the surgical outcome solely depend on the experience and the skill of the operating surgeon. In this paper we present our approach of a surgical robotic system to be used in maxillofacial surgery. Special stress is being laid upon the modelling of the environment in the operating theatre and the motion planning of our surgical robot},
  doi       = {10.1109/IECON.1998.724068},
  keywords  = {bone;medical computing;medical robotics;path planning;surgery;Computer and Sensor Aided Surgery;bore holes;cavities;intelligent intraoperative surgical tool;maxillofacial surgery;medical applications;milled surfaces;on-line motion planning;operating theatre environment;operation planning;preoperatively planned bone cuts;surgical interventions;surgical robotic system;surgical robotics;three dimensional computer model;Biomedical equipment;Intelligent robots;Intelligent sensors;Medical robotics;Medical services;Motion planning;Robot sensing systems;Surgery;Surges;X-rays},
}

@InProceedings{724121,
  author    = {C. Burghart and J. Raczkowsky and U. Rembold and H. Worn},
  title     = {Robot cell for craniofacial surgery},
  booktitle = {Industrial Electronics Society, 1998. IECON '98. Proceedings of the 24th Annual Conference of the IEEE},
  year      = {1998},
  volume    = {4},
  pages     = {2506-2511 vol.4},
  month     = {Aug},
  abstract  = {Using a robot in craniofacial surgery might seem to be a rather far fetched idea. However, a high level of precision, skill and experience is necessary in order to perform maxillofacial interventions successfully and harmonically. Thus the Institute of Process Control and Robotics has devised a complex computer aided surgical system which supports the surgeon before and during the operation. On the one hand preoperative plans can be generated by using a three-dimensional computer model of the patient's skull. On the other hand the best planning device is rather useless if there is no adequate means at hand to intraoperatively transpose the established plan with the required accuracy. In this paper we present our surgical robotic system for craniofacial surgery and give an insight into some of its features},
  doi       = {10.1109/IECON.1998.724121},
  keywords  = {medical computing;medical robotics;surgery;Institute of Process Control and Robotics;computer aided surgical system;craniofacial surgery;maxillofacial interventions;patient skull;preoperative plans;robot cell;surgical robotic system;three-dimensional computer model;Defense industry;Electrical equipment industry;Kinematics;Minimally invasive surgery;Navigation;Orthopedic surgery;Process control;Service robots;Surges;Surgical instruments},
}

@InProceedings{4543729,
  author    = {J. Burgner and Yaokun Zhang and J. Raczkowsky and H. Woern and G. Eggers and J. Muehling},
  title     = {Methods for end-effector coupling in robot assisted interventions},
  booktitle = {2008 IEEE International Conference on Robotics and Automation},
  year      = {2008},
  pages     = {3395-3400},
  month     = {May},
  abstract  = {Robot assisted interventions often require coupling and decoupling of the robot to/from a specific tool. By using manual gripper changing systems these operations are facilitated, but the robot has to approach to and move away from the coupling position. Industrial applications are mostly based on movements which are teached-in, since the working environment is perfectly described (i.e. working cell). Especially in robot assisted surgery we are facing non fixed tools to which the robot has to be coupled (e.g. a holding device attached to a mobilised bone) and restricted working areas with special safety requirements. In this paper we present an automatic end-effector registration method and a semiautomatic coupling procedure exemplarily for robot assisted orthognathic surgery. By using means of an optical localisation system and force- /torque sensing, the coupling procedure is controlled by a multi- sensor data fusion approach. The developed methods can be adapted to any robot assisted intervention.},
  doi       = {10.1109/ROBOT.2008.4543729},
  issn      = {1050-4729},
  keywords  = {end effectors;medical robotics;surgery;automatic end-effector registration method;end-effector coupling;manual gripper changing systems;robot assisted interventions;robot assisted orthognathic surgery;robot assisted surgery;Bones;Grippers;Manuals;Mobile robots;Optical sensors;Robot sensing systems;Robotics and automation;Safety devices;Service robots;Surgery},
}

@InProceedings{5509228,
  author    = {H. Mönnich and D. Stein and J. Raczkowsky and H. Wörn},
  title     = {An automatic and complete self-calibration method for robotic guided laser ablation},
  booktitle = {2010 IEEE International Conference on Robotics and Automation},
  year      = {2010},
  pages     = {1086-1087},
  month     = {May},
  abstract  = {This paper describes the content of the video for ICRA 2010. The approach presented in the video is a complete automatic registration of all needed devices for robotic guided CO2 bone processing. The system consists of an optical tracking system, a lightweight robot and a scan head. While a standard point to point transformation for the transformation between optical tracking system and robot is inadequate, a special registration algorithm was developed and therefore is used. This allows an automatic registration by moving the robot to different positions and storing the position of a body attached to the robot TCP. The scan head is registered to the optical tracking system or to the robot using a camera that tracks the prototype laser. Only the patient must be registered manually. The result is that the robot moves to the target position on the bone defined inside the segmented 3D CT dataset. With the known registration between robot and tracking system the robot can also be controlled via visual servoing.},
  doi       = {10.1109/ROBOT.2010.5509228},
  issn      = {1050-4729},
  keywords  = {calibration;laser ablation;medical robotics;self-adjusting systems;surgery;tracking;automatic self-calibration method;complete automatic registration;complete self-calibration method;lightweight robot;optical tracking system;robotic guided CO2 bone processing;robotic guided laser ablation;scan head;visual servoing;Bones;Cameras;Control systems;Laser ablation;Prototypes;Robot vision systems;Robotics and automation;Standards development;Target tracking;Visual servoing;Surgical robotics;laser osteotomy;scan head;self calibration},
}

@InProceedings{5152518,
  author    = {J. Burgner and J. Raczkowsky and H. Woern},
  title     = {End-effector calibration and registration procedure for robot assisted laser material processing: Tailored to the particular needs of short pulsed CO2 laser bone ablation},
  booktitle = {2009 IEEE International Conference on Robotics and Automation},
  year      = {2009},
  pages     = {3091-3096},
  month     = {May},
  abstract  = {Material processing using a laser has become a widely used method for industrial procedures (e.g. laser welding or cutting). Furthermore the medical laser has become an integral part of dermatology, neurosurgery, ENT, esthetic, plastic and general surgery. Recent publications have shown, that the short pulsed CO2 laser is suitable to ablate bony and cartilage tissue and proposes fundamentally new operative techniques in medicine. The obtainable precision in cutting (in the hundred micrometers range) with a laser system can only be reached using means of computer and robot assisted surgery. We established the first robot assisted laser bone ablation setup, comprising a prototype CO2 laser system and a six degree of freedom robot. The laser beam is guided through a passive articulated mirror arm to the robots end-effector. The end-effector is composed of a two mirror galvanometric scan head, which deflects the pulsed laser beam onto the tissue. In this paper we present an end-effector calibration and registration method to determine the parameters which are critical in obtaining precise and accurate cutting results.},
  doi       = {10.1109/ROBOT.2009.5152518},
  issn      = {1050-4729},
  keywords  = {laser ablation;laser materials processing;medical robotics;freedom robot;laser bone ablation;robot assisted laser material processing:;robot assisted surgery;Calibration;Laser ablation;Laser beam cutting;Laser beams;Laser surgery;Materials processing;Medical robotics;Mirrors;Optical pulses;Robots},
}

@InProceedings{1433684,
  author    = {A. Eslami and S. Kasaei and M. Jahed},
  title     = {Radial multiscale cyst segmentation in ultrasound images of kidney},
  booktitle = {Proceedings of the Fourth IEEE International Symposium on Signal Processing and Information Technology, 2004.},
  year      = {2004},
  pages     = {42-45},
  month     = {Dec},
  abstract  = {Cysts are one of the most common lesions in kidneys and can be diagnosed exploiting ultrasound images. In this paper, we develop an automatic approach for cyst segmentation in ultrasound images. The proposed approach comprises three steps: finding a seed point in the object exploiting the Gibbs random field, detecting the boundary based on multiresolution signal processing and edge refining and finally shape model features extracting to verify whether the object is a cyst. The proposed approach is fast and less complex to suit ultrasound exploration applications.},
  doi       = {10.1109/ISSPIT.2004.1433684},
  keywords  = {biomedical ultrasonics;feature extraction;image resolution;image segmentation;kidney;medical image processing;Gibbs random field;boundary detection;cyst segmentation;diagnosis;edge refining;kidney lesion;multiresolution signal processing;radial multiscale;shape model feature extraction;ultrasound image;Biomedical imaging;Image edge detection;Image resolution;Image segmentation;Lesions;Medical diagnostic imaging;Pixel;Random variables;Shape;Ultrasonic imaging},
}

@InProceedings{4221402,
  author    = {A. Eslami and N. Sadati and M. Jahed},
  title     = {Fuzzy Motion Interpolation for Mesh-Based Motion Estimation},
  booktitle = {2007 IEEE Symposium on Computational Intelligence in Image and Signal Processing},
  year      = {2007},
  pages     = {101-106},
  month     = {April},
  abstract  = {Mesh-based motion estimation is an important tool for video coding especially with low bit rate. In this paper, a new method for interpolating pixel motion from adjacent mesh nodes with the ability of omitting independent nodes is proposed. By exploiting fuzzy rules to determine the association of pixel and neighboring nodes, the proposed interpolation can detach pixels from some nodes. Consequently, it can deal with those critical patches on objects boundary which their nodes do not belong to one object. Updating the membership functions of each rule with specified strategy makes the interpolation adaptive with non-stationary conditions of image sequences and decreases sensitivity to initial selection of parameters. Experimental results show that the proposed method, in comparison with the conventional methods, increases the motion interpolation accuracy while does not accompanies with massive time cost and complexity},
  doi       = {10.1109/CIISP.2007.369301},
  keywords  = {fuzzy set theory;image sequences;interpolation;motion estimation;video coding;fuzzy membership functions;fuzzy motion interpolation;fuzzy rules;image sequences;mesh-based motion estimation;object boundary;pixel association;pixel detachment;pixel motion interpolation;video coding;Computational intelligence;Image sequences;Intelligent robots;Interpolation;Laboratories;Machine vision;Motion estimation;PSNR;Robot vision systems;Video coding},
}

@InProceedings{6402589,
  author    = {O. Pauly and A. Katouzian and A. Eslami and P. Fallavollita and N. Navab},
  title     = {Supervised classification for customized intraoperative augmented reality visualization},
  booktitle = {2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
  year      = {2012},
  pages     = {311-312},
  month     = {Nov},
  abstract  = {In this paper, we present a fusion algorithm supplemented with appropriate visualization by selecting relevant information from different modalities in mixed and augmented reality (AR). This encompasses a learning based method upon relevance of information, defined by an expert, which ultimately enables confident interventional decisions based on mixed reality (MR) images. The performance of our developed fusion and tailored visualization techniques was evaluated by employing X-ray/optical images during surgery and validated qualitatively using a 5-point Likert scale. Our observations indicated that the proposed technique provided semantic contextual information about underlying pixels and in general was preferred over the traditional pixel-wise linear alpha-blending method.},
  doi       = {10.1109/ISMAR.2012.6402589},
  keywords  = {augmented reality;biomedical optical imaging;data visualisation;diagnostic radiography;image classification;learning (artificial intelligence);medical image processing;sensor fusion;5-point Likert scale;AR;MR;X-ray image;augmented reality;customized intraoperative augmented reality visualization;fusion algorithm;information relevance;interventional decision;learning based method;mixed reality;optical image;pixel-wise linear alpha-blending method;supervised classification;surgery;Augmented reality;Biomedical optical imaging;Optical imaging;Optical mixing;Surgery;Visualization;X-ray imaging;CamC;Fusion;Medical Augmented Reality;Relevant Information;Visualization;X-ray},
}

@InProceedings{7098843,
  author    = {M. H. Kadbi and E. Fatemizaheh and A. Eslami and M. Khosroshahi},
  title     = {A new method for colourizing of multichannel MR images based on real colour of human brain},
  booktitle = {2007 15th European Signal Processing Conference},
  year      = {2007},
  pages     = {449-453},
  month     = {Sept},
  abstract  = {In this paper, we propose a novel approach for colourizing of multichannel MR images based on real colour of human brain. At first, we use independent component analysis (ICA) to transfer three MR modalities into different representations, which enhance the physical characteristics of tissues. Then, Statistical Features are extracted from three independent components and their wavelet coefficients to generate a feature vector for each pixel of images. Huge amount of features are reduced by using some feature reduction methods such as Principle Component Analysis (PCA) to classify the brain into 3 primary classes. The colour space of Visible Human Project (VHP) dataset is quantized to 10 colours by using a partially supervised algorithm included self-organization map and Linear Vector Quantization (LVQ) algorithms. Acquired 10 colours are allotted to each pixel based on texture and intensity of image in two different steps. Consequently, a fast-automated algorithm is proposed, which has low sensitivity to variation of pixel intensity and precise result regarding to VHP images.},
  keywords  = {biomedical MRI;brain;feature extraction;image colour analysis;image texture;independent component analysis;learning (artificial intelligence);medical image processing;principal component analysis;ICA;LVQ algorithms;PCA;VHP dataset;VHP images;feature reduction methods;human brain;image intensity;image pixel;image texture;independent component analysis;linear vector quantization;multichannel MR images;physical characteristics;pixel intensity;principle component analysis;real colour;self-organization map;statistical features;supervised algorithm;visible human project;wavelet coefficients;Classification algorithms;Feature extraction;Image color analysis;Image segmentation;Magnetic resonance imaging;Signal processing algorithms;Wavelet packets},
}

@InProceedings{6090731,
  author    = {A. Eslami and M. Yigitsoy and N. Navab},
  title     = {Manifold learning for shape guided segmentation of Cardiac boundaries: Application to 3D #x002B;t Cardiac MRI},
  booktitle = {2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society},
  year      = {2011},
  pages     = {2658-2662},
  month     = {Aug},
  abstract  = {In this paper we propose a new method for shape guided segmentation of cardiac boundaries based on manifold learning of the shapes represented by the phase field approximation of the Mumford-Shah functional. A novel distance is defined to measure the similarity of shapes without requiring deformable registration. Cardiac motion is compensated and phases are mapped into one reference phase, that is the end of diastole, to avoid time warping and synchronization at all cardiac phases. Non-linear embedding of these 3D shapes extracts the manifold of the inter-subject variation of the heart shape to be used for guiding the segmentation for a new subject. For validation the method is applied to a comprehensive dataset of 3D+t cardiac Cine MRI from normal subjects and patients.},
  doi       = {10.1109/IEMBS.2011.6090731},
  issn      = {1094-687X},
  keywords  = {biomedical MRI;cardiology;image segmentation;medical image processing;3D+t Cardiac MRI;Cardiac boundaries;Cine MRI;Mumford-Shah functional;cardiac motion;manifold learning;shape guided segmentation;Approximation methods;Heart;Image segmentation;Magnetic resonance imaging;Manifolds;Motion segmentation;Shape;Heart;Humans;Magnetic Resonance Imaging},
}

@Article{6269061,
  author   = {A. Katouzian and A. Karamalis and D. Sheet and E. Konofagou and B. Baseri and S. G. Carlier and A. Eslami and A. König and N. Navab and A. F. Laine},
  title    = {Iterative Self-Organizing Atherosclerotic Tissue Labeling in Intravascular Ultrasound Images and Comparison With Virtual Histology},
  journal  = {IEEE Transactions on Biomedical Engineering},
  year     = {2012},
  volume   = {59},
  number   = {11},
  pages    = {3039-3049},
  month    = {Nov},
  issn     = {0018-9294},
  abstract = {Intravascular ultrasound (IVUS) is the predominant imaging modality in the field of interventional cardiology that provides real-time cross-sectional images of coronary arteries and the extent of atherosclerosis. Due to heterogeneity of lesions and stringent spatial/spectral behavior of tissues, atherosclerotic plaque characterization has always been a challenge and still is an open problem. In this paper, we present a systematic framework from in vitro data collection, histology preparation, IVUS-histology registration along with matching procedure, and finally a robust texture-derived unsupervised atherosclerotic plaque labeling. We have performed our algorithm on in vitro and in vivo images acquired with single-element 40 MHz and 64-elements phased array 20 MHz transducers, respectively. In former case, we have quantified results by local contrasting of constructed tissue colormaps with corresponding histology images employing an independent expert and in the latter case, virtual histology images have been utilized for comparison. We tackle one of the main challenges in the field that is the reliability of tissues behind arc of calcified plaques and validate the results through a novel random walks framework by incorporating underlying physics of ultrasound imaging. We conclude that proposed framework is a formidable approach for retrieving imperative information regarding tissues and building a reliable training dataset for supervised classification and its extension for in vivo applications.},
  doi      = {10.1109/TBME.2012.2213338},
  keywords = {biological tissues;biomedical transducers;biomedical ultrasonics;blood vessels;cardiology;diseases;image registration;image segmentation;information retrieval systems;medical image processing;ultrasonic transducer arrays;IVUS-histology registration;calcified plaques;constructed tissue colormaps;coronary arteries;frequency 20 MHz;frequency 40 MHz;image registration;image segmentation;imperative information retrieval;in vitro data collection;interventional cardiology;intravascular ultrasound images;iterative self-organizing atherosclerotic tissue labeling;lesion heterogeneity;phased array transducers;random walks framework;real-time cross-sectional images;reliable training dataset;tissue stringent spatial-spectral behavior;virtual histology;Arteries;Atherosclerosis;Classification algorithms;Feature extraction;Filter banks;Reliability;Ultrasonic imaging;Atherosclerosis;histology;intravascular ultrasound (IVUS);plaque characterization;random walks;wavelet packets;Algorithms;Echocardiography;Histological Techniques;Humans;Image Processing, Computer-Assisted;Myocardium;Plaque, Atherosclerotic;Ultrasonography, Interventional},
}

@Article{8007327,
  author   = {H. Roodaki and N. Navab and A. Eslami and C. Stapleton and N. Navab},
  title    = {SonifEye: Sonification of Visual Information using Physical Modeling Sound Synthesis},
  journal  = {IEEE Transactions on Visualization and Computer Graphics},
  year     = {2017},
  volume   = {PP},
  number   = {99},
  pages    = {1-1},
  issn     = {1077-2626},
  abstract = {Sonic interaction as a technique for conveying information has advantages over conventional visual augmented reality methods specially when augmenting the visual field with extra information brings distraction. Sonification of knowledge extracted by applying computational methods to sensory data is a well-established concept. However, some aspects of sonic interaction design such as aesthetics, the cognitive effort required for perceiving information, and avoiding alarm fatigue are not well studied in literature. In this work, we present a sonification scheme based on employment of physical modeling sound synthesis which targets focus demanding tasks requiring extreme precision. Proposed mapping techniques are designed to require minimum training for users to adapt to and minimum mental effort to interpret the conveyed information. Two experiments are conducted to assess the feasibility of the proposed method and compare it against visual augmented reality in high precision tasks. The observed quantitative results suggest that utilizing sound patches generated by physical modeling achieve the desired goal of improving the user experience and general task performance with minimal training.},
  doi      = {10.1109/TVCG.2017.2734327},
  keywords = {Acceleration;Auditory displays;Augmented reality;Computational modeling;Load modeling;Mathematical model;Visualization;Aural augmented reality;auditory feedback;sonic interaction;sonification},
}

@InProceedings{4042369,
  author    = {A. Eslami and M. Babaeizadeh},
  title     = {Adaptive Block Motion Prediction},
  booktitle = {2006 IEEE International Symposium on Signal Processing and Information Technology},
  year      = {2006},
  pages     = {908-913},
  month     = {Aug},
  abstract  = {Block motion estimation is an important field of video processing. This paper presents a new scheme to reach in faster block motion estimation based on motion prediction. The scheme exploits an adaptive filter to predict the block motion from its spatio-temporal motion compensated adjacent blocks, the predicted motion determines the initial candidate block for search methods with biased search center, in special case of partial distortion search, this strategy reduces search time by preventing from complete distortion calculation for more loser candidates, after comparison of three conventional adaptive filters, a normalized least mean square filter with convergence monitoring is recommended for motion prediction from 5 neighbor blocks. Experimental results imply on adaptive filter ability for block motion prediction and its efficiency in reducing the time cost of motion estimation search strategy},
  doi       = {10.1109/ISSPIT.2006.270927},
  issn      = {2162-7843},
  keywords  = {adaptive filters;least mean squares methods;motion compensation;motion estimation;search problems;video coding;adaptive block motion prediction;adaptive filter;block motion estimation;convergence monitoring;motion estimation search strategy;normalized least mean square filter;partial distortion search;search methods;spatio-temporal motion compensated adjacent blocks;video processing;Adaptive filters;Adaptive signal processing;Convergence;Costs;Information technology;Monitoring;Motion estimation;Nonlinear distortion;Search methods;Telephony},
}

@InProceedings{6235901,
  author    = {A. Eslami and A. Teimori and M. Yigitsoy and N. Navab},
  title     = {A new framework for morphological and morphometric study of fish species based on groupwise registration of otolith images},
  booktitle = {2012 9th IEEE International Symposium on Biomedical Imaging (ISBI)},
  year      = {2012},
  pages     = {1679-1682},
  month     = {May},
  abstract  = {Morphology of bones, teeth, and some particular structures are widely used for categorizing species and studying their evolution. In this paper, we used groupwise registration to provide a representative image from the set of the image samples that represents its typical morphology. We also provided perturbation map which indicates the deviation of each point through the ensemble. These images support qualitative discussions about the morphology of structures in different species. The perturbation map can be further exploited for determining appropriate landmarks for morphometric analysis. Knowing the deformation between the prototype and each image sample from the species, the framework allows for automatic detection of corresponding points. Once the user puts a landmark on the prototype image, the corresponding points on all the image samples will be determined. This maximizes the accuracy in measuring the morphological indices by eliminating the human error due to uncertainty in locating landmarks.},
  doi       = {10.1109/ISBI.2012.6235901},
  issn      = {1945-7928},
  keywords  = {biological techniques;bone;evolution (biological);image registration;zoology;bones;evolution;fish species;groupwise registration;morphological study;morphometric study;otolith images;perturbation map;prototype image;teeth;uncertainty;Biomedical imaging;Humans;Marine animals;Measurement uncertainty;Morphology;Shape;Splines (mathematics);Evolution;Groupwise registration;Morphology;Morphometric;Otolith},
}

@InProceedings{5742239,
  author    = {A. Sadr and M. Jahed and P. Salehian and A. Eslami},
  title     = {Leukocyte's nucleus segmentation using active contour in YCbCr colour space},
  booktitle = {2010 IEEE EMBS Conference on Biomedical Engineering and Sciences (IECBES)},
  year      = {2010},
  pages     = {257-260},
  month     = {Nov},
  abstract  = {Blood cell segmentation is a crucial part of many medical and laboratory procedures such as cell counting and blood cell disorder diagnosis. Among different types of blood cells, white blood cells are the most important clinically, as they suffer greatest from blood disorders. In this paper we propose a method for automatic segmentation of white blood cells nucleus. A distinctive function is used in YCbCr color space to segment the white blood cells nucleus. Next, the sub-images are extracted which contain the whole body of white blood cell nucleus. Then an active contour method is applied to the sub-images extracted from the previous step to accurately segment the cell nucleus boundary. Our analysis conducted on 20 samples, show a great success on segmentation of white blood cells nucleus.},
  doi       = {10.1109/IECBES.2010.5742239},
  keywords  = {biomedical optical imaging;blood;cellular biophysics;image colour analysis;image segmentation;medical image processing;YCbCr colour space;active contour;blood cell disorder diagnosis;blood cell segmentation;cell counting;leukocyte;nucleus segmentation;subimages extraction;white blood cells;Algorithm design and analysis;Biomedical imaging;Filtering;Image color analysis;Image edge detection;Image segmentation;World Wide Web;Active Contour;Image segmentation;White blood cell nucleus;YCbCr colour space},
}

@InProceedings{7945729,
  author    = {R. Frikha and R. Ejbali and M. Zaied},
  title     = {Handling occlusion in Augmented Reality surgical training based instrument tracking},
  booktitle = {2016 IEEE/ACS 13th International Conference of Computer Systems and Applications (AICCSA)},
  year      = {2016},
  pages     = {1-5},
  month     = {Nov},
  abstract  = {In the medical field, research studies have shown that Augmented Reality (AR) based surgical training has a good potential in making the learning process more active. However, lack of displaying the correct occlusion between the real surgical instrument and virtual organ greatly limits the trainee surgeons understanding and reduces the overall system usability. In this paper, we propose a novel mutual occlusion handling method based on surgical instrument tracking and 3D positioning approach in AR environment. Therefore, we introduce a monocular image processing based paradigm that aims at (1) tracking the instrument using both background subtraction and Hough transform method (2) calculating the 3D position of the instrument using the geometry of perspective projection (3) comparing the 3D coordinates of the real instrument with the virtual organ to achieve a realistic AR rendering system. The experimental results show that our approach is highly accurate and can handle the mutual occlusion automatically in real time.},
  doi       = {10.1109/AICCSA.2016.7945729},
  keywords  = {Hough transforms;augmented reality;computer based training;medical computing;object tracking;position control;rendering (computer graphics);surgery;3D positioning;AR environment;AR rendering system;Hough transform;augmented reality surgical training based instrument tracking;background subtraction;learning process;medical field;monocular image processing based paradigm;occlusion handling;perspective projection geometry;surgical instrument tracking;Cameras;Instruments;Real-time systems;Surgery;Three-dimensional displays;Tools;Training;3D positioning;Augmented Reality;handling occlusion;instrument tracking;surgical simulation training},
}

@InProceedings{7520404,
  author    = {C. He and Y. Liu and Y. Wang},
  title     = {Sensor-fusion based augmented-reality surgical navigation system},
  booktitle = {2016 IEEE International Instrumentation and Measurement Technology Conference Proceedings},
  year      = {2016},
  pages     = {1-5},
  month     = {May},
  abstract  = {Surgical navigation systems can overlay medical images on the patients and help surgeons obtain surgical information in operations through motion tracking and fusion display. To improve the performance of display and accuracy of motion tracking of surgical navigation systems, this paper proposes an inertial-optical hybrid tracking system and a half-mirror based surgical navigation system. Extended Kalman filter is used to fuse the measurements from the inertial and optical sensors, as well as provide the orientation and position of the surgical tool and patient. The hybrid tracking method can compensate for the occlusion under the conditions that the markers are partially occluded in long-term. The surgical navigation image is related to the real targets by the hybrid tracking method and displayed in a half-mirror display device which overlays the virtual targets on the surgical tool and patient. The experimental results indicate that the proposed surgical navigation system based on sensor fusion can accurately track the motion of the surgical targets and display the guiding image in real time.},
  doi       = {10.1109/I2MTC.2016.7520404},
  keywords  = {Kalman filters;augmented reality;image motion analysis;nonlinear filters;sensor fusion;surgery;augmented-reality surgical navigation system;extended Kalman filter;half-mirror based surgical navigation system;half-mirror display device;inertial-optical hybrid tracking system;motion tracking;sensor fusion;Biomedical monitoring;Monitoring;Navigation;Position measurement;Surgery;Target tracking;motion tracking;sensors fusion;surgical navigation},
}

@InProceedings{1320126,
  author    = {R. J. Lapeer and R. S. Rowland and Min Si Chen},
  title     = {PC-based volume rendering for medical visualisation and augmented reality based surgical navigation},
  booktitle = {Proceedings. Eighth International Conference on Information Visualisation, 2004. IV 2004.},
  year      = {2004},
  pages     = {67-72},
  month     = {July},
  abstract  = {We describe a generic software architecture for stereoscopic augmented reality microsurgery, built upon a general framework for volume rendering based 3D visualisation. The software is called ARView and allows the user to perform the standard procedures for stereoscopic augmented reality surgical navigation: calibration of a visualisation device; registration of pre-operatively and intra-operatively acquired patient image data; overlaying with either volume or surface rendered (segmented) data using different methodologies; and tracking of moving objects in the surgical scene.},
  doi       = {10.1109/IV.2004.1320126},
  issn      = {1093-9547},
  keywords  = {augmented reality;data visualisation;medical computing;rendering (computer graphics);surgery;3D visualisation;ARView;PC-based volume rendering;medical visualisation;patient image data;software architecture;stereoscopic augmented reality microsurgery;surface rendering;surgical navigation;visualisation device calibration;Augmented reality;Biomedical imaging;Microsurgery;Navigation;Rendering (computer graphics);Software architecture;Software performance;Software standards;Surgery;Visualization},
}

@Article{7733143,
  author   = {X. Zhang and G. Chen and H. Liao},
  title    = {High-Quality See-Through Surgical Guidance System Using Enhanced 3-D Autostereoscopic Augmented Reality},
  journal  = {IEEE Transactions on Biomedical Engineering},
  year     = {2017},
  volume   = {64},
  number   = {8},
  pages    = {1815-1825},
  month    = {Aug},
  issn     = {0018-9294},
  abstract = {Objective: Precise minimally invasive surgery (MIS) has significant advantages over traditional open surgery in clinic. Although pre-/intraoperative diagnosis images can provide necessary guidance for therapy, hand-eye discoordination occurs when guidance information is displayed away from the surgical area. In this study, we introduce a real three-dimensional (3-D) see-through guidance system for precision surgery. Methods: To address the resolution and viewing angle limitation as well as the accuracy degradation problems of autostereoscopic 3-D display, we design a high quality and high accuracy 3-D integral videography (IV) medical image display method. Furthermore, a novel see-through microscopic device is proposed to assist surgeons with the superimposition of real 3-D guidance onto the surgical target is magnified by an optical visual magnifier module. Results: Spatial resolutions of 3-D IV image in different depths have been increased 50%~70%, viewing angles of different image sizes have been increased 9%~19% compared with conventional IV display methods. Average accuracy of real 3-D guidance superimposed on surgical target was 0.93 mm ± 0.41 mm. Preclinical studies demonstrated that our system could provide real 3-D perception of anatomic structures inside the patient's body. Conclusion: The system showed potential clinical feasibility to provide intuitive and accurate in situ see-through guidance for microsurgery without restriction on observers' viewing position. Significance: Our system can effectively improve the precision and reliability of surgical guidance. It will have wider applicability in surgical planning, microscopy, and other fields.},
  doi      = {10.1109/TBME.2016.2624632},
  keywords = {augmented reality;biomedical optical imaging;medical image processing;optical microscopy;stereo image processing;surgery;video signal processing;visual perception;3D integral videography medical image display method;3D perception;anatomic structures;conventional IV display methods;enhanced 3D autostereoscopic augmented reality;hand-eye discoordination;high-quality see-through surgical guidance system;microsurgery;minimally invasive surgery;open surgery;optical visual magnifier module;preintraoperative diagnosis images;see-through microscopic device;surgical planning;viewing angle limitation;Biomedical optical imaging;Lenses;Optical imaging;Spatial resolution;Surgery;Three-dimensional displays;Image-guided surgery (IGS);integral videography (IV);microsurgery;real 3-D see-through device;surgical guidance},
}

@InProceedings{4461813,
  author    = {T. Akinbiyi and C. E. Reiley and S. Saha and D. Burschka and C. J. Hasser and D. D. Yuh and A. M. Okamura},
  title     = {Dynamic Augmented Reality for Sensory Substitution in Robot-Assisted Surgical Systems},
  booktitle = {2006 International Conference of the IEEE Engineering in Medicine and Biology Society},
  year      = {2006},
  pages     = {567-570},
  month     = {Aug},
  abstract  = {Teleoperated robot-assisted surgical systems provide surgeons with improved precision, dexterity, and visualization over traditional minimally invasive surgery. The addition of haptic (force and/or tactile) feedback has been proposed as a way to further enhance the performance of these systems. However, due to limitations in sensing and control technologies, implementing direct haptic feedback to the surgeon's hands remains impractical for clinical application. A new, intuitive augmented reality system for presentation of force information through sensory substitution has been developed and evaluated. The augmented reality system consists of force-sensing robotic instruments, a kinematic tool tracker, and a graphic display that overlays a visual representation of force levels on top of the moving instrument tips. The system is integrated with the da Vinci Surgical System (Intuitive Surgical, Inc.) and tested by several users in a phantom knot tying task. The augmented reality system decreases the number of broken sutures, decreases the number of loose knots, and results in more consistent application of forces},
  doi       = {10.1109/IEMBS.2006.259707},
  issn      = {1557-170X},
  keywords  = {augmented reality;force sensors;haptic interfaces;medical robotics;phantoms;surgery;telerobotics;da Vinci surgical system;dynamic augmented reality;force-sensing robotic instrument;graphic display;haptic feedback;kinematic tool tracker;phantom knot tying task;sensory substitution;teleoperated robot-assisted surgical system;visual representation;Augmented reality;Force feedback;Force sensors;Haptic interfaces;Instruments;Kinematics;Minimally invasive surgery;Robot sensing systems;Surges;Visualization;Computer Graphics;Computer Simulation;Imaging, Three-Dimensional;Models, Theoretical;Robotics;Surgery, Computer-Assisted;Suture Techniques;Touch;User-Computer Interface},
}

@InProceedings{7319323,
  author    = {X. Zhang and G. Chen and H. Liao},
  title     = {A high-accuracy surgical augmented reality system using enhanced integral videography image overlay},
  booktitle = {2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
  year      = {2015},
  pages     = {4210-4213},
  month     = {Aug},
  abstract  = {Image guided surgery has been used in clinic to improve the surgery safety and accuracy. Augmented reality (AR) technique, which can provide intuitive image guidance, has been greatly evolved these years. As one promising approach of surgical AR systems, integral videography (IV) autostereoscopic image overlay has achieved accurate fusion of full parallax guidance into surgical scene. This paper describes an image enhanced high-accuracy IV overlay system. A flexible optical image enhancement system (IES) is designed to increase the resolution and quality of IV image. Furthermore, we introduce a novel IV rendering algorithm to promote the spatial accuracy with the consideration of distortion introduced by micro lens array. Preliminary experiments validated that the image accuracy and resolution are improved with the proposed methods. The resolution of the IV image could be promoted to 1 mm for a micro lens array with pitch of 2.32 mm and IES magnification value of 0.5. The relative deviation of accuracy in depth and lateral directions are -4.68±0.83% and -9.01±0.42%.},
  doi       = {10.1109/EMBC.2015.7319323},
  issn      = {1094-687X},
  keywords  = {augmented reality;image resolution;medical image processing;surgery;video recording;autostereoscopic image overlay;enhanced integral videography image overlay;high accuracy surgical augmented reality system;image accuracy;image guided surgery;image resolution;intuitive image guidance;optical image enhancement system;surgery accuracy;surgery safety;Accuracy;Image resolution;Lenses;Optical distortion;Rendering (computer graphics);Surgery;Three-dimensional displays},
}

@InProceedings{6974276,
  author    = {T. Loescher and S. Y. Lee and J. P. Wachs},
  title     = {An augmented reality approach to surgical telementoring},
  booktitle = {2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
  year      = {2014},
  pages     = {2341-2346},
  month     = {Oct},
  abstract  = {Optimal surgery and trauma treatment integrates different surgical skills frequently unavailable in rural/field hospitals. Telementoring can provide the missing expertise, but current systems require the trainee to focus on a nearby telestrator, fail to illustrate coming surgical steps, and give the mentor an incomplete picture of the ongoing surgery. A new telementoring system is presented that utilizes augmented reality to enhance the sense of co-presence. The system allows a mentor to add annotations to be displayed for a mentee during surgery. The annotations are displayed on a tablet held between the mentee and the surgical site as a heads-up display. As it moves, the system uses computer vision algorithms to track and align the annotations with the surgical region. Tracking is achieved through feature matching. To assess its performance, comparisons are made between SURF and SIFT detector, brute force and FLANN matchers, and hessian blob thresholds. The results show that the combination of a FLANN matcher and a SURF detector with a 1500 hessian threshold can optimize this system across scenarios of tablet movement and occlusion.},
  doi       = {10.1109/SMC.2014.6974276},
  issn      = {1062-922X},
  keywords  = {augmented reality;feature extraction;head-up displays;image matching;patient treatment;surgery;telemedicine;FLANN matchers;SIFT detector;SURF detector;augmented reality approach;computer vision algorithms;feature matching;heads-up display;hessian blob thresholds;occlusion;optimal surgery treatment;rural-field hospitals;surgical site;surgical skills;surgical telementoring;tablet movement;telementoring system;telestrator;trauma treatment;Accuracy;Augmented reality;Context;Detectors;Feature extraction;Surgery;Visualization},
}

@Article{6788163,
  author   = {A. P. King and P. J. Edwards and C. R. Maurer and D. A. d. Cunha and R. P. Gaston and M. Clarkson and D. L. G. Hill and D. J. Hawkes and M. R. Fenlon and A. J. Strong and T. C. S. Cox and M. J. Gleeson},
  title    = {Stereo Augmented Reality in the Surgical Microscope},
  journal  = {Presence},
  year     = {2000},
  volume   = {9},
  number   = {4},
  pages    = {360-368},
  month    = {Aug},
  issn     = {1054-7460},
  abstract = {This paper describes the MAGI (microscope-assisted guided interventions) augmented-reality system, which allows surgeons to view virtual features segmented from preoperative radiological images accurately overlaid in stereo in the optical path of a surgical microscope. The aim of the system is to enable the surgeon to see in the correct 3-D position the structures that are beneath the physical surface. The technical challenges involved are calibration, segmentation, registration, tracking, and visualization. This paper details our solutions to these problems. As it is difficult to make reliable quantitative assessments of the accuracy of augmented-reality systems, results are presented from a numerical simulation, and these show that the system has a theoretical overlay accuracy of better than 1 mm at the focal plane of the microscope. Implementations of the system have been tested on volunteers, phantoms, and seven patients in the operating room. Observations are consistent with this accuracy prediction.},
  doi      = {10.1162/105474600566862},
}

@InProceedings{6346203,
  author    = {S. Rodríguez Palma and B. C. Becker and L. A. Lobes and C. N. Riviere},
  title     = {Comparative evaluation of monocular augmented-reality display for surgical microscopes},
  booktitle = {2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society},
  year      = {2012},
  pages     = {1409-1412},
  month     = {Aug},
  abstract  = {Medical augmented reality has undergone much development recently. However, there is a lack of studies quantitatively comparing the different display options available. This paper compares the effects of different graphical overlay systems in a simple micromanipulation task with “soft” visual servoing. We compared positioning accuracy in a real-time visually-guided task using Micron, an active handheld tremor-canceling microsurgical instrument, using three different displays: 2D screen, 3D screen, and microscope with monocular image injection. Tested with novices and an experienced vitreoretinal surgeon, display of virtual cues in the microscope via an augmented reality injection system significantly decreased 3D error (p <; 0.05) compared to the 2D and 3D monitors when confounding factors such as magnification level were normalized.},
  doi       = {10.1109/EMBC.2012.6346203},
  issn      = {1094-687X},
  keywords  = {augmented reality;computer displays;control engineering computing;medical computing;medical robotics;micromanipulators;microscopes;robot vision;screens (display);surgery;visual servoing;2D screen;3D screen;Micron;active handheld tremor-canceling microsurgical instrument;comparative evaluation;graphical overlay systems;medical augmented reality;micromanipulation task;monocular augmented-reality display;monocular image injection;real-time visually-guided task;robotic system;soft visual servoing;surgical microscopes;virtual cues display;vitreoretinal surgeon;Accuracy;Fixtures;Microscopy;Monitoring;Surgery;Three dimensional displays;Visualization;Equipment Design;Hand;Humans;Image Processing, Computer-Assisted;Microscopy;Signal Processing, Computer-Assisted;Surgery, Computer-Assisted;Tremor;User-Computer Interface},
}

@InProceedings{6266298,
  author    = {C. Shi and B. C. Becker and C. N. Riviere},
  title     = {Inexpensive monocular pico-projector-based augmented reality display for surgical microscope},
  booktitle = {2012 25th IEEE International Symposium on Computer-Based Medical Systems (CBMS)},
  year      = {2012},
  pages     = {1-6},
  month     = {June},
  abstract  = {This paper describes an inexpensive pico-projector-based augmented reality (AR) display for a surgical microscope. The system is designed for use with Micron, an active handheld surgical tool that cancels hand tremor of surgeons to improve microsurgical accuracy. Using the AR display, virtual cues can be injected into the microscope view to track the movement of the tip of Micron, show the desired position, and indicate the position error. Cues can be used to maintain high performance by helping the surgeon to avoid drifting out of the workspace of the instrument. Also, boundary information such as the view range of the cameras that record surgical procedures can be displayed to tell surgeons the operation area. Furthermore, numerical, textual, or graphical information can be displayed, showing such things as tool tip depth in the work space and on/off status of the canceling function of Micron.},
  doi       = {10.1109/CBMS.2012.6266298},
  issn      = {1063-7125},
  keywords  = {augmented reality;biomedical equipment;surgery;AR;Micron;active handheld surgical tool;inexpensive monocular pico-projector-based augmented reality display;surgical microscope;Adaptive optics;Lenses;Microscopy;Optical filters;Optical imaging;Optical reflection;Surgery},
}

@InProceedings{5899135,
  author    = {J. Hong and S. Kim and M. Hashizume and H. Cho},
  title     = {Endoscopic image overlay surgical navigation using augmented and virtual reality technologies},
  booktitle = {2011 8th Asian Control Conference (ASCC)},
  year      = {2011},
  pages     = {574-578},
  month     = {May},
  abstract  = {Surgical navigation system using an endoscope has some limitations because of 2D endoscopic camera image. The endoscopic image has no information about spatial depth of the organs below the surface. To resolve this problem, augmented reality and virtual reality techniques were used for surgical navigation in endoscopic surgery. Phantom experiment was conducted for accuracy evaluation, and the feasibility was checked in clinical application.},
  keywords  = {augmented reality;biological organs;endoscopes;medical image processing;medical robotics;path planning;surgery;telemedicine;2D endoscopic camera image;augmented reality;endoscopic image overlay surgical navigation;organs;virtual reality;augmented reality;otologic surgery;surgical navigation},
}

@InProceedings{1548574,
  author    = {A. Pandya and G. Auner},
  title     = {Simultaneous augmented and virtual reality for surgical navigation},
  booktitle = {NAFIPS 2005 - 2005 Annual Meeting of the North American Fuzzy Information Processing Society},
  year      = {2005},
  pages     = {429-435},
  month     = {June},
  abstract  = {We use a passive articulated arm to track a calibrated end-effector mounted video camera. In real time, we can superimpose the live video view with the synchronized graphical view of CT-derived segmented object(s) of interest within a phantom skull (augmented reality (AR)) and provide the trajectory of the end-effector (translated to the focal point) in orthogonal image data scans and 3D models (VR). Augmented reality generation is a natural extension for the surgeon because it does both the 2D to 3D transformation and projects the views directly onto the patient view. However, there are distinct advantages for also having a VR (image guided surgery) view of the tools trajectory. Both AR and VR visualization have advantages and disadvantages depending on the stage of the surgery and surgeons should have the option to select. In this paper, we provide the software design and the network communication details of a multi-user, on-demand, near real-time simultaneous AR/VR system for surgical guidance.},
  doi       = {10.1109/NAFIPS.2005.1548574},
  keywords  = {augmented reality;image segmentation;medical image processing;medical robotics;surgery;systems analysis;CT-derived segmented object;augmented reality;end-effector mounted video camera;image guided surgery view;live video view;orthogonal image data scan;passive articulated arm;software design;surgical guidance;surgical navigation;synchronized graphical view;tools trajectory;virtual reality;Augmented reality;Cameras;Image segmentation;Imaging phantoms;Navigation;Skull;Surgery;Surges;Virtual reality;Visualization},
}

@InProceedings{854777,
  author    = {A. Kosaka and A. Saito and Y. Furuhashi and T. Shibasaki},
  title     = {Augmented reality system for surgical navigation using robust target vision},
  booktitle = {Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662)},
  year      = {2000},
  volume    = {2},
  pages     = {187-194 vol.2},
  abstract  = {This paper presents a robust and accurate vision-based augmented reality system for surgical navigation. The key point of our system is a robust and real-time monocular vision algorithm to estimate the 3D pose of surgical tools, utilizing specially designed code markers and Kalman filter-based position updating. The vision system is not impaired by occlusion and rapid change of illumination. The augmented reality system superimposes the 3D object wireframe onto the live viewing image taken from the surgical microscope as well as displaying other useful navigation information, while allowing the surgeons to freely change its zoom and focus for viewing. The experimental results verified the robustness and usefulness of the system, and acquired the image registration error less than 2 mm},
  doi       = {10.1109/CVPR.2000.854777},
  issn      = {1063-6919},
  keywords  = {augmented reality;image registration;medical image processing;surgery;augmented reality;image registration;monocular vision algorithm;robust target vision;surgical navigation;surgical tools;Algorithm design and analysis;Augmented reality;Kalman filters;Lighting;Machine vision;Microscopy;Navigation;Real time systems;Robustness;Surgery},
}

@InProceedings{7836497,
  author    = {N. Haouchine and M. O. Berger and S. Cotin},
  title     = {Simultaneous Pose Estimation and Augmentation of Elastic Surfaces from a Moving Monocular Camera},
  booktitle = {2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)},
  year      = {2016},
  pages     = {199-202},
  month     = {Sept},
  abstract  = {We present in this paper an original method to estimate the pose of a monocular camera while simultaneously modeling and capturing the elastic deformation of the object to be augmented. Our method tackles a challenging problem where ambiguities between rigid motion and non-rigid deformation are present. This issue represents a major lock for the establishment of an efficient surgical augmented reality where endoscopic camera moves and organs deform. Using an underlying physical model to estimate the low stressed regions our algorithm separates the rigid body motion from the elastic deformations using polar decomposition of the strain tensor. Following this decomposition, a constrained minimization, that encodes both the optical and the physical constraints, is resolved at each frame. Results on real and simulated data are exposed to show the effectiveness of our approach.},
  doi       = {10.1109/ISMAR-Adjunct.2016.0076},
  keywords  = {augmented reality;cameras;elastic deformation;image motion analysis;pose estimation;shear modulus;constrained minimization;elastic deformation;elastic surfaces;endoscopic camera;low stressed regions estimate;moving monocular camera;nonrigid deformation;optical constraints;physical constraints;physical model;polar decomposition;pose estimation;rigid body motion;strain tensor;surgical augmented reality;Cameras;Computational modeling;Pose estimation;Shape;Strain;Stress;Three-dimensional displays;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial;and virtual realities; I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling—Physically based modeling;augmented},
}

@InProceedings{1492780,
  author    = {P. R. Rizun and G. R. Sutherland},
  title     = {Surgical Laser Augmented with Haptic Feedback and Visible Trajectory},
  booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
  year      = {2005},
  pages     = {241-244},
  month     = {March},
  abstract  = {This application sketch describes a conceptual surgical laser system, designed for incorporation with a surgical robot, that provides haptic and visual feedback. Initial results from a prototype haptic laser system, built with a low-power (noncutting) laser, are also presented.},
  doi       = {10.1109/VR.2005.1492780},
  issn      = {1087-8270},
  keywords  = {data visualisation;human computer interaction;operating systems (computers);rendering (computer graphics);solid modelling;user centred design;user interfaces;virtual reality;augmented reality, haptics, lasers, surgical robotics;Distance measurement;Force feedback;Haptic interfaces;Laser feedback;Laser modes;Laser surgery;Laser theory;Prototypes;Robot sensing systems;Surface emitting lasers},
}

@InProceedings{7097222,
  title     = {Table of contents},
  booktitle = {2014 CACS International Automatic Control Conference (CACS 2014)},
  year      = {2014},
  pages     = {3-3},
  month     = {Nov},
  abstract  = {The following topics are dealt with: intelligent robot; robotic exoskeleton; time delay; humanoid robots; health-care robots; robotic manipulator; signal processing; image processing; augmented reality; surgical system; nonlinear control; robust model predictive control; human-robot interaction; service robots; biomorphic robots; and adaptive control.},
  doi       = {10.1109/CACS.2014.7097222},
  keywords  = {adaptive control;delays;intelligent control;medical control systems;nonlinear control systems;predictive control;robots;robust control;surgery;adaptive control;augmented reality;biomorphic robots;health-care robots;human-robot interaction;humanoid robots;image processing;intelligent robot;nonlinear control;robotic exoskeleton;robotic manipulator;robust model predictive control;service robots;signal processing;surgical system;time delay;Organizing},
}

@InProceedings{7970440,
  author    = {G. Ion-Eugen and R. Ionut-Cristian and B. N. George},
  title     = {Haptic devices synchronization into a software simulator},
  booktitle = {2017 18th International Carpathian Control Conference (ICCC)},
  year      = {2017},
  pages     = {440-445},
  month     = {May},
  abstract  = {In this paper is presented the design and developing process of a software simulator that allows both a pre-surgical planning and training future physician about anatomy and liver damage. For this it was made a design three-dimensional (3D) liver and was added properties of soft tissues in order to have a simulator surgery as realistic as to provide a complete answer and complex palpation by devices Geomagic Touch X and Phantom Omni. It highlighted the acute need for such simulator in medicine in general and surgery in particular, and its advantages over traditional methods of learning. Emphasis was placed on the use of virtual reality and haptic feedback to improve the realism of the simulator. The final goal was the synchronization between two or more haptic device using events to acquire more feedback information in the real time. Experiments conducted using different obj flies representing normal liver, hepatic liver, cirrhosis liver and cystic liver demonstrated the reliability and the robustness of the proposed method. The next step will involve the usage of Augmented Reality and Virtual Reality to provide training to physicians and healthcare staff.},
  doi       = {10.1109/CarpathianCC.2017.7970440},
  keywords  = {augmented reality;biomedical education;computer based training;digital simulation;force feedback;haptic interfaces;health care;liver;medicine;personnel;realistic images;surgery;synchronisation;3D liver design;Geomagic Touch X;Phantom Omni;anatomy;augmented reality;cirrhosis liver;complex palpation;cystic liver;feedback information;haptic device synchronization;haptic feedback;healthcare staff;hepatic liver;liver damage;medicine;physician training;pre-surgical planning;realistic surgery simulator;reliability;soft tissues;software simulator;three-dimensional liver design;virtual reality;algorithms;behavior model;collision detection;force feedback;haptic device;haptic rendering;software simulator},
}

@InProceedings{7946843,
  author    = {H. Ghandorh and J. Mackenzie and R. Eagleson and S. de Ribaupierre},
  title     = {Development of augmented reality training simulator systems for neurosurgery using model-driven software engineering},
  booktitle = {2017 IEEE 30th Canadian Conference on Electrical and Computer Engineering (CCECE)},
  year      = {2017},
  pages     = {1-6},
  month     = {April},
  abstract  = {Neurosurgical procedures are complicated processes, providing challenges and demands ranging from medical knowledge and judgment to the neurosurgeons dexterity and perceptual capacities. Deliberate training of common neurosurgical procedures and underlying tasks is extremely important. One effective method for the training is to enhance the required surgical training tasks through the use of neurosurgical simulators. Development of neurosurgical simulators is challenging due to many reasons. In this work, we proposed to facilitate the development of new augmented reality neurosurgical simulator systems through the adoption of model-driven engineering. Our developed systems involve the interactive visualization of three-dimension brain meshes in order to train users and simulate a targeting task towards a variety of predetermined virtual targets. We present our results in a way which highlights two new design artifacts through our MDE approach.},
  doi       = {10.1109/CCECE.2017.7946843},
  keywords  = {augmented reality;data visualisation;medical diagnostic computing;software architecture;surgery;MDE approach;augmented reality training simulator systems;design artifacts;interactive visualization;medical knowledge;model-driven software engineering;neurosurgeons dexterity;neurosurgical procedures;perceptual capacities;predetermined virtual targets;surgical training tasks;three-dimension brain meshes;Computational modeling;Computers;Neurosurgery;Software;Three-dimensional displays;Training;Unity3D;Vuforia;augmented reality;external ventricular drain;model-driven engineering;neurosurgery;simulator;targeting task},
}

@InProceedings{7943058,
  author    = {E. Burke and P. Felle and C. Crowley and J. Jones and E. Mangina and A. G. Campbell},
  title     = {Augmented reality EVAR training in mixed reality educational space},
  booktitle = {2017 IEEE Global Engineering Education Conference (EDUCON)},
  year      = {2017},
  pages     = {1571-1579},
  month     = {April},
  abstract  = {The future of surgical training is changing. One of the bottlenecks in the training of surgeons is the availability of either real patients while being supervised or cadavers. Both these resources can be in limited supply. Surgical trainers have been a common solution to this problem but can be expensive thus limiting their use in the developing world. This paper will examine how using Augmented Reality (AR), Virtual Reality (VR) and inexpensive 3D printing can act as a replacement to expensive training devices for sole purposes. For the work described in this paper, the EndoVascular Aneurysm Repair (EVAR) operation will be examined to explore whether medical training for young surgeons can be improved with the integration of VR/AR. EVAR operation is an innovative, less invasive procedure used to treat problems affecting the blood vessels, such as an aneurysm, which is a swelling or “ballooning” of the blood vessel. The surgery involves making a small incision near each hip to access the blood vessels. The paper will outline an approach as a proof of concept, in order to create a mixed reality training tool that through the use of 3D printing can give tactile feedback to the trainee during training.},
  doi       = {10.1109/EDUCON.2017.7943058},
  keywords  = {augmented reality;blood vessels;computer based training;haptic interfaces;medical computing;surgery;three-dimensional printing;3D printing;augmented reality EVAR training;blood vessels;endovascular aneurysm repair operation;medical training;mixed reality educational space;mixed reality training tool;surgery;tactile feedback;virtual reality;Biomedical imaging;Computed tomography;Solid modeling;Surgery;Three-dimensional displays;Training;Virtual reality;Augmented Reality & Education;Innovation for Medical Training;Virtual Reality & Education},
}

@Article{7181689,
  author   = {S. Kumar and P. Singhal and V. N. Krovi},
  title    = {Computer-Vision-Based Decision Support in Surgical Robotics},
  journal  = {IEEE Design Test},
  year     = {2015},
  volume   = {32},
  number   = {5},
  pages    = {89-97},
  month    = {Oct},
  issn     = {2168-2356},
  abstract = {With the rapid adoption of laparoscopic robotic surgery, numerous unanticipated safety and reliability challenges have surfaced for teleoperated devices. A large fraction of the procedure failures are accounted due to the sensor depravation, limited field of view, and lack of planning during procedures. This article surveys the benefits of computer vision for preoperative, intraoperative, and postoperative surgical stages to assist with planning; tool detection, identification, pose tracking, and augmented reality; and surgical skill assessment and retrospective analysis of the procedure. The appropriate use of these computer vision techniques has the potential to improve the safety and efficacy of robotic surgery as it becomes more commonplace.},
  doi      = {10.1109/MDAT.2015.2465135},
  keywords = {augmented reality;control engineering computing;decision support systems;medical computing;medical robotics;object tracking;robot vision;surgery;telerobotics;augmented reality;computer vision based decision support;identification;intraoperative surgical stage;laparoscopic robotic surgery;pose tracking;postoperative surgical stage;preoperative surgical stage;retrospective analysis;surgical robotics;surgical skill assessment;teleoperated devices;tool detection;Computer vision;Medical robots;Robot sensing systems;Surgery;Visualization;Computer Integrated Surgery;Computer Vision;Decision Support;Operator Interfaces},
}

@InProceedings{7866296,
  author    = {S. V. Brecht and Y. S. Krieger and J. U. Stolzenburg and T. C. Lueth},
  title     = {A new concept for a Single Incision Laparoscopic Manipulator System integrating intraoperative Laparoscopic Ultrasound},
  booktitle = {2016 IEEE International Conference on Robotics and Biomimetics (ROBIO)},
  year      = {2016},
  pages     = {51-56},
  month     = {Dec},
  abstract  = {Minimally invasive surgery offers the benefits of reduced patient trauma, faster recovery and shorter hospital stay to the patients, what motivates its development towards further less invasive applications. In this work, we present a concept for adapting our 3D-printed overtube manipulator to single incision laparoscopic surgery. Furthermore, we describe the integration of a surgeon supporting ultrasound imaging into the manipulator system, which can be used to supply augmented reality features. With our concept, we aim to meet the demand for systems that enable enhanced tissue manipulation as well as the request for the supply of additional information for the surgeon regarding the inside of organs and tissue structures in order to improve the surgical outcome.},
  doi       = {10.1109/ROBIO.2016.7866296},
  keywords  = {augmented reality;manipulators;surgery;3D-printed overtube manipulator;augmented reality features;hospital;intraoperative laparoscopic ultrasound;minimally invasive surgery;patients;reduced patient trauma;single incision laparoscopic manipulator system;single incision laparoscopic surgery;surgeon;surgical outcome;tissue manipulation;tissue structures;ultrasound imaging;Laparoscopes;Manipulators;Probes;Surgery;Tumors;Ultrasonic imaging},
}

@InProceedings{7556132,
  author    = {V. Ferrari and E. M. Calabrò},
  title     = {Wearable light field optical see-through display to avoid user dependent calibrations: A feasibility study},
  booktitle = {2016 SAI Computing Conference (SAI)},
  year      = {2016},
  pages     = {1211-1216},
  month     = {July},
  abstract  = {Wearable augmented reality (AR) is a promising technology for surgical navigation and also for non-medical tasks. Optical see-through displays allow a direct view of the real world augmented with patient related virtual information, which is usually projected on semi-transparent displays placed in front of the eyes. A user-dependent display calibration is required to guarantee a coherent alignment between the virtual information projected on the display and the light rays of the real light field perceived by the user. Integral imaging (II) is a known approach to synthetically generate a light field. A semi-transparent mirror placed at 45° in front of an II display allows obtaining a light field see-through display. In this approach real and virtual information are perceived as coherently aligned independently to the user. In this work we used geometric optics rules to design wearable AR displays, based on II and a semi-transparent mirror, with an optimized visual quality and a tolerated range of movement of the eye in respect to the display. An early implementation demonstrates the feasibility of the proposed solution to implement optical see-through displays that not require user dependent calibrations.},
  doi       = {10.1109/SAI.2016.7556132},
  keywords  = {augmented reality;calibration;medical image processing;surgery;geometric optics rules;integral imaging;nonmedical tasks;patient related virtual information;semitransparent displays;semitransparent mirror;surgical navigation;user-dependent display calibration;visual quality;wearable AR displays;wearable augmented reality;wearable light field optical see-through display;Biomedical optical imaging;Calibration;Cameras;Mirrors;Optical distortion;Optical imaging;Two dimensional displays;Augmented Reality;Integral Imaging;Light Field Displays;See-through Display;Surgical Navigation},
}

@InProceedings{7523651,
  author    = {N. Evangeliou and A. Tzes},
  title     = {Development of an SMA-actuated redundant robotic platform for minimally invasive surgery},
  booktitle = {2016 6th IEEE International Conference on Biomedical Robotics and Biomechatronics (BioRob)},
  year      = {2016},
  pages     = {353-358},
  month     = {June},
  abstract  = {The design, fabrication and testing of an SMA-actuated redundant robotic-probe for minimally invasive surgical procedures is the subject of this article. The resulting probe is lightweight, modular and its controller relies on an antagonistic principle for each pair of tendons. The developed platform consists of a twelve Degree of Freedom snake-like probe attached to a pan-tilt mechanized unit. The probe's dimensions are fit for typical ablations and endoscopic operations. An augmented reality environment is developed, coupling haptic feedback to visual servoing techniques under the assumption that the surgeon uses a HID-device (virtual reality headset) for assistance during the operation.},
  doi       = {10.1109/BIOROB.2016.7523651},
  keywords  = {augmented reality;endoscopes;medical computing;medical robotics;surgery;visual servoing;HID-device;SMA-actuated redundant robotic platform;SMA-actuated redundant robotic-probe;ablations;antagonistic principle;augmented reality environment;endoscopic operations;haptic feedback coupling;minimally invasive surgery;minimally invasive surgical procedures;pan-tilt mechanized unit;twelve degree of freedom snake-like probe;virtual reality headset;visual servoing;Actuators;Bandwidth;Probes;Resistance;Robots;Tendons;Wires},
}

@InProceedings{7399146,
  author    = {A. Kumar and Y. Y. Wang and K. C. Liu and S. W. Huang and W. N. Lie and C. C. Huang},
  title     = {Stereoscopic Augmented Reality for Single Camera Endoscope Using Optical Tracker: A Study on Phantom},
  booktitle = {2015 Third International Conference on Robot, Vision and Signal Processing (RVSP)},
  year      = {2015},
  pages     = {55-58},
  month     = {Nov},
  abstract  = {Introduction: Endoscopic surgery causes reduced injury to tissues which helps in rapid recovery and less painful post-operative period of patients. However, its narrow field of view and loss of depth perception in endoscope image make surgeon's task difficult. Moreover, in endoscopic surgery surgeons can only see the surface of the surgical anatomy. In this study these limitations have been addressed with the development of an augmented reality system with 3D visualization. Method: The system is comprising of infrared based optical tracker, 2D endoscope system, computer and a 3D monitor. The system was calibrated to bring all the components to a single reference frame of the optical tracker. A phantom and its 3D CT model were used, respectively, to form the real and virtual part of the augmented reality. The endoscope camera position was tracked in 3D using a tracker tool mounted on it. The camera in the virtual environment was updated with the new positions of the endoscope camera. The endoscope video frame and the virtual model rendered image were superimposed to form augmented reality view. The depth map of the virtual scene was further used to make an stereo pair of the augmented reality. Results: The AR system was examined with endoscope video of phantom. It could produce well aligned real and virtual component. The contour different of the two components was 5 to 10 mm. Conclusion: An AR system for endoscopic surgery was successfully implemented on a phantom. It needs to include motion and deformation model to be applied on real patients.},
  doi       = {10.1109/RVSP.2015.22},
  keywords  = {augmented reality;data visualisation;endoscopes;infrared imaging;medical image processing;optical tracking;phantoms;rendering (computer graphics);stereo image processing;surgery;2D endoscope system;3D CT model;3D visualization;endoscope camera position tracking;endoscope video frame;endoscopic surgery;infrared based optical tracker;phantom;single camera endoscope;stereoscopic augmented reality;virtual model rendered image;virtual scene depth map;Augmented reality;Cameras;Endoscopes;Solid modeling;Surgery;Three-dimensional displays;Visualization;augmented reality;endoscopic surgery;stereo endoscopy},
}

@InProceedings{7379332,
  author    = {G. Barresi and E. Olivieri and D. G. Caldwell and L. S. Mattos},
  title     = {Brain-Controlled AR Feedback Design for User's Training in Surgical HRI},
  booktitle = {2015 IEEE International Conference on Systems, Man, and Cybernetics},
  year      = {2015},
  pages     = {1116-1121},
  month     = {Oct},
  abstract  = {Brain-computer interfaces (BCIs) offer high potential for enhancing training in many tasks, especially those that require maintaining high levels of concentration such as surgery. Training focus and attention can play a critical role in surgery since concentration on the task at hand is fundamental to prevent life-threatening errors. In this paper we propose a new method for concentration training in the context of robot-assisted laser microsurgery associated to a feedback design that makes the interaction more intuitive. This approach couples augmented reality (AR) features to both BCI-based on-line measurement of the user's mental focus and the control of the surgical robot. The methodology is described as a brain-controlled augmented reality (BcAR) training system. AR is used to maintain the surgeon's perceptual contact with the real operating setting, while focus stimulation is provided by modifying features of an AR item based on real-time monitoring of the user's mental state. In this research a low-cost EEG device is used and the BcAR is implemented in the form of an AR scalpel that behaves as a "retractable" knife according to the user's mental focus: low concentration levels retract the knife and prevent cutting. This design provides directional compatibility between the AR feedback animation and the spontaneous motion of user's attention along the AR tool, resulting in an intuitive system with real impact on the training outcome. This is demonstrated through user trials and comparison with training based on simple AR feedback (no EEG). Results demonstrate the potential of the approach, showing a significant improvement in post-training task execution time without any detriment to user experience. Subjective questionnaires also confirmed the critical role of directional compatibility in the AR feedback. Such findings allow the identification of further improvements and novel potential applications of this interaction paradigm.},
  doi       = {10.1109/SMC.2015.200},
  keywords  = {augmented reality;brain-computer interfaces;computer animation;electroencephalography;feedback;human-robot interaction;laser applications in medicine;medical robotics;surgery;AR features;AR feedback animation;BCI-based on-line measurement;BcAR training system;augmented reality features;brain-computer interfaces;brain-controlled AR feedback design;brain-controlled augmented reality training system;directional compatibility;feedback design;life-threatening errors;low-cost EEG device;post-training task execution time;real-time user mental state monitoring;robot-assisted laser microsurgery;surgeon perceptual contact;surgical HRI;user mental focus;user training;Animation;Augmented reality;Electroencephalography;Robots;Surgery;Training;Trajectory;Augmented Reality;Brain-Computer Interfaces;Human-Robot Interaction;Surgery;Training;User Interfaces},
}

@Article{7150398,
  author   = {M. S. Nosrati and R. Abugharbieh and J. M. Peyrat and J. Abinahed and O. Al-Alao and A. Al-Ansari and G. Hamarneh},
  title    = {Simultaneous Multi-Structure Segmentation and 3D Nonrigid Pose Estimation in Image-Guided Robotic Surgery},
  journal  = {IEEE Transactions on Medical Imaging},
  year     = {2016},
  volume   = {35},
  number   = {1},
  pages    = {1-12},
  month    = {Jan},
  issn     = {0278-0062},
  abstract = {In image-guided robotic surgery, segmenting the endoscopic video stream into meaningful parts provides important contextual information that surgeons can exploit to enhance their perception of the surgical scene. This information provides surgeons with real-time decision-making guidance before initiating critical tasks such as tissue cutting. Segmenting endoscopic video is a challenging problem due to a variety of complications including significant noise attributed to bleeding and smoke from cutting, poor appearance contrast between different tissue types, occluding surgical tools, and limited visibility of the objects' geometries on the projected camera views. In this paper, we propose a multi-modal approach to segmentation where preoperative 3D computed tomography scans and intraoperative stereo-endoscopic video data are jointly analyzed. The idea is to segment multiple poorly visible structures in the stereo/multichannel endoscopic videos by fusing reliable prior knowledge captured from the preoperative 3D scans. More specifically, we estimate and track the pose of the preoperative models in 3D and consider the models' non-rigid deformations to match with corresponding visual cues in multi-channel endoscopic video and segment the objects of interest. Further, contrary to most augmented reality frameworks in endoscopic surgery that assume known camera parameters, an assumption that is often violated during surgery due to non-optimal camera calibration and changes in camera focus/zoom, our method embeds these parameters into the optimization hence correcting the calibration parameters within the segmentation process. We evaluate our technique on synthetic data, ex vivo lamb kidney datasets, and in vivo clinical partial nephrectomy surgery with results demonstrating high accuracy and robustness.},
  doi      = {10.1109/TMI.2015.2452907},
  keywords = {biological tissues;cameras;computerised tomography;image segmentation;kidney;medical image processing;medical robotics;pose estimation;surgery;3D computed tomography scans;3D nonrigid pose estimation;camera focus-zoom;camera views;endoscopic video stream;image-guided robotic surgery;intraoperative stereo-endoscopic video data;lamb kidney;multistructure segmentation;nonoptimal camera calibration;nonrigid deformations;partial nephrectomy surgery;real-time decision-making guidance;stereo-multichannel endoscopic videos;surgical tools;tissue cutting;Cameras;Image segmentation;Optimization;Robots;Shape;Surgery;Three-dimensional displays;3D to 2D non-rigid pose estimation;Segmentation;augmented reality;endoscopy;robotic surgery},
}

@InProceedings{7328071,
  author    = {X. Wang and S. Habert and M. Ma and C. H. Huang and P. Fallavollita and N. Navab},
  title     = {[POSTER] RGB-D/C-arm Calibration and Application in Medical Augmented Reality},
  booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
  year      = {2015},
  pages     = {100-103},
  month     = {Sept},
  abstract  = {Calibration and registration are the first steps for augmented reality and mixed reality applications. In the medical field, the calibration between an RGB-D camera and a mobile C-arm fluoroscope is a new topic which introduces challenges. In this paper, we propose a precise 3D/2D calibration method to achieve a video augmented fluoroscope. With the design of a suitable calibration phantom for RGB-D/C-arm calibration, we calculate the projection matrix from the depth camera coordinates to the X-ray image. Through a comparison experiment by combining different steps leading to the calibration, we evaluate the effect of every step of our calibration process. Results demonstrated that we obtain a calibration RMS error of 0.54±1.40 mm which is promising for surgical applications. We conclude this paper by showcasing two clinical applications. One is a markerless registration application, the other is an RGB-D camera augmented mobile C-arm visualization.},
  doi       = {10.1109/ISMAR.2015.31},
  keywords  = {augmented reality;diagnostic radiography;image colour analysis;image registration;medical image processing;surgery;3D-2D calibration method;RGB-D camera;RGB-D/C-arm calibration;X-ray image;markerless registration application;medical augmented reality;mixed reality application;mobile C-arm fluoroscope;red-green-blue-depth camera;surgical application;video augmented fluoroscope;Biomedical imaging;Calibration;Cameras;Distortion;Sensors;Three-dimensional displays;X-ray imaging},
}

@InProceedings{7328052,
  author    = {F. Bork and B. Fuers and A. K. Schneider and F. Pinto and C. Graumann and N. Navab},
  title     = {Auditory and Visio-Temporal Distance Coding for 3-Dimensional Perception in Medical Augmented Reality},
  booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
  year      = {2015},
  pages     = {7-12},
  month     = {Sept},
  abstract  = {Image-guided medical interventions more frequently rely on Augmented Reality (AR) visualization to enable surgical navigation. Current systems use 2-D monitors to present the view from external cameras, which does not provide an ideal perception of the 3-D position of the region of interest. Despite this problem, most research targets the direct overlay of diagnostic imaging data, and only few studies attempt to improve the perception of occluded structures in external camera views. The focus of this paper lies on improving the 3-D perception of an augmented external camera view by combining both auditory and visual stimuli in a dynamic multi-sensory AR environment for medical applications. Our approach is based on Temporal Distance Coding (TDC) and an active surgical tool to interact with occluded virtual objects of interest in the scene in order to gain an improved perception of their 3-D location. Users performed a simulated needle biopsy by targeting virtual lesions rendered inside a patient phantom. Experimental results demonstrate that our TDC-based visualization technique significantly improves the localization accuracy, while the addition of auditory feedback results in increased intuitiveness and faster completion of the task.},
  doi       = {10.1109/ISMAR.2015.16},
  keywords  = {augmented reality;cameras;data visualisation;medical computing;patient diagnosis;surgery;3-Dimensional Perception;TDC-based visualization technique;active surgical tool;auditory;camera;diagnostic imaging data;dynamic multisensory AR environment;image-guided medical intervention;medical augmented reality visualization;patient phantom;surgical navigation;visio-temporal distance coding;Accuracy;Biopsy;Encoding;Lesions;Needles;Shape;Visualization;Auditory and Visual Stimuli;Medical Augmented Reality;Multi-Sensory Environment;Temporal Distance Coding},
}

@InProceedings{7319495,
  author    = {E. Olivieri and G. Barresi and L. S. Mattos},
  title     = {BCI-based user training in surgical robotics},
  booktitle = {2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
  year      = {2015},
  pages     = {4918-4921},
  month     = {Aug},
  abstract  = {Human error is a critical risk in surgery, so an important aim of surgical robotic systems is to improve the performance and the safety of surgical operations. Such systems can be potentially enhanced by a brain-computer interface (BCI) able to monitor the user's mental focus and use this information to improve the level of safety of the procedures. In order to evaluate such potential usage of BCIs, this paper describes a novel framework for training the user to regulate his/her own mental state while performing surgery-like tasks using a robotic system. This self-regulation is based on augmented reality (AR) feedback representing the BCI-monitored mental state, which helps the user's effort in maintaining a high level of mental focus during the task. A comparison between a BCI-based training and a training without a BCI highlighted a reduction of post-training trial times as a result of the enhanced training setup, without any loss in performance or in user experience. Such finding allows the identification of further improvements and novel potential applications of this training and interaction paradigm.},
  doi       = {10.1109/EMBC.2015.7319495},
  issn      = {1094-687X},
  keywords  = {augmented reality;brain-computer interfaces;computer based training;medical robotics;surgery;AR feedback;BCI-based user training;augmented reality;brain-computer interface;surgical robotic systems;Augmented reality;Haptic interfaces;Measurement;Robots;Safety;Surgery;Training},
}

@InProceedings{7148129,
  author    = {B. Wu and S. H. Sim and A. Enquobahrie and R. Ortiz},
  title     = {Effects of visual latency on visual-haptic experience of stiffness},
  booktitle = {2015 Seventh International Workshop on Quality of Multimedia Experience (QoMEX)},
  year      = {2015},
  pages     = {1-6},
  month     = {May},
  abstract  = {In multimodal virtual-reality, augmented-reality, and tele-operation systems, a temporal asynchrony often exists between visual and haptic feedback due to differences in processing and rendering the two types of signals. We have conducted two psychophysical experiments to examine how such asynchrony influences our perceptual experience with an object's stiffness. Participants explored a virtual elastic material using a haptic interface, and saw the deformation of the material in a simulated ultrasound that was displayed with a constant or variable latency relative to the haptic feedback. Their perception of stiffness and ability to differentiate stiffness were measured. The results showed that the perceived stiffness increased with the visual latency while the differentiation threshold was little influenced. When the visual latency was variable, the effects were reduced and participants relied more on the haptic sensations. Such effects will be further evaluated in clinical settings using a neural surgical simulator.},
  doi       = {10.1109/QoMEX.2015.7148129},
  keywords  = {augmented reality;haptic interfaces;asynchrony influences;augmented-reality;differentiation threshold;haptic feedback;haptic interface;haptic sensations;material deformation;multimodal virtual-reality;neural surgical simulator;object stiffness;perceived stiffness;perceptual experience;psychophysical experiments;tele-operation systems;temporal asynchrony;variable latency;virtual elastic material;visual feedback;visual latency;visual-haptic experience;Atmospheric measurements;Delays;Force;Haptic interfaces;Springs;Ultrasonic imaging;Visualization;delay;perceptual experience;psychophysical evaluation;stiffness;visual-haptic},
}

@Article{6998039,
  author   = {K. Abhari and J. S. H. Baxter and E. C. S. Chen and A. R. Khan and T. M. Peters and S. de Ribaupierre and R. Eagleson},
  title    = {Training for Planning Tumour Resection: Augmented Reality and Human Factors},
  journal  = {IEEE Transactions on Biomedical Engineering},
  year     = {2015},
  volume   = {62},
  number   = {6},
  pages    = {1466-1477},
  month    = {June},
  issn     = {0018-9294},
  abstract = {Planning surgical interventions is a complex task, demanding a high degree of perceptual, cognitive, and sensorimotor skills to reduce intra- and post-operative complications. This process requires spatial reasoning to coordinate between the preoperatively acquired medical images and patient reference frames. In the case of neurosurgical interventions, traditional approaches to planning tend to focus on providing a means for visualizing medical images, but rarely support transformation between different spatial reference frames. Thus, surgeons often rely on their previous experience and intuition as their sole guide is to perform mental transformation. In case of junior residents, this may lead to longer operation times or increased chance of error under additional cognitive demands. In this paper, we introduce a mixed augmented-/virtual-reality system to facilitate training for planning a common neurosurgical procedure, brain tumour resection. The proposed system is designed and evaluated with human factors explicitly in mind, alleviating the difficulty of mental transformation. Our results indicate that, compared to conventional planning environments, the proposed system greatly improves the nonclinicians' performance, independent of the sensorimotor tasks performed (p <; 0.01). Furthermore, the use of the proposed system by clinicians resulted in a significant reduction in time to perform clinically relevant tasks (p <; 0.05). These results demonstrate the role of mixed-reality systems in assisting residents to develop necessary spatial reasoning skills needed for planning brain tumour resection, improving patient outcomes.},
  doi      = {10.1109/TBME.2014.2385874},
  keywords = {augmented reality;biomedical education;brain;computer aided instruction;human factors;medical computing;surgery;training;tumours;virtual reality;brain tumour resection;cognitive skills;conventional planning environment;human factors;intraoperative complications;junior residents;mixed augmented reality-virtual reality system;neurosurgical interventions;neurosurgical procedure;nonclinician performance;patient reference frames;perceptual skills;post operative complications;preoperatively acquired medical images;sensorimotor skills;spatial reasoning skills;surgeons;surgical intervention planning;tumour resection planning training;Head;Phantoms;Planning;Rendering (computer graphics);Surgery;Tumors;Visualization;Augmented reality (AR);Augmented-Reality, Virtual-Reality, Neurosurgical Planning, Neurosurgical Training, Tumour Resection, User Study, Human Factors;Human Factors;Neurosurgical Planning;Neurosurgical Training;Tumour Resection;User Study;Virtual-Reality;human factors;neurosurgical planning;neurosurgical training;tumour resection;user study;virtual reality (VR);Female;Head;Human Engineering;Humans;Imaging, Three-Dimensional;Male;Neurosurgical Procedures;Phantoms, Imaging;Surgery, Computer-Assisted;User-Computer Interface},
}

@InProceedings{7057421,
  author    = {C. Lee and M. Jeon and C. Kim},
  title     = {Advanced intraoperative surgical photoacoustic microscopy},
  booktitle = {2014 11th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)},
  year      = {2014},
  pages     = {59-62},
  month     = {Nov},
  abstract  = {We developed an advanced intraoperative surgical photoacoustic microscopy (AISPAM) system by fusing a laser-scanning based optical-resolution photoacoustic microscopy (LS-OR-PAM) with 1064 nm pulsed laser, augmented reality device, and commercial surgical microscope. By sharing the same optical path of sample part in the LS-OR-PAM and microscope system, a cross-sectional PAM and microscope images were acquired at the same time. In addition, using a small-sized commercial beam projector and simple optical device, we have projected cross-sectional PAM images on microscope view plane as augmented reality. It does not require additional display tool for the PAM image monitoring. Therefore, surgeons can monitor both 2D PAM and microscopic images via an ocular lens of the microscope without movement of their sights during surgeries. To demonstrate the feasibility of the AISPAM system, needle intervention in melanoma tumor was successfully monitored in vivo.},
  doi       = {10.1109/URAI.2014.7057421},
  keywords  = {biomedical optical imaging;biomedical ultrasonics;laser applications in medicine;optical microscopy;photoacoustic effect;surgery;tumours;advanced intraoperative surgical photoacoustic microscopy;augmented reality device;laser-scanning based optical-resolution photoacoustic microscopy;melanoma tumor;ocular lens;pulsed laser;surgical microscope;Biomedical optical imaging;Laser beams;Microscopy;Optical imaging;Optical microscopy;Surgery;Tomography;Augmented reality;Photoascoustic microscopy;Surgical microscopy},
}

@InProceedings{6974537,
  author    = {P. Rapp and O. Sawodny and C. Tarín and C. R. Pech and J. Mischinger and C. Schwentner},
  title     = {A concept for a novel surgical navigation system},
  booktitle = {2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
  year      = {2014},
  pages     = {3884-3889},
  month     = {Oct},
  abstract  = {A new concept for surgical navigation in minimally-invasive interventions is presented, which allows an enhanced orientation of the surgeon with reduced cost and system complexity. The navigation system takes advantage of recorded image data, including the preoperatively computed tomography (CT) and magnetic resonance tomography (MRT) data, which are registered and segmented in order to obtain relevant biological markers, as well as the intraoperative camera data. A device for absolute positioning, included in the laparoscope and consisting of an acoustic indoor localization with a supporting inertial measurement unit (IMU), fuses these different navigation data for reliable 6 degree-of-freedom (DOF) position and orientation estimation. With those navigation information of the laparoscope, the landmarks of the preoperative data are localized on the intraoperative camera images via augmented reality and thus the site of the relevant features (e. g., a carcinoma) is determined.},
  doi       = {10.1109/SMC.2014.6974537},
  issn      = {1062-922X},
  keywords  = {biomedical MRI;computerised tomography;image sensors;medical image processing;surgery;CT data;IMU;MRT data;acoustic indoor localization;computed tomography;inertial measurement unit;intraoperative camera data;laparoscope;magnetic resonance tomography;minimally-invasive interventions;recorded image data;surgical navigation system;Cameras;Computed tomography;Estimation;Instruments;Laparoscopes;Navigation;Surgery},
}

@InProceedings{6944908,
  author    = {I. Cheng and R. Shen and R. Moreau and V. Brizzi and N. Rossol and A. Basu},
  title     = {An augmented reality framework for optimization of computer assisted navigation in endovascular surgery},
  booktitle = {2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society},
  year      = {2014},
  pages     = {5647-5650},
  month     = {Aug},
  abstract  = {Endovascular surgery is performed by placing a catheter through blood vessels. Due to the fragility of arteries and the difficulty in controlling a long elastic wire to reach the target region, training plays an extremely important role in helping a surgeon acquire the required complex skills. Virtual reality simulators and augmented reality systems have proven to be effective in minimally invasive surgical training. These systems, however, often employ pre-captured or computer-generated medical images. We have developed an augmented reality system for ultrasound-guided endovascular surgical training, where real ultrasound images captured during the procedure are registered with a pre-scanned phantom model to give the operator a realistic experience. Our goal is to extend the planning and training environment to deliver a system for computer assisted remote endovascular surgery where the navigation of a catheter can be controlled through a robotic device based on the guidance provided by an endovascular surgeon.},
  doi       = {10.1109/EMBC.2014.6944908},
  issn      = {1094-687X},
  keywords  = {augmented reality;biomedical ultrasonics;blood vessels;catheters;optimisation;phantoms;surgery;arteries;augmented reality framework;blood vessels;catheter;computer assisted navigation optimization;computer generated medical images;endovascular surgery;minimally invasive surgical training;prescanned phantom model;ultrasound guided endovascular surgical training;virtual reality simulators;Arteries;Biomedical imaging;Catheters;Navigation;Surgery;Three-dimensional displays;Training},
}

@InProceedings{6948434,
  author    = {T. Collins and D. Pizarro and A. Bartoli and M. Canis and N. Bourdel},
  title     = {Computer-Assisted Laparoscopic myomectomy by augmenting the uterus with pre-operative MRI data},
  booktitle = {2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
  year      = {2014},
  pages     = {243-248},
  month     = {Sept},
  abstract  = {An active research objective in Computer Assisted Intervention (CAI) is to develop guidance systems to aid surgical teams in laparoscopic Minimal Invasive Surgery (MIS) using Augmented Reality (AR). This involves registering and fusing additional data from other modalities and overlaying it onto the laparoscopic video in realtime. We present the first AR-based image guidance system for assisted myoma localisation in uterine laparosurgery. This involves a framework for semi-automatically registering a pre-operative Magnetic Resonance Image (MRI) to the laparoscopic video with a deformable model. Although there has been several previous works involving other organs, this is the first to tackle the uterus. Furthermore, whereas previous works perform registration between one or two laparoscopic images (which come from a stereo laparoscope) we show how to solve the problem using many images (e.g. 20 or more), and show that this can dramatically improve registration. Also unlike previous works, we show how to integrate occluding contours as registration cues. These cues provide powerful registration constraints and should be used wherever possible. We present retrospective qualitative results on a patient with two myomas and quantitative semi-synthetic results. Our multi-image framework is quite general and could be adapted to improve registration in other organs with other modalities such as CT.},
  doi       = {10.1109/ISMAR.2014.6948434},
  keywords  = {augmented reality;biomedical MRI;image registration;medical image processing;surgery;video signal processing;AR;AR-based image guidance system;CAI;MIS;augmented reality;computer assisted intervention;computer-assisted laparoscopic myomectomy;deformable model;image registration;laparoscopic minimal invasive surgery;laparoscopic video;magnetic resonance imaging;preoperative MRI data;Deformable models;Laparoscopes;Magnetic resonance imaging;Solid modeling;Surgery;Three-dimensional displays;Transforms},
}

@InProceedings{6943639,
  author    = {M. Hollensteiner and D. Fuerst and A. Schrempf},
  title     = {Artificial muscles for a novel simulator in minimally invasive spine surgery},
  booktitle = {2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society},
  year      = {2014},
  pages     = {506-509},
  month     = {Aug},
  abstract  = {Vertebroplasty and kyphoplasty are commonly used minimally invasive methods to treat vertebral compression fractures. Novice surgeons gather surgical skills in different ways, mainly by “learning by doing” or training on models, specimens or simulators. Currently, a new training modality, an augmented reality simulator for minimally invasive spine surgeries, is going to be developed. An important step in investigating this simulator is the accurate establishment of artificial tissues. Especially vertebrae and muscles, reproducing a comparable haptical feedback during tool insertion, are necessary. Two artificial tissues were developed to imitate natural muscle tissue. The axial insertion force was used as validation parameter. It appropriates the mechanical properties of artificial and natural muscles. Validation was performed on insertion measurement data from fifteen artificial muscle tissues compared to human muscles measurement data. Based on the resulting forces during needle insertion into human muscles, a suitable material composition for manufacturing artificial muscles was found.},
  doi       = {10.1109/EMBC.2014.6943639},
  issn      = {1094-687X},
  keywords  = {augmented reality;biomechanics;biomedical education;biomimetics;bone;computer based training;continuing professional development;educational aids;feedback;fracture;haptic interfaces;medical computing;medical control systems;muscle;needles;neurophysiology;surgery;artificial muscle manufacturing;artificial muscle material composition;artificial muscle mechanical properties;artificial tissue development;augmented reality simulator;axial insertion force;haptical feedback;human muscles measurement data;insertion measurement data;kyphoplasty;minimally invasive spine surgery simulator;natural muscle mechanical properties;natural muscle tissue imitation;needle insertion forces;novice surgeon learning-by-doing;novice surgeon training modality;surgical skills;tool insertion;validation parameter;vertebrae tissues;vertebral compression fracture treatment;vertebroplasty;Force;Force measurement;Haptic interfaces;Muscles;Needles;Surgery;Training;artificial soft tissues;needle insertion force;patient simulator;vertebro-/kyphoplasty},
}

@InProceedings{6943635,
  author    = {S. Parrini and F. Cutolo and C. Freschi and M. Ferrari and V. Ferrari},
  title     = {Augmented reality system for freehand guide of magnetic endovascular devices},
  booktitle = {2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society},
  year      = {2014},
  pages     = {490-493},
  month     = {Aug},
  abstract  = {Magnetic guide of endovascular devices or magnetized therapeutic microparticles to the specific target in the arterial tree is increasingly studied, since it could improve treatment efficacy and reduce side effects. Most proposed systems use external permanent magnets attached to robotic manipulators or magnetic resonance imaging (MRI) systems to guide internal carriers to the region of treatment. We aim to simplify this type of procedures, avoiding or reducing the need of robotic arms and MRI systems in the surgical scenario. On account of this we investigated the use of a wearable stereoscopic video see-through augmented reality system to show the hidden vessel to the surgeon; in this way, the surgeon is able to freely move the external magnet, following the showed path, to lead the endovascular magnetic device towards the desired position. In this preliminary study, we investigated the feasibility of such an approach trying to guide a magnetic capsule inside a vascular mannequin. The high rate of success and the positive evaluation provided by the operators represent a good starting point for further developments of the system.},
  doi       = {10.1109/EMBC.2014.6943635},
  issn      = {1094-687X},
  keywords  = {augmented reality;biomagnetism;biomedical equipment;diseases;magnetic devices;medical image processing;permanent magnets;phantoms;stereo image processing;surgery;video signal processing;MRI systems;arterial tree;external magnet;hidden vessel;magnetic capsule;magnetic endovascular devices;magnetic resonance imaging systems;magnetized therapeutic microparticles;permanent magnets;robotic manipulators;side effects;surgical scenario;vascular mannequin;wearable stereoscopic video see-through augmented reality system;Magnetic devices;Magnetic resonance imaging;Phantoms;Solid modeling;Stereo image processing;Surgery;Three-dimensional displays},
}

@Article{6815658,
  author   = {G. A. Puerto-Souza and J. A. Cadeddu and G. L. Mariottini},
  title    = {Toward Long-Term and Accurate Augmented-Reality for Monocular Endoscopic Videos},
  journal  = {IEEE Transactions on Biomedical Engineering},
  year     = {2014},
  volume   = {61},
  number   = {10},
  pages    = {2609-2620},
  month    = {Oct},
  issn     = {0018-9294},
  abstract = {By overlaying preoperative radiological 3-D models onto the intraoperative laparoscopic video, augmented-reality (AR) displays promise to increase surgeons' visual awareness of high-risk surgical targets (e.g., the location of a tumor). Existing AR surgical systems lack in robustness and accuracy because of the many challenges in endoscopic imagery, such as frequent changes in illumination, rapid camera motions, prolonged organ occlusions, and tissue deformations. The frequent occurrence of these events can cause the loss of image (anchor) points, and thus, the loss of the AR display after a few frames. In this paper, we present the design of a new AR system that represents a first step toward long term and accurate augmented surgical display for monocular (calibrated and uncalibrated) endoscopic videos. Our system uses correspondence-search methods, and a new weighted sliding-window registration approach, to automatically and accurately recover the overlay by predicting the image locations of a high number of anchor points that were lost after a sudden image change. The effectiveness of the proposed system in maintaining a long term (over 2 min) and accurate (less than 1 mm) augmentation has been documented over a set of real partial-nephrectomy laparascopic videos.},
  doi      = {10.1109/TBME.2014.2323999},
  keywords = {augmented reality;biomedical optical imaging;endoscopes;image registration;medical image processing;radiology;augmented surgical display;augmented-reality displays;correspondence-search methods;high-risk surgical targets;intraoperative laparoscopic video;monocular endoscopic videos;preoperative radiological 3D models;tumor location;weighted sliding-window registration;Biological systems;Cameras;Feature extraction;Solid modeling;Surgery;Tracking;Videos;Augmented reality (AR);endoscopic vision;feature tracking;0},
}

@InProceedings{6867933,
  author    = {S. Bernhardt and S. A. Nicolau and V. Agnus and L. Soler and C. Doignon and J. Marescaux},
  title     = {Automatic detection of endoscope in intraoperative CT image: Application to AR guidance in laparoscopic surgery},
  booktitle = {2014 IEEE 11th International Symposium on Biomedical Imaging (ISBI)},
  year      = {2014},
  pages     = {563-567},
  month     = {April},
  abstract  = {Augmented reality in minimally invasive surgery has rapidly grown over the recent years. Commonly, the surgical scene is augmented through the endoscopic view with a 3D model extracted from a preoperative acquisition. Nevertheless, due to the probable pneumoperitoneum and the patient displacement, the organs of interest often drastically change in shape and place. Methods exist that attempt to recover or simulate the distortion between the pre- and intraoperative states, but require yet another intraoperative 3D acquisition in order to reach the required accuracy. In this article, we propose a new approach to automatically registering the reconstruction from an intraoperative CT acquisition with the static endoscopic view, by locating the endoscope tip consciously intruding in the model. This method does not require any external tracking apparatus nor analysis of the endoscopic image. In the last section, we also present some early quantitative and qualitative results proving the feasibility and clinical potential of our approach.},
  doi       = {10.1109/ISBI.2014.6867933},
  issn      = {1945-7928},
  keywords  = {augmented reality;biomedical optical imaging;computerised tomography;endoscopes;image reconstruction;image registration;medical image processing;surgery;3D model extraction;augmented reality;endoscopic image analysis;intraoperative CT image acquisition;intraoperative CT image reconstruction;intraoperative CT image registration;laparoscopic surgery;minimally invasive surgery;Cameras;Computed tomography;Endoscopes;Liver;Minimally invasive surgery;Three-dimensional displays;C-arm;augmented reality;intraoperative CT;registration},
}

@InProceedings{6868106,
  author    = {A. Chhatkuli and A. Bartoli and A. Malti and T. Collins},
  title     = {Live image parsing in uterine laparoscopy},
  booktitle = {2014 IEEE 11th International Symposium on Biomedical Imaging (ISBI)},
  year      = {2014},
  pages     = {1263-1266},
  month     = {April},
  abstract  = {Augmented Reality (AR) can improve the information delivery to surgeons. In laparosurgery, the primary goal of AR is to provide multimodal information overlaid in live laparoscopic videos. For gynecologic laparoscopy, the 3D reconstruction of uterus and its deformable registration to preoperative data form the major problems in AR. Shape-from-Shading (SfS) and inter-frame registration require an accurate identification of the uterus region, the occlusions due to surgical tools, specularities, and other tissues. We propose a cascaded patient-specific real-time segmentation method to identify these four important regions. We use a color based Gaussian Mixture Model (GMM) to segment the tools and a more elaborate color and texture model to segment the uterus. The specularities are obtained by a saturation test. We show that our segmentation improves SfS and inter-frame registration of the uterus.},
  doi       = {10.1109/ISBI.2014.6868106},
  issn      = {1945-7928},
  keywords  = {Gaussian processes;augmented reality;biological organs;biological tissues;biomedical optical imaging;endoscopes;gynaecology;image colour analysis;image registration;image segmentation;image texture;medical image processing;mixture models;surgery;3D uterus reconstruction;AR problem;GMM;SfS method;augmented reality;cascaded patient-specific real-time segmentation;color based Gaussian mixture model;deformable registration;gynecologic laparoscopy;information delivery;inter-frame registration;laparosurgery;live image parsing;live laparoscopic video;multimodal information overlay;occlusion region identification;preoperative data;saturation test;shape-from-shading method;specularity identification;surgical tool segmentation;tissue identification;uterine color;uterine laparoscopy;uterine texture model;uterus region identification;uterus segmentation;Biomedical imaging;Image color analysis;Image segmentation;Laparoscopes;Surgery;Three-dimensional displays;Videos;Segmentation;laparoscopy;uterus},
}

@InProceedings{6836418,
  author    = {A. Rao and M. Hong and A. Shankaran and W. Fink and J. Rozenblit},
  title     = {Performance assessment and optimization of motion planning in a surgical trainer for potential space applications},
  booktitle = {2014 IEEE Aerospace Conference},
  year      = {2014},
  pages     = {1-12},
  month     = {March},
  abstract  = {Medical surgeries in the space environment, including long term space travel (e.g., to Mars) and permanent presence on other planetary bodies (e.g., Moon and Mars), are posing an inherent logistical, and in the absence of appropriately trained personnel (i.e., surgeons), even a potentially life-threatening challenge. As a potential mitigation the use of an existing surgical trainer tool that would allow crewmembers to acquire basic surgical skills is proposed, and to train space station personnel both in space and on the Moon and Mars to hone these skills long-term. Furthermore, this tool would potentially allow for tele-conducted surgeries, akin to the da Vinci Surgical System, controlled from Earth but executed onboard, e.g., the International Space Station. On Earth the surgical trainer can be used to train surgeons and flight surgeons. The efficiency of any surgical training system plays a significant role in its reduction of operative risks and stress associated with insufficient experience of the trainee. The primary goal of such systems is to raise the trainee to a higher level of proficiency without putting patients at risk in the operating room. The prototype for the Computer Assisted Surgical Trainer (CAST) being developed at the University of Arizona realizes an optimal motion-planning algorithm. The underlying system consists of mechanical fixtures equipped with encoders and DC motors. This hardware provides a means to accurately track the tip movements of laparoscopic instruments used in minimally invasive surgery. Furthermore it provides haptic and visual feedback to trainees by using a PID controller and augmented reality visualization. Examples of surgical guidance and the improvement of surgeon performance over time using CAST are presented.},
  doi       = {10.1109/AERO.2014.6836418},
  issn      = {1095-323X},
  keywords  = {DC motors;Mars;Moon;augmented reality;optimisation;path planning;personnel;space vehicles;surgery;three-term control;Arizona University;CAST;DC motors;International Space Station;Mars;Moon;PID controller;augmented reality visualization;computer assisted surgical trainer;da Vinci surgical system;encoders;flight surgeons;haptic feedback;laparoscopic instruments;life-threatening challenge;long term space travel;mechanical fixtures;medical surgery;optimal motion-planning;optimization;performance assessment;planetary bodies;potential space applications;space environment;space station personnel;surgical skills;tele-conducted surgery;visual feedback;Aerospace electronics;Haptic interfaces;Instruments;Space stations;Surgery;Training;Visualization},
}

@Article{6716056,
  author   = {J. Wang and H. Suenaga and K. Hoshi and L. Yang and E. Kobayashi and I. Sakuma and H. Liao},
  title    = {Augmented Reality Navigation With Automatic Marker-Free Image Registration Using 3-D Image Overlay for Dental Surgery},
  journal  = {IEEE Transactions on Biomedical Engineering},
  year     = {2014},
  volume   = {61},
  number   = {4},
  pages    = {1295-1304},
  month    = {April},
  issn     = {0018-9294},
  abstract = {Computer-assisted oral and maxillofacial surgery (OMS) has been rapidly evolving since the last decade. State-of-the-art surgical navigation in OMS still suffers from bulky tracking sensors, troublesome image registration procedures, patient movement, loss of depth perception in visual guidance, and low navigation accuracy. We present an augmented reality navigation system with automatic marker-free image registration using 3-D image overlay and stereo tracking for dental surgery. A customized stereo camera is designed to track both the patient and instrument. Image registration is performed by patient tracking and real-time 3-D contour matching, without requiring any fiducial and reference markers. Real-time autostereoscopic 3-D imaging is implemented with the help of a consumer-level graphics processing unit. The resulting 3-D image of the patient's anatomy is overlaid on the surgical site by a half-silvered mirror using image registration and IP-camera registration to guide the surgeon by exposing hidden critical structures. The 3-D image of the surgical instrument is also overlaid over the real one for an augmented display. The 3-D images present both stereo and motion parallax from which depth perception can be obtained. Experiments were performed to evaluate various aspects of the system; the overall image overlay error of the proposed system was 0.71 mm.},
  doi      = {10.1109/TBME.2014.2301191},
  keywords = {augmented reality;biomedical optical imaging;dentistry;graphics processing units;image matching;image registration;image sensors;medical image processing;stereo image processing;surgery;3D image overlay;IP-camera registration;augmented display;augmented reality navigation;automatic marker-free image registration;bulky tracking sensors;computer-assisted oral surgery;consumer-level graphics processing unit;customized stereo camera;dental surgery;depth perception;depth perception loss;half-silvered mirror;hidden critical structures;low navigation accuracy;maxillofacial surgery;motion parallax;overall image overlay error;patient anatomy;patient movement;patient tracking;real-time 3D contour matching;real-time autostereoscopic 3D imaging;state-of-the-art surgical navigation;stereo parallax;stereo tracking;surgical instrument;surgical site;troublesome image registration procedures;visual guidance;Cameras;Dentistry;IP networks;Instruments;Navigation;Surgery;Three-dimensional displays;3-D image overlay;Augmented reality (AR);dental surgery;integral photography (IP);stereo tracking;surgical navigation;Dentistry, Operative;Head;Humans;Imaging, Three-Dimensional;Models, Biological;Phantoms, Imaging;Photography, Dental;User-Computer Interface},
}

@InProceedings{6751571,
  author    = {O. Weede and J. Wünscher and H. Kenngott and B. P. Müller-Stich and H. Wörn},
  title     = {Knowledge-based planning of port positions for minimally invasive surgery},
  booktitle = {2013 IEEE Conference on Cybernetics and Intelligent Systems (CIS)},
  year      = {2013},
  pages     = {12-17},
  month     = {Nov},
  abstract  = {The success of a minimally invasive intervention strongly depends on the position of the incision in the abdominal wall for inserting the instruments and the endoscopic camera. A new knowledge-based system for planning these ports is presented. First time, the positioning of the surgeon and the camera assistant in respect to the patient is included. Modeling of the patient, the intervention and the optimization criteria are described. The intervention model is inferred from recorded trajectories of the surgical instruments. As an example application rectal resection is chosen. The optimization is performed with the Seed Throwing Optimization meta-heuristic. A new set of criteria for optimal setup configuration is presented, including ergonomic working directions, collision avoidance and reachability of target areas. The combination of these criteria using a t-norm is described. The optimized port positions are projected on the patient's abdomen with an augmented reality system. The time needed for the planning algorithm was measured and is sufficient. The results show that the computed configuration leads to maximum access in the operation field in the narrow pelvis, maximum separation dexterity for the surgeon and the camera assistant and allows an ergonomic completion of the intervention.},
  doi       = {10.1109/ICCIS.2013.6751571},
  issn      = {2326-8123},
  keywords  = {augmented reality;collision avoidance;endoscopes;ergonomics;knowledge based systems;medical computing;optimisation;planning (artificial intelligence);reachability analysis;surgery;abdominal wall;augmented reality system;camera assistant positioning;collision avoidance;endoscopic camera;ergonomic working directions;incision position;knowledge-based port position planning system;maximum separation dexterity;minimally invasive intervention model;minimally invasive surgery;narrow pelvis;optimal setup configuration;optimization criteria;patient abdomen;port position optimization;recorded trajectories;seed throwing optimization metaheuristic;surgeon positioning;surgical instruments;t-norm;target area reachability;Cameras;Ergonomics;Instruments;Optimization;Planning;Ports (Computers);Surgery},
}

@InProceedings{6721843,
  author    = {M. Simoes and C. G. L. Cao},
  title     = {Leonardo: A First Step towards an Interactive Decision Aid for Port-Placement in Robotic Surgery},
  booktitle = {2013 IEEE International Conference on Systems, Man, and Cybernetics},
  year      = {2013},
  pages     = {491-496},
  month     = {Oct},
  abstract  = {Leonardo is a prototype interactive augmented reality system designed to aid a surgeon in port-placement prior to performing robot-assisted laparoscopic surgery. The system components consist of a computer, a projector, and a motion and color tracking device. The human-computer interface through which the surgeon interacts with the system is the patient's abdomen itself. Based on the parameters of the patient body and surgical procedure, an optimized set of port locations is provided to the surgeon as colored icons projected onto the abdomen. The surgeon can adjust the suggested locations by direct manipulation with the gloved hand and fingers in real-time. This interactive decision-aid is expected to shorten the planning phase, and enhance performance of robot-assisted laparoscopic surgery.},
  doi       = {10.1109/SMC.2013.90},
  issn      = {1062-922X},
  keywords  = {augmented reality;control engineering computing;decision making;image colour analysis;interactive systems;medical computing;medical robotics;object tracking;surgery;user interfaces;Leonardo;color tracking device;computer;gloved hand;human-computer interface;interactive decision aid;motion tracking device;patient abdomen;patient body;planning phase;port locations;port-placement;projector;prototype interactive augmented reality system;robot-assisted laparoscopic surgery;surgeon;surgical procedure;Cameras;Laparoscopes;Minimally invasive surgery;Ports (Computers);Robot kinematics;da Vinci Surgical System;human computer interaction;laparoscopy surgery;surface augmented reality},
}

@InProceedings{6703923,
  author    = {S. W. Choi and H. C. Kim and H. S. Kang and S. Kim and J. Choi},
  title     = {A haptic augmented reality surgeon console for a laparoscopic surgery robot system},
  booktitle = {2013 13th International Conference on Control, Automation and Systems (ICCAS 2013)},
  year      = {2013},
  pages     = {355-357},
  month     = {Oct},
  abstract  = {Robot surgery needs measures for safety and dexterous control of the surgical instrument for wider application, in spite of evident clinical efficacy in diverse surgery areas. Integration of the advanced human machine interface technologies including haptic rendering and augmented reality in surgeon console for robot-assisted laparoscopic surgery to provide enhanced safety and easier system control has been tried in this study. The surgeon console is composed of various hardware and software modules for endoscope video signal capture, image/vision signal processing, 3D deformable model handling, haptic and graphic rendering, and interface to displays and haptic devices. Intra-operative endoscopic video signal is processed to extract information for the tracking of “Object-Of-Interest (OOI)”s such as anatomic structure that needs cautious handling, bleeding site and the relative position of the surgical instruments, and displayed with overlaid image of 3D-reconstructed preoperative medical imaging data. Parts of the extracted or user-defined OOIs can be transformed into a deformable 3D model and interactively manipulated by the surgeon during the operation for intuitive information utilization. The haptic rendering provides virtual force field experience for the surgeon to have safer handling of the surgical instruments and dexterous execution of surgical task. The surgeon console framework has been implemented on a PC integrated in a laparoscopic surgery robot system under development. The system showed successfully the feasibility of the concept and further development for enhanced usability and graphical contents quality is underway.},
  doi       = {10.1109/ICCAS.2013.6703923},
  issn      = {2093-7121},
  keywords  = {augmented reality;dexterous manipulators;endoscopes;haptic interfaces;human-robot interaction;medical image processing;medical robotics;rendering (computer graphics);solid modelling;surgery;3D deformable model handling;3D-reconstructed preoperative medical imaging data;OOI tracking;advanced human machine interface technologies;anatomic structure;bleeding site;clinical efficacy;deformable 3D model;endoscope video signal capture;graphic rendering;graphical content quality;haptic augmented reality surgeon console;haptic devices;haptic rendering;hardware modules;image-vision signal processing;intraoperative endoscopic video signal;laparoscopic surgery robot system;object-of-interest tracking;robot-assisted laparoscopic surgery;software modules;surgical instrument dexterous control;surgical instrument safety;surgical task dexterous execution;user-defined OOls;virtual force field;Deformable models;Haptic interfaces;Robots;Three-dimensional displays;Visualization;Workstations;augmented reality;haptic;laparoscopic;robot surgery;safety},
}

@InProceedings{6696173,
  author    = {K. D. Katyal and M. S. Johannes and T. G. McGee and A. J. Harris and R. S. Armiger and A. H. Firpi and D. McMullen and G. Hotson and M. S. Fifer and N. E. Crone and R. J. Vogelstein and B. A. Wester},
  title     = {HARMONIE: A multimodal control framework for human assistive robotics},
  booktitle = {2013 6th International IEEE/EMBS Conference on Neural Engineering (NER)},
  year      = {2013},
  pages     = {1274-1278},
  month     = {Nov},
  abstract  = {Effective user control of highly dexterous and robotic assistive devices requires intuitive and natural modalities. Although surgically implanted brain-computer interfaces (BCIs) strive to achieve this, a number of non-invasive engineering solutions may provide a quicker path to patient use by eliminating surgical implantation. We present the development of a semi-autonomous control system that utilizes computer vision, prosthesis feedback, effector centric device control, smooth movement trajectories, and appropriate hand conformations to interact with objects of interest. Users can direct a prosthetic limb through an intuitive graphical user interface to complete multi-stage tasks using patient appropriate combinations of control inputs such as eye tracking, conventional prosthetic controls/joysticks, surface electromyography (sEMG) signals, and neural interfaces (ECoG, EEG). Aligned with activities of daily living (ADL), these tasks include directing the prosthetic to specific locations or objects, grasping of objects by modulating hand conformation, and action upon grasped objects such as self-feeding. This Hybrid Augmented Reality Multimodal Operation Neural Integration Environment (HARMONIE) semi-autonomous control system lowers the user's cognitive load, leaving the bulk of command and control of the device to the computer. This flexible and intuitive control system could serve patient populations ranging from wheelchair-bound quadriplegics to upper-limb amputees.},
  doi       = {10.1109/NER.2013.6696173},
  issn      = {1948-3546},
  keywords  = {artificial limbs;assisted living;augmented reality;brain-computer interfaces;cognition;dexterous manipulators;electroencephalography;electromyography;end effectors;graphical user interfaces;handicapped aids;medical robotics;medical signal processing;mobile robots;robot vision;wheelchairs;ADL;BCI;ECoG;EEG;HARMONIE;activities of daily living;command and control;computer vision;dexterous assistive devices;effector centric device control;eye tracking;graphical user interface;hand conformations;human assistive robotics;hybrid augmented reality multimodal operation neural integration environment;intuitive control system;intuitive modalities;multimodal control framework;multistage tasks;natural modalities;neural interfaces;noninvasive engineering solutions;objects grasping;objects interaction;patient populations;patient use;prosthesis feedback;prosthetic controls;prosthetic joysticks;prosthetic limb;robotic assistive devices;sEMG signals;self-feeding;semiautonomous control system;smooth movement trajectories;surface electromyography signal;surgical implantation;surgically implanted brain-computer interfaces;upper-limb amputees;user cognitive load;user control;wheelchair-bound quadriplegics;Computers;Electroencephalography;Prosthetics;Robots;Sensors;Trajectory;Assistive robotics;brain-computer interface;brain-machine interface;computer vision;hybrid BCI/BMI;intelligent robotics;modular prosthetic limb;neural prosthetic system;prosthetics;robotic limb;semi-autonomous},
}

@InProceedings{6631349,
  author    = {G. A. Puerto-Souza and G. L. Mariottini},
  title     = {Toward long-term and accurate Augmented-Reality display for minimally-invasive surgery},
  booktitle = {2013 IEEE International Conference on Robotics and Automation},
  year      = {2013},
  pages     = {5384-5389},
  month     = {May},
  abstract  = {Augmented-Reality (AR) displays increase surgeon's visual awareness of high-risk surgical targets (e.g., the location of a tumor) by accurately overlaying pre-operative radiological 3-D model onto the intra-operative laparoscopic video. Existing AR systems lack in accuracy and robustness against frequent illumination changes, camera motions, and organ occlusions, which rapidly cause the loss of image (anchor) points, and thus the loss of the AR display after a few seconds. In this paper, we present a new AR system, which represents the first step toward long term and accurate augmented surgical display. Our system leverages feature matching to automatically recover the overlay by predicting the image locations of a high number of anchor points that were lost after a sudden image change. Additionally, a weighted sliding-window least-squares approach is also used to increase the accuracy of the AR display over time. The effectiveness of the proposed system in maintaining a long term, stable, and accurate augmentation has been tested over a set of real partial-nephrectomy laparascopic monocular videos from a DaVinci surgical robot.},
  doi       = {10.1109/ICRA.2013.6631349},
  issn      = {1050-4729},
  keywords  = {augmented reality;feature extraction;image matching;kidney;least squares approximations;medical computing;medical image processing;medical robotics;radiology;surgery;video signal processing;AR display;DaVinci surgical robot;augmented surgical display;augmented-reality display;automatic overlay recovery;camera motion;feature matching;for minimally-invasive surgery;high-risk surgical targets;illumination change;image anchor points;image location prediction;intraoperative laparoscopic video;organ occlusion;partial-nephrectomy laparascopic monocular video;preoperative radiological 3D model overlaying;sudden image change;surgeon visual awareness;tumor location;weighted sliding-window least-squares approach;Cameras;Estimation;Feature extraction;Laparoscopes;Mathematical model;Solid modeling;Tracking},
}

@InProceedings{6572714,
  author    = {M. A. Rahman and P. Mahmud and M. S. Mashuk},
  title     = {Augmented and Virtual Reality based approaches in Minimally Invasive Surgery training},
  booktitle = {2013 International Conference on Informatics, Electronics and Vision (ICIEV)},
  year      = {2013},
  pages     = {1-4},
  month     = {May},
  abstract  = {Continuing development in surgical techniques has elevated the level of sophistication in surgery. In order to reduce trauma to healthy tissue of the patient body, scientists are continuously trying different technologies in surgery. Minimally Invasive Surgery is one of this new practices adopted by the surgeons, which allows doctors to operate on patient with minimal damage and this reduces the post-operative pain of the patient and prolonged hospital stay. The biggest drawback of this operation technique is its complexity. Due to this reason it takes a lot of practice and training for a surgeon to master this process before operating. Augmented Reality and Virtual Reality simulation systems offer different solutions to this problem. In this short communication, recent advances in surgery simulation systems and how they are performing in teaching these techniques are discussed. In addition to it, a comparative study on contemporary augmented reality and virtual reality simulation systems in learning surgery has been also presented.},
  doi       = {10.1109/ICIEV.2013.6572714},
  keywords  = {augmented reality;biomedical communication;biomedical education;computer based training;hospitals;surgery;augmented reality-based simulation approach;healthy tissue;minimally-invasive surgery training;patient body;patient postoperative pain reduction;prolonged-hospital stay reduction;surgery learning;surgery simulation systems;surgical techniques;trauma reduction;virtual reality-based simulation approach;Augmented reality;Haptic interfaces;Minimally invasive surgery;Solid modeling;Training;Augmented Reality;Haptic Feedback;MIS;Tactile Realism;Training;Virtual Reality},
}

@InProceedings{6556799,
  author    = {S. Kaeppler and W. Wu and T. Chen and M. Koch and A. P. Kiraly and N. Strobel and J. Hornegger},
  title     = {Semi-automatic catheter model generation using biplane x-ray images},
  booktitle = {2013 IEEE 10th International Symposium on Biomedical Imaging},
  year      = {2013},
  pages     = {1416-1419},
  month     = {April},
  abstract  = {Recently, techniques for the automatic detection or tracking of surgical instruments in X-ray guided computer-assisted interventions have emerged. The purposes of these methods are to facilitate inter-modality registration, motion compensation, enhanced visualization or automatic landmark generation in augmented-reality applications. Most techniques incorporate a model of the device as prior information to evaluate results obtained from a low-level detector. In this paper, we present novel approaches which are able to generate both 2-D and 3-D models of circular and linear catheters from biplane X-ray images with only minimal user input. We apply these methods in the context of Electrophysiology to generate models of ablation and mapping catheters. An evaluation on clinical data sets yielded promising results.},
  doi       = {10.1109/ISBI.2013.6556799},
  issn      = {1945-7928},
  keywords  = {augmented reality;bioelectric phenomena;catheters;diagnostic radiography;image enhancement;image registration;medical image processing;motion compensation;surgery;X-ray guided computer-assisted interventions;augmented-reality applications;automatic landmark generation;automatic surgical instrument detection;biplane X-ray images;circular catheters;electrophysiology;enhanced visualization;intermodality registration;linear catheters;low-level detector;motion compensation;semiautomatic catheter model generation;surgical instrument tracking;Biomedical imaging;Catheters;Computational modeling;Electrodes;Image reconstruction;Solid modeling;X-ray imaging;Ablation;Biplane;Catheter;Detection;Electrophysiology;Mapping;Model;Reconstruction;Semi-automatic;X-ray},
}

@InProceedings{6426963,
  author    = {R. Wen and C. B. Chng and C. K. Chui and K. B. Lim and S. H. Ong and S. K. Y. Chang},
  title     = {Robot-assisted RF ablation with interactive planning and mixed reality guidance},
  booktitle = {2012 IEEE/SICE International Symposium on System Integration (SII)},
  year      = {2012},
  pages     = {31-36},
  month     = {Dec},
  abstract  = {Radiofrequency (RF) ablation delivered through interventional procedure is a good alternative to hepatic resection or liver transplant for treatment of liver tumor. However, inefficient RF needle navigation and limited thermal ablation region challenge surgeons' manipulations. This paper presents robot-assisted RF ablation system for liver tumor treatment with interactive planning and mixed reality guidance. Interactive planning involves ablation model planning and surgeon's supervisory feedback via projector-based augmented reality (AR). The preoperatively defined surgical planning data is visualized directly on the patient body through the AR display, supplying information of insertion points, ablation points as well as preplanned trajectories, before execution by robot. The needle's spatial insertion is intraoperatively navigated by stereoscopic tracking and simulated in a model based virtual environment comprising anatomic and RF needle models. The robotic execution benefited from improved accuracy on needle's tip tracking and model based simulation. Ex vivo model and in vivo animal studies were conducted to validate and assess the performance of proposed mechanism. This solution is promising in overcoming current technological limitations and practical constraints of precise transcutaneous ablation therapy.},
  doi       = {10.1109/SII.2012.6426963},
  keywords  = {augmented reality;data visualisation;liver;medical robotics;needles;object tracking;stereo image processing;surgery;tumours;AR display;RF needle models;RF needle navigation;ablation model planning;ablation points;anatomic models;ex vivo model;hepatic resection;in vivo animal studies;insertion points;interactive planning;intraoperative navigation;liver transplant;liver tumor treatment;mixed guidance;model-based virtual environment;needle tip tracking;patient body;performance assessment;performance validation;projector-based AR;projector-based augmented reality;radiofrequency ablation;robot-assisted RF ablation system;robotic execution;spatial needle insertion;stereoscopic tracking;supervisory feedback;surgeon manipulations;surgical planning data visualization;thermal ablation region;transcutaneous ablation therapy;Needles;Planning;Robot kinematics;Surgery;Trajectory;Tumors},
}

@InProceedings{6386169,
  author    = {T. Xia and S. Léonard and A. Deguet and L. Whitcomb and P. Kazanzides},
  title     = {Augmented reality environment with virtual fixtures for robotic telemanipulation in space},
  booktitle = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  year      = {2012},
  pages     = {5059-5064},
  month     = {Oct},
  abstract  = {This paper presents an augmented reality framework, implemented on the master console of a modified da Vinci® surgical robot, that enables the operator to design and implement assistive virtual fixtures during teleoperation. Our specific goal is to facilitate teleoperation with large time delays, such as the delay of several seconds that occurs with ground-based control of robotic systems in earth orbit. The virtual fixtures give immediate visual feedback and motion guidance to the operator, while the remote slave performs motions consistent with those constraints. This approach is suitable for tasks in unstructured environments, such as servicing of existing on-orbit spacecraft that were not designed for servicing. We conducted a pilot study by teleoperating a remote slave robot for a thermal barrier blanket cutting task using virtual fixtures with and without time delay. The results show that virtual fixtures reduce the time required to complete the task while also eliminating significant manipulation errors, such as tearing the blanket. The improvement in performance is especially dramatic when a simulated time delay (4 seconds) is introduced.},
  doi       = {10.1109/IROS.2012.6386169},
  issn      = {2153-0858},
  keywords  = {augmented reality;control engineering computing;delays;feedback;manipulators;medical robotics;surgery;telerobotics;assistive virtual fixtures;augmented reality environment;earth orbit;ground-based control;master console;modified da Vinci surgical robot;motion guidance;on-orbit spacecraft;pilot remote slave robot;robotic telemanipulation;thermal barrier blanket cutting task;time 4 s;time delays;visual feedback;Delay;Delay effects;Robot sensing systems;Satellites;Space vehicles;Trajectory},
}

@InProceedings{6347306,
  author    = {D. Fuerst and D. Stephan and P. Augat and A. Schrempf},
  title     = {Foam phantom development for artificial vertebrae used for surgical training},
  booktitle = {2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society},
  year      = {2012},
  pages     = {5773-5776},
  month     = {Aug},
  abstract  = {Currently the surgical training of kyphoplasty and vertebroplasty is performed on patients or specimens. To improve patient safety, a project was initiated to develop an Augmented Reality simulator for the surgical training of these interventions. Artificial vertebral segments should be integrated to provide realistic haptic feedback. To reach this, resulting forces during needle insertions (trans- and extrapedicular) into formalin-fixed vertebral specimens were measured. The same insertion procedure was also performed on six customized polyurethane blocks with varying mechanical parameters. Based on the results of these measurements, a specific foam phantom was generated and the insertion force measured. Additionally a parametric model for the needle insertion into bone was designed calculating three characteristic parameters for all insertion measurements. The resulting insertion force for the foam phantom was comparable to the specimen measurements and the parametric model provided comprehensible characteristic parameters. Based on the resulting force during needle insertion into human vertebrae, a possible foam recipe for manufacturing artificial segments was found. Furthermore, the parametric model provides characteristic parameters for the assessment of phantoms as well as the development of its production process.},
  doi       = {10.1109/EMBC.2012.6347306},
  issn      = {1094-687X},
  keywords  = {biomechanics;biomedical materials;biomedical measurement;bone;cellular biophysics;learning (artificial intelligence);neurophysiology;orthopaedics;phantoms;physiological models;polymers;surgery;artificial segments;artificial vertebral segments;augmented reality simulator;bone;comprehensible characteristic parameters;customized polyurethane blocks;foam phantom development;formalin-fixed vertebral specimens;insertion force measurement;kyphoplasty;mechanical parameters;needle insertion;needle insertions;parametric model;patient safety;realistic haptic feedback;specific foam phantom;specimen measurements;surgical training;vertebroplasty;Bones;Force;Force measurement;Geometry;Needles;Phantoms;Surgery;Artificial vertebrae;kyphoplasty;polyurethane foam;Artificial Organs;Biocompatible Materials;Biomechanical Phenomena;General Surgery;Humans;Models, Anatomic;Spine},
}

@Article{6280672,
  author   = {T. K. Adebar and M. C. Yip and S. E. Salcudean and R. N. Rohling and C. Y. Nguan and S. L. Goldenberg},
  title    = {Registration of 3D Ultrasound Through an Air #x2013;Tissue Boundary},
  journal  = {IEEE Transactions on Medical Imaging},
  year     = {2012},
  volume   = {31},
  number   = {11},
  pages    = {2133-2142},
  month    = {Nov},
  issn     = {0278-0062},
  abstract = {In this study, we evaluated a new method for registering three-dimensional ultrasound (3DUS) data to external coordinate systems. First, 3DUS was registered to the stereo endoscope of a da Vinci Surgical System by placing a registration tool against an air–tissue boundary so that the 3DUS could image ultrasound fiducials while the stereo endoscope could image camera markers on the same tool. The common points were used to solve the registration between the 3DUS and camera coordinate systems. The target registration error (TRE) when imaging through a polyvinyl chloride (PVC) tissue phantom ranged from $3.85pm 1.76~{hbox {mm}}$ to $1.82pm 1.03~{hbox {mm}}$ using one to four registration tool positions. TRE when imaging through an ex vivo liver tissue sample ranged from $2.36pm 1.01~{hbox {mm}}$ to $1.51pm 0.70~{hbox {mm}}$ using one to four registration tool positions. Second, using a similar method, 3DUS was registered to the kinematic coordinate system of a da Vinci Surgical System by using the da Vinci surgical manipulators to identify common points on an air–tissue boundary. TRE when imaging through a PVC tissue phantom was $0.95pm 0.38~{hbox {mm}}$. This registration method is simpler and potentially more accurate than methods using commercial motion tracking systems. This method may be useful in the future in augmented reality systems for laparoscopic and robotic-assisted surgery.},
  doi      = {10.1109/TMI.2012.2215049},
  keywords = {Augmented reality;Endoscopes;Medical robotics;Phantoms;Transducers;Ultrasonic imaging;Endoscopy;medical robotics;prostate;registration;surgical guidance/navigation;ultrasound;virtual/augmented reality;Air;Algorithms;Animals;Endoscopy;Imaging, Three-Dimensional;Liver;Male;Phantoms, Imaging;Prostate;Robotics;Surgery, Computer-Assisted;Swine;Ultrasonography},
}

@Article{6264103,
  author   = {M. C. Yip and D. G. Lowe and S. E. Salcudean and R. N. Rohling and C. Y. Nguan},
  title    = {Tissue Tracking and Registration for Image-Guided Surgery},
  journal  = {IEEE Transactions on Medical Imaging},
  year     = {2012},
  volume   = {31},
  number   = {11},
  pages    = {2169-2182},
  month    = {Nov},
  issn     = {0278-0062},
  abstract = {Vision-based tracking of tissue is a key component to enable augmented reality during a surgical operation. Conventional tracking techniques in computer vision rely on identifying strong edge features or distinctive textures in a well-lit environment; however endoscopic tissue images do not have strong edge features, are poorly lit and exhibit a high degree of specular reflection. Therefore, prior work in achieving densely populated 3-D features for describing tissue surface profiles require complex image processing techniques and have been limited in providing stable, long-term tracking or real-time processing. In this paper, we present an integrated framework for accurately tracking tissue in surgical stereo-cameras at real-time speeds. We use a combination of the STAR feature detector and binary robust independent elementary features to acquire salient features that can be persistently tracked at high frame rates. The features are then used to acquire a densely-populated map of the deformations of tissue surface in 3-D. We evaluate the method against popular feature algorithms in in vivo animal study video sequences, and we also apply the proposed method to human partial nephrectomy video sequences. We extend the salient feature framework to support region tracking in order to maintain the spatial correspondence of a tracked region of tissue or a medical image registration to the surrounding tissue. In vitro tissue studies show registration accuracies of 1.3–3.3 mm using a rigid-body transformation method.},
  doi      = {10.1109/TMI.2012.2212718},
  keywords = {Feature extraction;Image registration;Real time systems;Stereo image processing;Surface reconstruction;Surgery;Tracking;Feature tracking;image registration;image-guided surgery;salient features;stereoscopy;surface reconstruction;Algorithms;Animals;Cattle;Endoscopy;Heart;Humans;Imaging, Three-Dimensional;Kidney;Liver;Models, Biological;Nephrectomy;Surgery, Computer-Assisted;Swine},
}

@InProceedings{6162890,
  author    = {A. Okur and S. A. Ahmadi and A. Bigdelou and T. Wendler and N. Navab},
  title     = {MR in OR: First analysis of AR/VR visualization in 100 intra-operative Freehand SPECT acquisitions},
  booktitle = {2011 10th IEEE International Symposium on Mixed and Augmented Reality},
  year      = {2011},
  pages     = {211-218},
  month     = {Oct},
  abstract  = {For the past two decades, medical Augmented Reality visualization has been researched and prototype systems have been tested in laboratory setups and limited clinical trials. Up to our knowledge, until now, no commercial system incorporating Augmented Reality visualization has been developed and used routinely within the real-life surgical environment. In this paper, we are reporting on observations and analysis concerning the usage of a commercially developed and clinically approved Freehand SPECT system, which incorporates monitor-based Mixed Reality visualization, during real-life surgeries. The workflow-based analysis we present is focused on an atomic sub-task of sentinel lymph node biopsy. We analyzed the usage of the Augmented and Virtual Reality visualization modes by the surgical team, while leaving the staff completely uninfluenced and unbiased in order to capture the natural interaction with the system. We report on our observations in over 100 Freehand SPECT acquisitions within different phases of 52 surgeries.},
  doi       = {10.1109/ISMAR.2011.6092388},
  keywords  = {Image reconstruction;Probes;Single photon emission computed tomography;Surgery;Target tracking;Three dimensional displays;Visualization},
}

@Article{6060914,
  author   = {J. Stoll and H. Ren and P. E. Dupont},
  title    = {Passive Markers for Tracking Surgical Instruments in Real-Time 3-D Ultrasound Imaging},
  journal  = {IEEE Transactions on Medical Imaging},
  year     = {2012},
  volume   = {31},
  number   = {3},
  pages    = {563-575},
  month    = {March},
  issn     = {0278-0062},
  abstract = {A family of passive echogenic markers is presented by which the position and orientation of a surgical instrument can be determined in a 3-D ultrasound volume, using simple image processing. Markers are attached near the distal end of the instrument so that they appear in the ultrasound volume along with the instrument tip. They are detected and measured within the ultrasound image, thus requiring no external tracking device. This approach facilitates imaging instruments and tissue simultaneously in ultrasound-guided interventions. Marker-based estimates of instrument pose can be used in augmented reality displays or for image-based servoing. Design principles for marker shapes are presented that ensure imaging system and measurement uniqueness constraints are met. An error analysis is included that can be used to guide marker design and which also establishes a lower bound on measurement uncertainty. Finally, examples of marker measurement and tracking algorithms are presented along with experimental validation of the concepts.},
  doi      = {10.1109/TMI.2011.2173586},
  keywords = {biomedical equipment;biomedical ultrasonics;error analysis;medical image processing;object tracking;design principles;error analysis;image processing;marker shape;passive echogenic markers;real time 3D ultrasound imaging;surgical instrument tracking;ultrasound guided intervention;Imaging;Instruments;Shafts;Shape;Surgery;Ultrasonic imaging;Uncertainty;Instrument tracking;surgery;ultrasound;Algorithms;Animals;Echocardiography;Fiducial Markers;Imaging, Three-Dimensional;Reproducibility of Results;Surgery, Computer-Assisted;Surgical Instruments;Swine;Ultrasonography},
}

@InProceedings{6004893,
  author    = {V. Lahanas and C. Loukas and N. Nikiteas and D. Dimitroulis and E. Georgiou},
  title     = {Psychomotor skills assessment in laparoscopic surgery using augmented reality scenarios},
  booktitle = {2011 17th International Conference on Digital Signal Processing (DSP)},
  year      = {2011},
  pages     = {1-6},
  month     = {July},
  abstract  = {The aim of this paper was to investigate the potential role of augmented reality (AR) in the assessment of psychomotor skills in laparoscopic surgery. A basic AR task was developed using the ARToolkitPlus. The simulation models (virtual spheres) were embedded in a conventional laparoscopic box trainer. An ARToolkitPlus marker was attached to a laparoscopic instrument for extracting its trajectory. Two different groups were recruited to perform the task: novices and experts. Hidden Markov Models (HMMs) and Dynamic Time Warping (DTW) were used to evaluate the surgical maneuvers. The results show the strength of the proposed approach for proficiency assessment based on the quality of surgical maneuvers performed in the AR environment.},
  doi       = {10.1109/ICDSP.2011.6004893},
  issn      = {1546-1874},
  keywords  = {augmented reality;hidden Markov models;psychology;surgery;ARToolkitPlus;DTW;HMM;augmented reality scenarios;dynamic time warping;hidden Markov models;laparoscopic box trainer;laparoscopic instrument;laparoscopic surgery;psychomotor skills assessment;simulation models;surgical maneuvers;trajectory extraction;virtual spheres;Hidden Markov models;Instruments;Laparoscopes;Solid modeling;Surgery;Training;Trajectory;Augmented reality;Hidden Markov models;laparoscopy;medical simulation},
}

@InProceedings{5980059,
  author    = {O. G. Grasa and J. Civera and J. M. M. Montiel},
  title     = {EKF monocular SLAM with relocalization for laparoscopic sequences},
  booktitle = {2011 IEEE International Conference on Robotics and Automation},
  year      = {2011},
  pages     = {4816-4821},
  month     = {May},
  abstract  = {In recent years, research on visual SLAM has produced robust algorithms providing, in real time at 30 Hz, both the 3D model of the observed rigid scene and the 3D camera motion using as only input the gathered image sequence. These algorithms have been extensively validated in rigid human-made environments -indoor and outdoor- showing robust performance in dealing with clutter, occlusions or sudden motions. Medical endoscopic sequences naturally pose a monocular SLAM problem: an unknown camera motion in an unknown environment. The corresponding map would be useful in providing 3D information to assist surgeons, to support augmented reality insertions or to be exploited by medical robots. In this paper we propose the combination EKF Monocular SLAM + 1-Point RANSAC + Randomised List Relocalization to process laparoscopic sequences -abdominal cavity images-. The sequences are challenging due to: 1) cluttering produced by tools; 2) sudden motions of the camera; 3) laparoscope frequently goes in and out of abdominal cavity; 4) tissue deformation caused by respiration, heartbeats and/or surgical tools. Real medical image sequences provide experimental validation.},
  doi       = {10.1109/ICRA.2011.5980059},
  issn      = {1050-4729},
  keywords  = {Kalman filters;SLAM (robots);augmented reality;cameras;endoscopes;image motion analysis;image sequences;medical image processing;medical robotics;solid modelling;1-point RANSAC;3D camera motion;3D information;3D model;EKF monocular SLAM;abdominal cavity images;augmented reality;laparoscopic sequences;medical endoscopic sequences;medical robots;randomised list relocalization;real medical image sequences;surgical tools;tissue deformation;visual SLAM;Cameras;Cavity resonators;Computational modeling;Laparoscopes;Simultaneous localization and mapping;Surgery;Three dimensional displays},
}

@InProceedings{5974025,
  author    = {L. T. De Paolis and G. Aloisio},
  title     = {Advanced interface for the pre-operative planning in pediatric laparoscopy},
  booktitle = {Proceedings of the ITI 2011, 33rd International Conference on Information Technology Interfaces},
  year      = {2011},
  pages     = {221-226},
  month     = {June},
  abstract  = {The practice of Minimally Invasive Surgery is becoming more and more widespread and is being adopted as an alternative to the classical procedure. This technique presents some limitations for surgeons. In particular, the lack of depth in perception and the difficulty in estimating the distance of the specific structures in laparoscopic surgery can impose limits to delicate dissection or suturing. The presence of new systems for the pre-operative planning can be very useful to the surgeon. In this paper we present an advanced interface for the pre-operative planning of the surgical procedure and the choice of the abdominal access points in pediatric laparoscopy; using the Augmented Reality technology, these points are overlapped on the real patient's body. Two case studies have been considered for the building of 3D models of the patient's organs from the CT images. The developed application allows the surgeon to gather information about the patient and his pathology, visualizing and interacting with the 3D models of the organs built from the patient's medical images, measuring the dimensions of the organs and deciding the best points to insert the trocars in the patient's body.},
  issn      = {1330-1012},
  keywords  = {augmented reality;computerised tomography;data visualisation;medical image processing;solid modelling;surgery;user interfaces;3D models;CT images;abdominal access points;augmented reality technology;distance estimation;laparoscopic surgery;minimally invasive surgery;pediatric laparoscopy;pre-operative planning;Augmented reality;Biomedical imaging;Planning;Solid modeling;Surgery;Three dimensional displays;Visualization;Augmented Reality;User interface;image processing;surgical planning},
}

@InProceedings{1115075,
  author    = {M. Figl and W. Birkfellner and C. Ede and J. Hummel and R. Hanel and F. Watzinger and F. Wanschitz and R. Ewers and H. Bergmann},
  title     = {The control unit for a head mounted operating microscope used for augmented reality visualization in computer aided surgery},
  booktitle = {Proceedings. International Symposium on Mixed and Augmented Reality},
  year      = {2002},
  pages     = {69-75},
  abstract  = {Two main concepts of head mounted displays (HMD) for augmented reality (AR) visualization exist, the optical and video-see through type. Several research groups have pursued both approaches for utilizing HMDs for computer aided surgery. While the hardware requirements for a video see through HMD to achieve acceptable time delay and frame rate seem to be enormous the clinical acceptance of such a device is doubtful from a practical point of view. Starting from previous work in displaying additional computer-generated graphics in operating microscopes, we have adapted a miniature head mounted operating microscope for AR by integrating two very small computer displays. To calibrate the projection parameters of this so called varioscope AR we have used Tsai's (1987) algorithm for camera calibration. Connection to a surgical navigation system was performed by defining an open interface to the control unit of the varioscope AR. The control unit consists of a standard PC with an dual head graphics adapter to render and display the desired augmentation of the scene. We connected this control unit to an computer aided surgery (CAS) system by the TCP/IP interface. In this paper we present the control unit for the HMD and its software design. We tested two different optical tracking systems, the Flash-point (Image Guided Technologies, Boulder, CO), which provided about 10 frames per second, and the Polaris (Northern Digital, Ontario, Can) which provided at least 30 frames per second, both with a time delay of one frame.},
  doi       = {10.1109/ISMAR.2002.1115075},
  keywords  = {augmented reality;biomedical equipment;biomedical optical imaging;calibration;computer displays;data visualisation;helmet mounted displays;medical image processing;microcomputer applications;optical microscopes;optical tracking;surgery;Flashpoint;PC;Polaris;TCP/IP interface;augmented reality visualization;computer aided surgery;computer displays;computer-generated graphics;control unit;dual head graphics adapter;head mounted displays;head mounted operating microscope;open interface;optical tracking systems;projection parameter calibration;rendering;software design;surgical navigation system;time delay;varioscope AR;Augmented reality;Biomedical optical imaging;Computer displays;Computer graphics;Control systems;Delay effects;Magnetic heads;Microscopy;Surgery;Visualization},
}

@Article{1076043,
  author   = {W. Birkfellner and M. Figl and K. Huber and F. Watzinger and F. Wanschitz and J. Hummel and R. Hanel and W. Greimel and P. Homolka and R. Ewers and H. Bergmann},
  title    = {A head-mounted operating binocular for augmented reality visualization in medicine - design and initial evaluation},
  journal  = {IEEE Transactions on Medical Imaging},
  year     = {2002},
  volume   = {21},
  number   = {8},
  pages    = {991-997},
  month    = {Aug},
  issn     = {0278-0062},
  abstract = {Computer-aided surgery (CAS), the intraoperative application of biomedical visualization techniques, appears to be one of the most promising fields of application for augmented reality (AR), the display of additional computer-generated graphics over a real-world scene. Typically a device such as a head-mounted display (HMD) is used for AR. However, considerable technical problems connected with AR have limited the intraoperative application of HMDs up to now. One of the difficulties in using HMDs is the requirement for a common optical focal plane for both the realworld scene and the computer-generated image, and acceptance of the HMD by the user in a surgical environment. In order to increase the clinical acceptance of AR, we have adapted the Varioscope (Life Optics, Vienna), a miniature, cost-effective head-mounted operating binocular, for AR. In this paper, we present the basic design of the modified HMD, and the method and results of an extensive laboratory study for photogrammetric calibration of the Varioscope's computer displays to a real-world scene. In a series of 16 calibrations with varying zoom factors and object distances, mean calibration error was found to be 1.24 ± 0.38 pixels or 0.12 ± 0.05 mm for a 640 × 480 display. Maximum error accounted for 3.33 ± 1.04 pixels or 0.33 ± 0.12 mm. The location of a position measurement probe of an optical tracking system was transformed to the display with an error of less than 1 mm in the real world in 56% of all cases. For the remaining cases, error was below 2 mm. We conclude that the accuracy achieved in our experiments is sufficient for a wide range of CAS applications.},
  doi      = {10.1109/TMI.2002.803099},
  keywords = {augmented reality;biomedical equipment;calibration;focal planes;helmet mounted displays;medical image processing;optical tracking;photogrammetry;position measurement;surgery;augmented reality visualization;clinical acceptance;common optical focal plane;computer-aided surgery;computer-generated image;head-mounted operating binocular;mean calibration error;medical instrumentation;object distances;optical tracking system;photogrammetric calibration;real-world scene;varioscope;zoom factors;Application software;Augmented reality;Biomedical computing;Biomedical optical imaging;Calibration;Computer displays;Content addressable storage;Layout;Surgery;Visualization;Calibration;Computer Graphics;Depth Perception;Equipment Design;Equipment Failure Analysis;Imaging, Three-Dimensional;Microscopy, Video;Microsurgery;Reproducibility of Results;Sensitivity and Specificity;Subtraction Technique;Surgical Equipment;User-Computer Interface;Video Recording},
}

@InProceedings{930259,
  author    = {Zou Qingsong and Kwoh Chee Keong and Ng Wan Sing},
  title     = {Interactive surgical planning using context based volume visualization techniques},
  booktitle = {Proceedings International Workshop on Medical Imaging and Augmented Reality},
  year      = {2001},
  pages     = {21-25},
  abstract  = {We present a new volume visualization scheme, context based volume visualization to assist the surgeon in surgical planning. This visualization scheme differs from ordinary surface based rendering and volume rendering by providing a framework to combine surface based rendering and volume rendering. We can achieve the powerful manipulating capability of surface based rendering and as good a rendering effect as volume rendering at the same time. Using a special data structured-segment tree to manage the visualization scene, which includes all the volume objects and graphics objects (the surgical tools) needed to be visualized, this visualization scheme provides a common context based interface for both graphics objects and volume objects, through which, we can control graphics and volume objects easily in the same way. The context based visualization scheme can greatly increase the performance of volume visualization by generating the scene much faster through selectively revisualizing the affected objects. Based on these ideas, we implement an interactive surgical planning system, Virtual Doctor based on OpenGL 1.1 on WinNT platform and VolumePro vg500 card. This system is a good 3D volume visualization tool and augmented reality system for interactive surgical planning and further research in this area},
  doi       = {10.1109/MIAR.2001.930259},
  keywords  = {augmented reality;data visualisation;interactive systems;medical image processing;rendering (computer graphics);surgery;tree data structures;OpenGL;Virtual Doctor;VolumePro vg500 card;WinNT platform;augmented reality;context based volume visualization;interactive surgical planning system;medical image processing;surface based rendering;surgeon;tree data structure;volume rendering;Augmented reality;Biomedical imaging;Data visualization;Graphics;Layout;Rendering (computer graphics);Surface reconstruction;Surgery;Surges;Ultrasonic imaging},
}

@InProceedings{930258,
  author    = {F. Devernay and F. Mourgues and E. Coste-Maniere},
  title     = {Towards endoscopic augmented reality for robotically assisted minimally invasive cardiac surgery},
  booktitle = {Proceedings International Workshop on Medical Imaging and Augmented Reality},
  year      = {2001},
  pages     = {16-20},
  abstract  = {One of the problems tightly linked to endoscopic surgery is the fact that, because of the narrow field of view, it is sometimes quite difficult to locate the objects that can be seen through the endoscope. This is especially true in cardiac surgery, where it is difficult to not confuse two coronary arteries on a beating heart. We propose a methodology to achieve coronary localisation by augmented reality on a robotized stereoscopic endoscope. The method we propose involves five steps: making a time-variant 3D model of the beating heart using coronarography and CT-scan or MRI, calibrating the stereoscopic endoscope, reconstructing the 3D operating field, registering the operating field surface with the 3D heart model, and adding information on the endoscopic images by augmented reality. The da VinciTM surgical system was used for our first experiments with the Cardiac Surgery team at Hopital Europeen Georges Pompidou, Paris, France},
  doi       = {10.1109/MIAR.2001.930258},
  keywords  = {augmented reality;cardiology;medical image processing;medical robotics;solid modelling;surgery;3D heart model;CT-scan;MRI;augmented reality;coronarography;coronary arteries;coronary localisation;da Vinci system;endoscopic surgery;medical robots;minimally invasive cardiac surgery;robot stereoscopic endoscope;time-variant 3D model;Augmented reality;Deformable models;Endoscopes;Heart;Instruments;Magnetic resonance imaging;Minimally invasive surgery;Robot sensing systems;Surges;Thyristors},
}

@Article{788580,
  author   = {G. Glombitza and H. Evers and S. Hassfeld and U. Engelmann and H. P. Meinzer},
  title    = {Virtual surgery in a (tele-)radiology framework},
  journal  = {IEEE Transactions on Information Technology in Biomedicine},
  year     = {1999},
  volume   = {3},
  number   = {3},
  pages    = {186-196},
  month    = {Sept},
  issn     = {1089-7771},
  abstract = {Presents telemedicine as an extension of a teleradiology framework through tools for virtual surgery. To classify the described methods and applications, the research field of virtual reality (VR) is broadly reviewed. Differences with respect to technical equipment, methodological requirements and areas of application are pointed out. VR, desktop VR and augmented reality are differentiated and discussed in some typical contexts of diagnostic support, surgical planning, therapeutic procedures, simulation and training. Visualization techniques are compared as a prerequisite for VR and assigned to distinct levels of immersion. The advantage of a hybrid visualization kernel is emphasized with respect to the desktop VR applications that are subsequently shown. Moreover, software design aspects are considered by outlining functional openness in the architecture of the host system. A teleradiology workstation was extended by dedicated tools for surgical planning through a plug-in mechanism. Examples of recent areas of application are introduced, such as liver tumor resection planning, diagnostic support in heart surgery, and craniofacial surgery planning. In the future, surgical planning systems will become more important. They will benefit from improvements in image acquisition and communication, new image processing approaches and techniques for data presentation. This will facilitate pre-operative planning and intra-operative applications.},
  doi      = {10.1109/4233.788580},
  keywords = {cardiology;data visualisation;liver;medical computing;radiology;surgery;telemedicine;tumours;virtual reality;visual communication;workstations;augmented reality;craniofacial surgery;data presentation;desktop virtual reality;diagnostic support;functionally open architecture;future;heart surgery;hybrid visualization kernel;image acquisition;image communication;image processing;immersion levels;intra-operative applications;liver tumor resection planning;methodological requirements;plug-in mechanism;pre-operative planning;simulation;software design;surgical planning;technical equipment;telemedicine;teleradiology framework;teleradiology workstation;therapeutic procedures;training;virtual surgery;Application software;Augmented reality;Computer architecture;Context modeling;Kernel;Software design;Surgery;Telemedicine;Virtual reality;Visualization;Surgical Procedures, Operative;Teleradiology;Therapy, Computer-Assisted},
}

@InProceedings{900451,
  author    = {O. Tonet and G. Megali and P. Dario and M. C. Carrozza and M. Marcacci and P. F. La Palombara},
  title     = {A novel navigation system for computer assisted orthopaedic surgery},
  booktitle = {Proceedings of the 22nd Annual International Conference of the IEEE Engineering in Medicine and Biology Society (Cat. No.00CH37143)},
  year      = {2000},
  volume    = {3},
  pages     = {1864-1865 vol.3},
  abstract  = {Arthroscopy is routinely used as a surgical treatment for several joint disorders. Its moderate invasiveness ensures minor tissue damage and faster post-surgical recovery. Arthroscopic surgical procedures require, however, considerable dexterity and skill due to the limited 2D view of the scene and restricted operating area. The target of this novel system for computer assisted arthroscopic surgery is to help the surgeon overcome these limitations by providing an augmented reality environment. The key feature of the system is an interactive graphical interface in which a 3D model and two 2D projections (frontal and sagittal) of the patient's joint are shown together with a model of the arthroscope and of other sensorized surgical instruments. Sensors are also fastened to the patient's anatomy. The positions of the sensorized objects are tracked in real-time by an optoelectronic localizer and displayed in the virtual scene. The field of view of the arthroscope is also dynamically highlighted on the model. The 3D model of the joint is reconstructed from a preoperative CT/MRI data set. The virtual model and the actual anatomy are matched by means of an ICP-based non-fiducial registration algorithm. The prototype system has been positively evaluated by a selected group of qualified orthopaedic surgeons. Preliminary tests on the overall accuracy of the system show that errors can be kept within 3°/3 mm and negotiation over the industrial exploitation of the prototype is currently in progress},
  doi       = {10.1109/IEMBS.2000.900451},
  issn      = {1094-687X},
  keywords  = {augmented reality;computerised navigation;image registration;medical image processing;orthopaedics;surgery;2D projections;3D model;arthroscopy;augmented reality environment;computer assisted orthopaedic surgery;dexterity;joint disorders;limited 2D view;minor tissue damage;navigation system;optoelectronic localizer;real-time tracking;restricted operating area;sensorized surgical instruments;surgical treatment;virtual scene;Anatomy;Augmented reality;Joints;Layout;Navigation;Orthopedic surgery;Prototypes;Surges;Surgical instruments;Tissue damage},
}

@InProceedings{880923,
  author    = {W. Birkfellner and K. Huber and F. Watzinger and M. Figl and F. Wanschitz and R. Hanel and D. Rafolt and R. Ewers and H. Bergmann},
  title     = {Development of the Varioscope AR. A see-through HMD for computer-aided surgery},
  booktitle = {Proceedings IEEE and ACM International Symposium on Augmented Reality (ISAR 2000)},
  year      = {2000},
  pages     = {54-59},
  abstract  = {In computer-aided surgery (CAS), an undesired side-effect of the necessity of handling sophisticated equipment in the operating room is the fact that the surgeon's attention is drawn from the operating field, since surgical progress is partially monitored on the computer's screen. Augmented reality (AR), the overlay of computer-generated graphics over a real-world scene, provides a possibility to solve this problem. The technical problems associated with this approach, such as viewing of the scenery within a common focal range on the head-mounted display (HMD) or latency in the display on the HMD, have, however, kept AR from widespread usage in CAS. The concept of the Varioscope AR, a lightweight head-mounted operating microscope used as a HMD, is introduced. The registration of the patient to the pre-operative image data, as well as pre-operative planning, take place on VISIT, a surgical navigation system developed at our hospital. Tracking of the HMD and stereoscopic visualisation take place on a separate POSIX.4-compliant real-time operating system running on PC hardware. We were able to overcome the technical problems described above; our work resulted in an AR visualisation system with an update rate of 6 Hz and a latency below 130 ms. It integrates seamlessly into a surgical navigation system and provides a common focus for both virtual and real-world objects. First evaluations of the photogrammetric 2D/3D registration have resulted in a match of 1.7 pixels on the HMD display. The Varioscope AR with its real-time visualisation unit is a major step towards the introduction of AR into clinical routine},
  doi       = {10.1109/ISAR.2000.880923},
  keywords  = {augmented reality;computer displays;data visualisation;helmet mounted displays;image registration;medical image processing;optical microscopes;photogrammetry;real-time systems;surgery;tracking;130 ms;6 Hz;PC hardware;POSIX.4-compliant real-time operating system;VISIT;Varioscope AR;augmented reality;clinical routine;computer-aided surgery;computer-generated graphics;display latency;focal range;operating field;operating microscope;patient registration;photogrammetric 2D/3D registration;preoperative image data;preoperative planning;real-time visualisation unit;real-world scene;see-through head-mounted display;stereoscopic visualisation;surgical navigation system;surgical progress monitoring;technical problems;tracking;update rate;Augmented reality;Computer displays;Computer graphics;Computerized monitoring;Content addressable storage;Delay;Navigation;Surgery;Surges;Visualization},
}

@InProceedings{756504,
  author    = {M. Kim and P. Milgram and J. Drake},
  title     = {Virtual tape measure for 3D measurements in micro-surgery},
  booktitle = {Engineering in Medicine and Biology Society, 1997. Proceedings of the 19th Annual International Conference of the IEEE},
  year      = {1997},
  volume    = {3},
  pages     = {967-969 vol.3},
  month     = {Oct},
  abstract  = {A unique measuring tool for use with a surgical operating microscope was developed in prototype form. Stereo-video images of the surgical field obtained through the operating microscope were combined with stereo-computer graphics to create a virtual tape measure. This augmented reality display allows accurate absolute distance measurements to be made between any two points of interest in the 3D surgical space. The overall performance of the system has a 95% confidence interval for the absolute measurement error of 0.2 to 0.7 mm. This paper describes the motivation for the development of the system, the configuration of the hardware and software components, and details of accuracy testing},
  doi       = {10.1109/IEMBS.1997.756504},
  issn      = {1094-687X},
  keywords  = {augmented reality;biomedical measurement;computer graphics;distance measurement;medical computing;optical microscopy;performance evaluation;stereo image processing;surgery;virtual instrumentation;3D measurements;absolute measurement error;accuracy testing;augmented reality display;confidence interval;distance measurements;hardware configuration;micro-surgery;software configuration;stereo-computer graphics;stereo-video images;surgical operating microscope;virtual tape measure;Augmented reality;Distance measurement;Graphics;Hardware;Measurement errors;Microscopy;Prototypes;Software testing;Surgery;Three dimensional displays},
}

@Article{736019,
  author   = {Y. Sato and M. Nakamoto and Y. Tamaki and T. Sasama and I. Sakita and Y. Nakajima and M. Monden and S. Tamura},
  title    = {Image guidance of breast cancer surgery using 3-D ultrasound images and augmented reality visualization},
  journal  = {IEEE Transactions on Medical Imaging},
  year     = {1998},
  volume   = {17},
  number   = {5},
  pages    = {681-693},
  month    = {Oct},
  issn     = {0278-0062},
  abstract = {This paper describes augmented reality visualization for the guidance of breast-conservative cancer surgery using ultrasonic images acquired in the operating room just before surgical resection. By combining an optical three-dimensional (3-D) position sensor, the position and orientation of each ultrasonic cross section are precisely measured to reconstruct geometrically accurate 3-D tumor models from the acquired ultrasonic images. Similarly, the 3-D position and orientation of a video camera are obtained to integrate video and ultrasonic images in a geometrically accurate manner. Superimposing the 3-D tumor models onto live video images of the patient's breast enables the surgeon to perceive the exact 3-D position of the tumor, including irregular cancer invasions which cannot be perceived by touch, as if it were visible through the breast skin. Using the resultant visualization, the surgeon can determine the region for surgical resection in a more objective and accurate manner, thereby minimizing the risk of a relapse and maximizing breast conservation. The system was shown to be effective in experiments using phantom and clinical data.},
  doi      = {10.1109/42.736019},
  keywords = {augmented reality;biomedical ultrasonics;cancer;mammography;medical image processing;surgery;breast-conservative therapy;clinical data;exact 3-D position;irregular cancer invasions;live video images;medical diagnostic imaging;phantom data;surgical resection;tumor;ultrasonic cross section;video camera;Augmented reality;Biomedical optical imaging;Breast cancer;Breast neoplasms;Oncological surgery;Optical sensors;Skin neoplasms;Surges;Ultrasonic imaging;Visualization;Breast Neoplasms;Female;Humans;Image Processing, Computer-Assisted;Phantoms, Imaging;Therapy, Computer-Assisted;Ultrasonography, Interventional;Ultrasonography, Mammary;Videotape Recording},
}

@InProceedings{970537,
  author    = {F. Mourgues and F. Devemay and E. Coste-Maniere},
  title     = {3D reconstruction of the operating field for image overlay in 3D-endoscopic surgery},
  booktitle = {Proceedings IEEE and ACM International Symposium on Augmented Reality},
  year      = {2001},
  pages     = {191-192},
  abstract  = {Building a 3D model of the operating field is the first stage to access the registration of pre-operative surface models in a per-operative context. We propose a method to learn the 3D model of the organs observed by a stereoscopic endoscope. This method is general and requires no a priori knowledge of the position of the instruments, whether robotized or manually-controlled, present in the field of view. The result of the learning process allows the removal of the instruments front the surgeon's field of view: "diminished reality". This work prefigures the 3D integration by augmented reality of a preoperative organs model (e.g. arteries) in stereoscopic images. Experiments are currently being performed to validate our approach using endoscopic sequences from the da VinciTM surgical system},
  doi       = {10.1109/ISAR.2001.970537},
  keywords  = {augmented reality;image reconstruction;medical image processing;stereo image processing;surgery;3D integration;3D model;3D reconstruction;3D-endoscopic surgery;arteries;augmented reality;da Vinci surgical system;diminished reality;endoscopic sequences;field of view;image overlay;learning process;operating field;pre-operative surface model registration;preoperative organs model;stereoscopic endoscope;stereoscopic images;Arteries;Augmented reality;Context modeling;Endoscopes;Image reconstruction;Instruments;Robots;Surface reconstruction;Surgery;Surges},
}

@Article{677169,
  author   = {Son-Lik Tang and Chee-Keong Kwoh and Ming-Yeong Teo and Ng Wan Sing and Keck-Voon Ling},
  title    = {Augmented reality systems for medical applications},
  journal  = {IEEE Engineering in Medicine and Biology Magazine},
  year     = {1998},
  volume   = {17},
  number   = {3},
  pages    = {49-58},
  month    = {May},
  issn     = {0739-5175},
  abstract = {Augmented reality (AR) is a technology in which a computer-generated image is superimposed onto the user's vision of the real world, giving the user additional information generated from the computer model. This technology is different from virtual reality, in which the user is immersed in a virtual world generated by the computer. Rather, the AR system brings the computer into the "world" of the user by augmenting the real environment with virtual objects. Using an AR system, the user's view of the real world is enhanced. This enhancement may be in the form of labels, 3D rendered models, or shaded modifications. In this article, the authors review some of the research involving AR systems, basic system configurations, image-registration approaches, and technical problems involved with AR technology. They also touch upon the requirements for an interventive AR system, which can help guide surgeons in executing a surgical plan.},
  doi      = {10.1109/51.677169},
  keywords = {biomedical equipment;image registration;medical image processing;reviews;sensory aids;surgery;3D rendered models;augmented reality systems;basic system configurations;computer model;computer-generated image;image-registration approaches;labels;real world view enhancement;shaded modifications;superimposed image;surgical plan execution guidance;technical problems;Augmented reality;Biomedical equipment;Biomedical imaging;Buildings;Computer vision;Engines;Medical services;Software prototyping;Surgery;Surges;Calibration;Computer Graphics;Data Display;Humans;Image Processing, Computer-Assisted;Psychomotor Performance;Sensitivity and Specificity;Surgical Procedures, Operative;Therapy, Computer-Assisted;Ultrasonography;User-Computer Interface;Video Recording},
}

@Article{737583,
  author   = {G. Riva},
  title    = {Virtual environments in neuroscience},
  journal  = {IEEE Transactions on Information Technology in Biomedicine},
  year     = {1998},
  volume   = {2},
  number   = {4},
  pages    = {275-281},
  month    = {Dec},
  issn     = {1089-7771},
  abstract = {Virtual environments (VEs) let users navigate and interact with computer generated three dimensional (3D) environments in real time, allowing for the control of complex stimuli presentation. These VEs have attracted much attention in medicine, especially in remote or augmented surgery, and surgical training, which are critically dependent on hand-eye coordination. Recently, however, some research projects have begun to test the possibility of using VEs for the study and rehabilitation of human cognitive and functional activities. The paper highlights recent and ongoing research related to the applications of VEs in the neuroscience arena. In particular, it focuses on the American and European initiatives in this field, including a description of the European Commission (EC) funded VREPAR projects. Finally, the paper provides a general introduction to virtual reality (VR), as it relates to its impact on cognitive and functional abilities.},
  doi      = {10.1109/4233.737583},
  keywords = {biomedical education;computer aided instruction;medical computing;neurophysiology;real-time systems;virtual reality;European Commission funded VREPAR projects;European initiatives;VEs;augmented surgery;complex stimuli presentation;computer generated three dimensional environments;functional abilities;hand-eye coordination;human cognitive activities;neuroscience;surgical training;virtual environments;virtual reality;Biomedical imaging;Computer displays;Computer graphics;Humans;Medical services;Neuroscience;Surgery;Telemedicine;Testing;Virtual reality;Computer Systems;Neurosciences},
}

@InProceedings{579639,
  author    = {P. Brodeur and J. Dansereau and J. De Guise and H. Labelle},
  title     = {A points-to-surfaces matching technique for the application of augmented reality during spine surgery},
  booktitle = {Proceedings of 17th International Conference of the Engineering in Medicine and Biology Society},
  year      = {1995},
  volume    = {2},
  pages     = {1197-1198 vol.2},
  month     = {Sep},
  abstract  = {An augmented reality system has been developed and is currently under evaluation to document changes induced by surgical correction of a spine deformity. A pre-operative measurement technique is developed that permits the matching between a patient and a virtual 3-D medical image of normally hidden anatomy. The matching algorithm tested on a phantom gives a RMS error of respectively 1.1 mm and 2.0 mm with 1.0 and 2.0 mm random noises added to the measured points},
  doi       = {10.1109/IEMBS.1995.579639},
  keywords  = {biomedical measurement;bone;image matching;medical image processing;random noise;stereo image processing;surgery;three-dimensional displays;virtual reality;RMS error;augmented reality;matching algorithm;normally hidden anatomy;patient;phantom;points-to-surfaces matching technique;pre-operative measurement technique;random noises;spine deformity;spine surgery;surgical correction;Anatomy;Augmented reality;Biomedical imaging;Chemical technology;Imaging phantoms;Measurement techniques;Production systems;Surgery;Surges;Testing},
}

@InProceedings{970512,
  author    = {M. Figl and W. Birkfellner and J. Hummel and R. Hanel and P. Homolka and F. Watzinger and F. Wanshit and R. Ewers and H. Bergmann},
  title     = {Current status of the Varioscope AR, a head-mounted operating microscope for computer-aided surgery},
  booktitle = {Proceedings IEEE and ACM International Symposium on Augmented Reality},
  year      = {2001},
  pages     = {20-29},
  abstract  = {Computer-aided surgery (CAS), the intraoperative application of biomedical visualization techniques, appears to be one of the most promising fields of application for augmented reality (AR), the display of additional computer generated graphics over a real-world scene. Typically a device such as a head-mounted display (HMD) is used for AR. However considerable technical problems connected with AR have limited the intraoperative application of HMDs up to now. One of the difficulties in using HMDs is the requirement for a common optical focal plane for both the real-world scene and the computer generated image, and acceptance of the HMD by the user in a surgical environment. In order to increase the clinical acceptance of AR, we have adapted the Varioscope (Life Optics, Vienna), a miniature, cost-effective head-mounted operating microscope, for AR. In this work, we present the basic design of the modified HMD, and the method and results of an extensive laboratory study for photogrammetric calibration of the Varioscope's computer displays to a real-world scene. In a series of sixteen calibrations with varying zoom factors and object distances, mean calibration error was found to be 1.24±0.38 pixels or 0.12±0.05 mm for a 640×480 display. Maximum error accounted for 3.33±1.04 pixels or 0.33±0.12 mm. The location of a position measurement probe of an optical tracking system was transformed to the display with an error of less than I mm in the real world in 56% of all cases. For the remaining cases, error was below 2 mm. We conclude that the accuracy achieved in our experiments is sufficient for a wide range of CAS applications},
  doi       = {10.1109/ISAR.2001.970512},
  keywords  = {augmented reality;calibration;data visualisation;helmet mounted displays;medical image processing;optical tracking;augmented reality;biomedical visualization;computer generated graphics;computer-aided surgery;head-mounted display;head-mounted operating microscope;intraoperative application;photogrammetric calibration;position measurement;real-world scene;varioscope augmented reality;Application software;Biomedical computing;Biomedical optical imaging;Calibration;Computer displays;Computer errors;Content addressable storage;Layout;Surgery;Visualization},
}

@Article{767086,
  author   = {E. Jovanov and K. Wegner and V. Radivojevic and D. Starcevic and M. S. Quinn and D. B. Karron},
  title    = {Tactical audio and acoustic rendering in biomedical applications},
  journal  = {IEEE Transactions on Information Technology in Biomedicine},
  year     = {1999},
  volume   = {3},
  number   = {2},
  pages    = {109-118},
  month    = {June},
  issn     = {1089-7771},
  abstract = {Complexity of biomedical data requires novel sophisticated analysis and presentation methods. Sonification is used as a new information display in augmented reality systems to overcome problems of existing human-computer interfaces (e.g., opaque or heavy head-mounted displays, slow computer graphics, etc.). A novel taxonomy of sonification methods and techniques is introduced. We present our experience with tactical audio and acoustic rendering in biomedical applications. Tactical audio as an audio feedback is used as support for precise manual positioning of a surgical instrument in the operating room. Acoustic rendering is applied as an additional information channel and/or warning signal in biomedical signal analysis and data presentation.},
  doi      = {10.1109/4233.767086},
  keywords = {acoustic signal processing;augmented reality;data analysis;medical computing;medical signal processing;surgery;user interfaces;EEG;acoustic rendering;audio feedback;augmented reality;biomedical applications;biomedical signal analysis;data analysis;data presentation methods;human-computer interfaces;information display;operating room;sonification;surgical instrument;tactical audio;Acoustic applications;Application software;Auditory displays;Augmented reality;Bioinformatics;Biomedical acoustics;Computer displays;Computer graphics;Rendering (computer graphics);Taxonomy;Acoustics;Brain;Humans;Man-Machine Systems;Music;User-Computer Interface},
}

@InProceedings{7329762,
  title     = {[Front cover]},
  booktitle = {2015 IEEE 19th International Conference on Intelligent Engineering Systems (INES)},
  year      = {2015},
  pages     = {c1-c1},
  month     = {Sept},
  abstract  = {The following topics are dealt with: learning; Web; urban water-supply system; IP Core; DCI approach; real-time sensor network; linked open data source; process mining; image coding; deep neural network architecture; human computer interaction; social human-robot interaction; VANET; authorized V2V communication; MIMO system; surgical robotics; ontologies; genetic algorithm; image reconstruction; mobile robot; artificial neural network; fuzzy reasoning; heat exchanger; fuzzy controller design; mobile device; human machine interface design; decision support system; data mining technique; discrete-time SISO system; augmented reality; visual analysis; content management system; Androids; nonlinear MPC; collaborative filtering; recommendation; wireless sensor networks;; humidity control; temperature control and stability.},
  doi       = {10.1109/INES.2015.7329762},
  keywords  = {Internet;MIMO systems;augmented reality;collaborative filtering;content management;data mining;decision support systems;discrete time systems;fuzzy control;fuzzy reasoning;genetic algorithms;heat exchangers;human computer interaction;humidity control;image coding;image reconstruction;learning (artificial intelligence);medical robotics;mobile robots;neural nets;nonlinear control systems;predictive control;public domain software;recommender systems;stability;surgery;temperature control;vehicular ad hoc networks;water supply;wireless sensor networks;Androids;DCI approach;IP Core;MIMO system;VANET;Web;artificial neural network;augmented reality;authorized V2V communication;collaborative filtering;content management system;data mining technique;decision support system;deep neural network architecture;discrete-time SISO system;fuzzy controller design;fuzzy reasoning;genetic algorithm;heat exchanger;human computer interaction;human machine interface design;humidity control;image coding;image reconstruction;learning;linked open data source;mobile device;mobile robot;nonlinear MPC;ontologies;process mining;real-time sensor network;recommendation;social human-robot interaction;stability;surgical robotics;temperature control;urban water-supply system;visual analysis;wireless sensor networks},
}

@InProceedings{6549344,
  author    = {H. Fuchs},
  title     = {The 2013 virtual reality career award},
  booktitle = {2013 IEEE Virtual Reality (VR)},
  year      = {2013},
  pages     = {xxiii-xxiii},
  month     = {March},
  abstract  = {The 2013 virtual reality career award goes to Henry Fuchs, The University of North Carolina at Chapel Hill, for his lifetime contributions to research and practice in virtual environments, telepresence, and medical applications. Since the 1970s, Henry Fuchs has made pioneer contributions to many of the technologies needed to enable virtual and augmented reality: automatic construction of 3D models and scenes, fast rendering algorithms (BSP Trees), graphics hardware (Pixel-Planes), large-area tracking systems (HiBall), optical and video see-through head-mounted displays. Many of these advances were inspired by demanding applications such as augmenting visualization for surgical assistance (by merging real and virtual objects) and teleimmersion for remote medical consultation. He continues to innovate within a multi-national telepresence research center with sites at ETH Zurich, NTU Singapore, and UNC Chapel Hill. The IEEE VGTC is pleased to award Henry Fuchs the 2013 Virtual Reality Career Award.},
  doi       = {10.1109/VR.2013.6549344},
  issn      = {1087-8270},
}

@InProceedings{6290956,
  author    = {P. E. Dupont and C. N. Riviere and R. Patel and R. D. Howe and G. Fischer and R. Coppola and A. Menciassi and N. Simaan and M. Fujie and V. Krovi and S. Misra and P. Poignet and R. J. Webster},
  title     = {Symposium on surgical robotics #x2014; Invited talks: Robotic percutaneous beating-heart intracardiac surgery},
  booktitle = {2012 4th IEEE RAS EMBS International Conference on Biomedical Robotics and Biomechatronics (BioRob)},
  year      = {2012},
  pages     = {lxiii-lxx},
  month     = {June},
  abstract  = {These invited talks discuss the following: Robotic Percutaneous Beating-Heart Intracardiac Surgery; Beating Hearts And Trembling Hands: Active And Passive Compensation Of Physiological Motion For Surgery; Optimal Planning For Robotics-Assisted Minimally Invasive Cardiac Surgery; Why 3D Ultrasound isn't the Default Image Guidance Modality-and Why it Should Be; Towards Real-time Needle Path Control in Live MRI; Robotics in general surgery: our first year experience; New Generation Robots for Endoluminal and Single Port Surgery; Intelligent Continuum Surgical Slaves; Proactive activities for a healthier society with intelligent medical robot based on physical model of human; Quantitative Skill Assessment within an Augmented Reality Biopsy Simulator; Predicting Target Motion for Planning of Medical Interventions; Needle insertion guidance: from adaptive motion planning to model-based control; and Enabling Technologies for Robot-Assisted Endonasal Skull Base Surgery.},
  doi       = {10.1109/BioRob.2012.6290956},
  issn      = {2155-1774},
  keywords  = {adaptive control;augmented reality;cardiology;compensation;intelligent robots;medical image processing;medical robotics;motion control;path planning;physiology;service robots;surgery;ultrasonic imaging;3D ultrasound;active compensation;adaptive motion planning;augmented reality biopsy simulator;default image guidance modality;endoluminal surgery;healthier society;human physical model;intelligent continuum surgical slaves;intelligent medical robot;live MRI;medical interventions planning;model-based control;needle insertion guidance;new generation robots;passive compensation;physiological motion;proactive activities;quantitative skill assessment;real-time needle path control;robot-assisted endonasal skull base surgery;robotic percutaneous beating-heart intracardiac surgery;robotics-assisted minimally invasive cardiac surgery;single port surgery;target motion prediction;trembling hands},
}

@InProceedings{4685284,
  title     = {[Front matter]},
  booktitle = {2008 IEEE International Workshop on Haptic Audio visual Environments and Games},
  year      = {2008},
  pages     = {i-iii},
  month     = {Oct},
  abstract  = {The following topics are dealt with: haptic audio visual environment; games; surgical applications; medical applications; distributed collaborative virtual environment; telepresence; augmented reality; human-computer interaction; rendering; motion modeling; shape modeling and object modeling.},
  doi       = {10.1109/HAVE.2008.4685284},
  keywords  = {computer games;groupware;haptic interfaces;human computer interaction;rendering (computer graphics);virtual reality;augmented reality;distributed collaborative virtual environment;games;haptic audio visual environment;human-computer interaction;medical application;motion modeling;object modeling;shape modeling;surgical application;telepresence},
}

@InProceedings{930255,
  title     = {Proceedings International Workshop on Medical Imaging and Augmented Reality},
  booktitle = {Proceedings International Workshop on Medical Imaging and Augmented Reality},
  year      = {2001},
  pages     = {iii-},
  month     = {June},
  abstract  = {The following topics were dealt with: intervention and surgical planning; ultra-fast imaging and intervention; adaptive and functional imaging in cardiovascular MRI; visualization and augmented reality; computational simulation and modelling; image processing and reconstruction; and image segmentation and understanding},
  doi       = {10.1109/MIAR.2001.930255},
  keywords  = {augmented reality;medical image processing;adaptive imaging;augmented reality;cardiovascular MRI;computational simulation;data visualization;functional imaging;image processing;image reconstruction;image segmentation;image understanding;medical imaging;modelling;surgical intervention;surgical planning;ultra-fast imaging},
}

@InProceedings{7989892,
  author    = {Q. H. Van and M. Harders},
  title     = {Augmenting contact stiffness in passive haptics #x2014; Preliminary results with twisted string actuation},
  booktitle = {2017 IEEE World Haptics Conference (WHC)},
  year      = {2017},
  pages     = {148-153},
  month     = {June},
  abstract  = {Haptic feedback is often employed in medical simulators, with the goal to improve user interaction and the training outcome. One option for providing touch sensations is using passive haptics, by including actual physical mock-ups of the anatomical objects in the simulated scene. While this approach has advantages, the usual one-to-one mapping between virtual and physical objects is a fundamental drawback, especially when invasive scene alterations are to be performed, such as cutting or drilling. In this work we propose to alleviate this situation by modifying the mock surgical instruments used for interaction. Twisted string actuation is employed to display variable stiffness while indenting an anatomical model. Quantitative experiments characterizing the performance of a testbed are reported and a prototype system for a surgical bone drill is introduced. Results show that the setup is capable of providing different stiffness augmentations, representing varying bone densities.},
  doi       = {10.1109/WHC.2017.7989892},
  keywords  = {augmented reality;biomedical education;bone;computer based training;elastic constants;haptic interfaces;medical computing;surgery;bone densities;contact stiffness;haptic feedback;invasive scene alterations;medical training simulations;mock surgical instruments;passive haptics;stiffness augmentations;surgical bone drill;surgical training;touch sensations;twisted string actuation;user interaction;Force;Force measurement;Haptic interfaces;Instruments;Springs;Surgery;Training},
}

@Article{7907202,
  author   = {P. Melillo and D. Riccio and L. Di Perna and G. Sanniti Di Baja and M. De Nino and S. Rossi and F. Testa and F. Simonelli and M. Frucci},
  title    = {Wearable Improved Vision System for Color Vision Deficiency Correction},
  journal  = {IEEE Journal of Translational Engineering in Health and Medicine},
  year     = {2017},
  volume   = {5},
  pages    = {1-7},
  issn     = {2168-2372},
  abstract = {Color vision deficiency (CVD) is an extremely frequent vision impairment that compromises the ability to recognize colors. In order to improve color vision in a subject with CVD, we designed and developed a wearable improved vision system based on an augmented reality device. The system was validated in a clinical pilot study on 24 subjects with CVD (18 males and 6 females, aged 37.4 ± 14.2 years). The primary outcome was the improvement in the Ishihara Vision Test score with the correction proposed by our system. The Ishihara test score significantly improved ($p = 0.03$ ) from 5.8 ± 3.0 without correction to 14.8 ± 5.0 with correction. Almost all patients showed an improvement in color vision, as shown by the increased test scores. Moreover, with our system, 12 subjects (50%) passed the vision color test as normal vision subjects. The development and preliminary validation of the proposed platform confirm that a wearable augmented-reality device could be an effective aid to improve color vision in subjects with CVD.},
  doi      = {10.1109/JTEHM.2017.2679746},
  keywords = {augmented reality;biomedical equipment;colour vision;Ishihara Vision Test score;color vision deficiency correction;wearable augmented-reality device;wearable improved vision system;Augmented reality;Cameras;Color;Dentistry;Electronic mail;Image color analysis;Surgery;Augmented reality;color vision deficiency;medical device;wearable device},
}

@InProceedings{7591252,
  author    = {Y. Nagaraj and C. Benedicks and P. Matthies and M. Friebe},
  title     = {Advanced inside-out tracking approach for real-time combination of MRI and US images in the radio-frequency shielded room using combination markers},
  booktitle = {2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
  year      = {2016},
  pages     = {2558-2561},
  month     = {Aug},
  abstract  = {For the real-time fusion of different modalities, a variety of tracking methods are available including the optical, electromagnetic (EM) and image-based tracking. But as a drawback optical tracking suffers from line of sight issues and EM tracking requires the manual referencing for the fusion procedure and is not usable in Magnetic Resonance Imaging (MRI) environment. To avoid these issues, we propose a real-time setup containing a camera capable of inside-Out tracking using combined circular markers attached to Ultrasound (US) probe and a suitable platform for automatic overlay of MRI and US image using markers. This new approach could help clinicians carry out successful surgical procedures by requiring least system interaction and solving line of sight issues. As a proof-of-concept, we show our first result by mimicking common liver tumor intervention using framed marker fusion technique in a candle gel phantom. We evaluated the tracking error distances using the combination of special markers with Inside-Out approach and conventional optical tracking. The results achieved show comparable performance to the standard Outside-In tracking and manual reference approach, while easing the interventional procedure in terms of hardware and line of sight requirements.},
  doi       = {10.1109/EMBC.2016.7591252},
  issn      = {1557-170X},
  keywords  = {biomedical MRI;biomedical ultrasonics;biomimetics;gels;image fusion;liver;medical image processing;optical tracking;phantoms;tumours;US-MRI image fusion procedure;candle gel phantom;electromagnetic tracking;framed marker fusion technique;image-based tracking;liver tumor intervention;magnetic resonance imaging;optical tracking;radiofrequency shielded room;ultrasound probe;Adaptive optics;Cameras;Magnetic resonance imaging;Optical imaging;Phantoms;Probes;Target tracking;External Markers;Fiducials;Inside-Out;Tracking;US-MR fusion},
}

@InProceedings{7591705,
  author    = {B. A. Ponce and E. W. Brabston and S. Zu and S. L. Watson and D. Baker and D. Winn and B. L. Guthrie and M. B. Shenai},
  title     = {Telemedicine with mobile devices and augmented reality for early postoperative care},
  booktitle = {2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
  year      = {2016},
  pages     = {4411-4414},
  month     = {Aug},
  abstract  = {Advanced features are being added to telemedicine paradigms to enhance usability and usefulness. Virtual Interactive Presence (VIP) is a technology that allows a surgeon and patient to interact in a “merged reality” space, to facilitate both verbal, visual, and manual interaction. In this clinical study, a mobile VIP iOS application was introduced into routine post-operative orthopedic and neurosurgical care. Survey responses endorse the usefulness of this tool, as it relates to The virtual interaction provides needed virtual follow-up in instances where in-person follow-up may be limited, and enhances the subjective patient experience.},
  doi       = {10.1109/EMBC.2016.7591705},
  issn      = {1557-170X},
  keywords  = {health care;iOS (operating system);patient care;smart phones;telemedicine;virtual reality;augmented reality;early postoperative care;in-person follow-up;merged reality space;mobile VIP iOS application;mobile device;post-operative neurosurgical care;post-operative orthopedic care;telemedicine;virtual follow-up;virtual interactive presence;Inspection;Mobile handsets;Surgery;Telemedicine;Visualization;Wounds},
}

@Article{7407375,
  author   = {D. Gorpas and D. Ma and J. Bec and D. R. Yankelevich and L. Marcu},
  title    = {Real-Time Visualization of Tissue Surface Biochemical Features Derived From Fluorescence Lifetime Measurements},
  journal  = {IEEE Transactions on Medical Imaging},
  year     = {2016},
  volume   = {35},
  number   = {8},
  pages    = {1802-1811},
  month    = {Aug},
  issn     = {0278-0062},
  abstract = {Fiber based fluorescence lifetime imaging has shown great potential for intraoperative diagnosis and guidance of surgical procedures. Here we describe a novel method addressing a significant challenge for the practical implementation of this technique, i.e., the real-time display of the quantified biochemical or functional tissue properties superimposed on the interrogated area. Specifically, an aiming beam (450 nm) generated by a continuous-wave laser beam was merged with the pulsed fluorescence excitation light in a single delivery/collection fiber and then imaged and segmented using a color-based algorithm. We demonstrate that this approach enables continuous delineation of the interrogated location and dynamic augmentation of the acquired frames with the corresponding fluorescence decay parameters. The method was evaluated on a fluorescence phantom and fresh tissue samples. Current results demonstrate that 34 frames per second can be achieved for augmenting videos of 640 × 512 pixels resolution. Also we show that the spatial resolution of the fluorescence lifetime map depends on the tissue optical properties, the scanning speed, and the frame rate. The dice similarity coefficient between the fluorescence phantom and the reconstructed maps was estimated to be as high as 93%. The reported method could become a valuable tool for augmenting the surgeon's field of view with diagnostic information derived from the analysis of fluorescence lifetime data in real-time using handheld, automated, or endoscopic scanning systems. Current method provides also a means for maintaining the tissue light exposure within safety limits. This study provides a framework for using an aiming beam with other point spectroscopy applications.},
  doi      = {10.1109/TMI.2016.2530621},
  keywords = {biochemistry;biological tissues;biomedical optical imaging;endoscopes;fluorescence;image reconstruction;image resolution;image segmentation;laser applications in medicine;medical image processing;phantoms;augmenting videos;automated scanning systems;color-based algorithm;continuous delineation;continuous-wave laser beam;diagnostic information;dynamic augmentation;endoscopic scanning systems;fiber based fluorescence lifetime imaging;fluorescence decay parameters;fluorescence lifetime map;fluorescence lifetime measurements;fluorescence phantom;fresh tissue samples;image segmentation;interrogated location;intraoperative diagnosis;pixel resolution;point spectroscopy applications;pulsed fluorescence excitation light;real-time visualization;single delivery-collection fiber;spatial resolution;surgery;tissue light exposure;tissue surface biochemical features;Fluorescence;Image segmentation;Imaging;Laser beams;Real-time systems;Structural beams;Videos;Aiming beam;augmented reality;color segmentation;fluorescence lifetime imaging;image overlay;real-time imaging},
}

@InProceedings{7517250,
  author    = {P. V. F. Paiva and L. D. S. Machado and A. M. G. Valença and R. M. De Moraes and T. V. V. Batista},
  title     = {Enhancing Collaboration on a Cloud-Based CVE for Supporting Surgical Education},
  booktitle = {2016 XVIII Symposium on Virtual and Augmented Reality (SVR)},
  year      = {2016},
  pages     = {29-36},
  month     = {June},
  abstract  = {In recent decades, a greater number of Collaborative Virtual Environments (CVEs) have been developed and sought for collaborative training of medical personnel in VR. Observing the needs identified in the literature, a multidisciplinary team developed a collaborative simulator for education and assessment of student groups in basic surgical routines, called SimCEC. The system was developed according to a strict methodology of design. Considering important needs with regard to storage guarantee, consistency and availability of the CVE, SimCEC was recently added by a cloud data distribution architecture for managing multiple virtual rooms for training of student teams, enabling collaboration among different areas of health. This paper discusses the theoretical and practical aspects of such tool, as well as its advantages and the new possibilities offered for surgical education area.},
  doi       = {10.1109/SVR.2016.16},
  keywords  = {biomedical education;cloud computing;computer aided instruction;digital simulation;groupware;medical computing;surgery;virtual reality;CVE availability;CVE consistency;SimCEC;VR;cloud data distribution architecture;cloud-based CVE;collaboration enhancement;collaborative simulator;collaborative training;collaborative virtual environments;medical personnel;multiple virtual room management;storage guarantee;student group assessment;student group education;student team training;surgical education;Cloud computing;Collaboration;Surgery;Three-dimensional displays;Training;Virtual environments;Cloud Computing;Collaborative Virtual Environments;Surgical training;Virtual Reality},
}

@InProceedings{7454467,
  author    = {B. Marques and R. Plantefève and F. Roy and N. Haouchine and E. Jeanvoine and I. Peterlik and S. Cotin},
  title     = {Framework for augmented reality in Minimally Invasive laparoscopic surgery},
  booktitle = {2015 17th International Conference on E-health Networking, Application Services (HealthCom)},
  year      = {2015},
  pages     = {22-27},
  month     = {Oct},
  abstract  = {This article presents a framework for fusing preoperative data and intra-operative data for surgery guidance. This framework is employed in the context of Minimally Invasive Surgery (MIS) of the liver. From stereoscopic images a three dimensional point cloud is reconstructed in real-time. This point cloud is then used to register a patient-specific biomechanical model derived from Computed Tomography images onto the laparoscopic view. In this way internal structures such as vessels and tumors can be visualized to help the surgeon during the procedure. This is particularly relevant since abdominal organs undergo large deformations in the course of the surgery, making it difficult for surgeons to correlate the laparoscopic view with the pre-operative images. Our method has the potential to reduce the duration of the operation as the biomechanical model makes it possible to estimate the in-depth position of tumors and vessels at any time of the surgery, which is essential to the surgical decision process. Results show that our method can be successfully applied during laparoscopic procedure without interfering with the surgical work flow.},
  doi       = {10.1109/HealthCom.2015.7454467},
  keywords  = {augmented reality;biomechanics;computerised tomography;liver;medical image processing;surgery;tumours;abdominal organs;augmented reality;computed tomography images;deformations;liver;minimally invasive laparoscopic surgery;patient-specific biomechanical model;tumors;vessels;Biological system modeling;Biomechanics;Image reconstruction;Laparoscopes;Liver;Surgery;Three-dimensional displays},
}

@InProceedings{7439546,
  author    = {K. Watanabe and M. Yagi and A. Shintani and S. Nankaku and K. Onishi and M. Koeda and H. Noborio and M. Kon and K. Matsui and M. Kaibori},
  title     = {A new 2D depth-depth matching algorithm whose translation and rotation freedoms are separated},
  booktitle = {2015 International Conference on Intelligent Informatics and Biomedical Sciences (ICIIBMS)},
  year      = {2015},
  pages     = {271-278},
  month     = {Nov},
  abstract  = {In this paper, we revise a previous 2D depth-depth-matching algorithm in order to copy motions fast from a real liver to a virtual liver in a surgical navigation. The real liver is always captured by 3D depth camera, and the virtual liver is represented by a polyhedron with STL format via DICOM captured by MRI/CT. In our algorithm, we firstly compare a 2D depth image in a real world and the Z-buffer of STL in a virtual world, and by using the difference of two depth images, we secondly search the best movement of a virtual liver from a huge number of possibilities with 3 translation and 3 rotation degrees-of-freedom. In this paper, we firstly divide translation and rotation D.O.F, and individually select the most adequate 3 DOF sets of a virtual liver following its real liver. Based on the division, we can find a sequence of following motions more precise and faster than our previous 2D depth-depth-matching algorithms.},
  doi       = {10.1109/ICIIBMS.2015.7439546},
  keywords  = {augmented reality;biomedical MRI;cameras;computerised tomography;graphics processing units;image matching;liver;medical image processing;surgery;2D depth-depth matching algorithm;3D depth camera;AR;CT image;GPU;MRI image;augmented reality;rotation freedom;surgical navigation;translation freedom;virtual liver;Algorithm design and analysis;Bioinformatics;Biomedical imaging;Graphics processing units;Liver;Surgery;Three-dimensional displays;Depth Camera;Depth Image;GPU;Parallel Processing;Randomized Steepest Descendent Method;Z-buffer},
}

@Article{7321051,
  author   = {B. Fuerst and J. Sprung and F. Pinto and B. Frisch and T. Wendler and H. Simon and L. Mengus and N. S. van den Berg and H. G. van der Poel and F. W. B. van Leeuwen and N. Navab},
  title    = {First Robotic SPECT for Minimally Invasive Sentinel Lymph Node Mapping},
  journal  = {IEEE Transactions on Medical Imaging},
  year     = {2016},
  volume   = {35},
  number   = {3},
  pages    = {830-838},
  month    = {March},
  issn     = {0278-0062},
  abstract = {In this paper we present the usage of a drop-in gamma probe for intra-operative Single-Photon Emission Computed Tomography (SPECT) imaging in the scope of minimally invasive robot-assisted interventions. The probe is designed to be inserted and reside inside the abdominal cavity during the intervention. It is grasped during the procedure using a robotic laparoscopic gripper enabling full six degrees of freedom handling by the surgeon. We demonstrate the first deployment of the tracked probe for intra-operative in-patient robotic SPECT enabling augmented-reality image guidance. The hybrid mechanical- and image-based in-patient probe tracking is shown to have an accuracy of 0.2 mm. The overall system performance is evaluated and tested with a phantom for gynecological sentinel lymph node interventions and compared to ground-truth data yielding a mean reconstruction accuracy of 0.67 mm.},
  doi      = {10.1109/TMI.2015.2498125},
  keywords = {augmented reality;grippers;gynaecology;image reconstruction;medical image processing;medical robotics;single photon emission computed tomography;surgery;abdominal cavity;augmented-reality image guidance;drop-in gamma probe;gynecological sentinel lymph node interventions;intraoperative-in-patient robotic SPECT;minimally invasive robot-assisted interventions;minimally invasive sentinel lymph node mapping;phantom;robotic laparoscopic gripper;single-photon emission computed tomography;Lymph nodes;Probes;Robots;Single photon emission computed tomography;Surgery;Tracking;Endoscopy/laparoscopy;SPECT;image-guided treatment;surgical guidance/navigation},
}

@InProceedings{7372343,
  author    = {M. Sugimoto},
  title     = {Augmented Tangibility Surgical Navigation Using Spatial Interactive 3-D Hologram zSpace with OsiriX and Bio-Texture 3-D Organ Modeling},
  booktitle = {2015 International Conference on Computer Application Technologies},
  year      = {2015},
  pages     = {189-194},
  month     = {Aug},
  abstract  = {We developed a new spatial navigation system for medical informatics by interactive superimposing 3-D hologram and 3D printing technology. Interactive stereo display was used for the existence of an interaction between the users and stereo images of the patient's anatomy depicted on the display in the form of tracking the user's head and hand/arm position. Sensing the user's head position created motion parallax information, an immersive depth cue that can be added to the binocular parallax already present in the display. We also developed new technology of bio-texture modeling by multi-material 3D printing to form 3D organ textures and structures. Based on patient-specific MDCT data sets, regions of interest were segmented using DICOM viewer OsiriX application. After generating 3D surface models of the organ and STL file out of the patient's 3D data, the inkjet 3D printer created a 3D multi-material organ replica. Sensing the user's hand or arm position using motion sensor attached the patient's life size 3-D printed organ model, allowed the user to manipulate the spatial attributes of the virtual and real printed organs, which can enhance spatial reasoning and augmented tangibility. These tangible organ replication provide better anatomical reference tool as a tailormade simulation and navigation, and contribute to medical safety and accuracy, less-invasiveness and improvement of the medical education for students and trainees.},
  doi       = {10.1109/CCATS.2015.53},
  keywords  = {augmented reality;biological organs;computerised tomography;holography;image motion analysis;image segmentation;image texture;ink jet printing;medical image processing;stereo image processing;surgery;three-dimensional printing;3D multimaterial organ replica;3D organ textures;3D printing technology;3D surface models;Bio-Texture 3D organ modeling;OsiriX 3D organ modeling;STL file;anatomical reference tool;arm position;augmented tangibility;augmented tangibility surgical navigation;binocular parallax;bio-texture modeling;hand-arm position;immersive depth cue;inkjet 3D printer;interactive stereo display;interactive superimposing 3-D hologram;medical accuracy;medical education;medical informatics;medical safety;motion parallax information;motion sensor;multimaterial 3D printing;patient anatomy;patient's life size 3D printed organ model;patient-specific MDCT data sets;real printed organs;regions-of-interest;segmented DICOM viewer OsiriX application;spatial attributes;spatial interactive 3D hologram zSpace;spatial navigation system;spatial reasoning;stereo images;tailor-made navigation;tailor-made simulation;tangible organ replication;user's head position;user's head tracking;virtual printed organs;Biological systems;Medical diagnostic imaging;Navigation;Solid modeling;Surgery;Three-dimensional displays;3-D printer;Augmented reality;Mixed reality;OsiriX;Virtual reality},
}

@InProceedings{7328053,
  author    = {S. Habert and J. Gardiazabal and P. Fallavollita and N. Navab},
  title     = {RGBDX: First Design and Experimental Validation of a Mirror-Based RGBD X-ray Imaging System},
  booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
  year      = {2015},
  pages     = {13-18},
  month     = {Sept},
  abstract  = {This paper presents the first design of a mirror based RGBD X-ray imaging system and includes an evaluation study of the depth errors induced by the mirror when used in combination with an infrared pattern-emission RGBD camera. Our evaluation consisted of three experiments. The first demonstrated almost no difference in depth measurements of the camera with and without the use of the mirror. The final two experiments demonstrated that there were no relative and location-specific errors induced by the mirror showing the feasibility of the RGBDX-ray imaging system. Lastly, we showcase the potential of the RGBDX-ray system towards a visualization application in which an X-ray image is fused to the 3D reconstruction of the surgical scene via the RGBD camera, using automatic C-arm pose estimation.},
  doi       = {10.1109/ISMAR.2015.17},
  keywords  = {data visualisation;diagnostic radiography;image colour analysis;image reconstruction;medical image processing;pose estimation;RGBDX system;automatic C-arm pose estimation;infrared pattern-emission RGBD camera;mirror-based RGBD X-ray imaging system;red-green-blue depth imaging;surgical scene reconstruction;visualization application;Cameras;Mirrors;Rendering (computer graphics);Surface reconstruction;Surgery;Three-dimensional displays;X-ray imaging;Medical Augmented Reality;Multi-modal Visualization;Range Imaging;X-ray imaging},
}

@InProceedings{7300734,
  author    = {I. F. M. S. d. Moura and L. d. S. Machado},
  title     = {Study of Cutting Techniques in Simulations with Virtual Reality},
  booktitle = {2015 XVII Symposium on Virtual and Augmented Reality},
  year      = {2015},
  pages     = {107-111},
  month     = {May},
  abstract  = {Several areas of knowledge have become interested in simulators made using Virtual Reality (VR) because of the possibility of reducing risks, costs, etc. One of these areas is Medicine, which currently has a range of simulators for the practice of surgical procedures. A large number of components is required to assemble these simulators and the ability to simulate incisions in the models of the organs involved one of them. We studied four existing cutting techniques, defining their requirements and exposing their peculiarities. A categorization of cutting techniques relevant to the context of VR simulations was also studied and applied to the four techniques previously studied. Finally, the applicability of these techniques in a scenario of surgical simulation with VR is discussed.},
  doi       = {10.1109/SVR.2015.22},
  keywords  = {cost reduction;medicine;risk analysis;simulation;surgery;virtual reality;VR;cost reduction;cutting techniques;medicine;risk reduction;simulations;surgical procedures;virtual reality;Biomedical imaging;Computational modeling;Instruments;Software;Solid modeling;Surgery;Virtual reality;cutting;incision;medicine;simulation;surgery;virtual reality},
}

@InProceedings{7300745,
  author    = {P. V. d. F. Paiva and L. d. S. Machado and T. V. V. Batista},
  title     = {A Collaborative and Immersive VR Simulator for Education and Assessment of Surgical Teams},
  booktitle = {2015 XVII Symposium on Virtual and Augmented Reality},
  year      = {2015},
  pages     = {176-185},
  month     = {May},
  abstract  = {Traditionally, the evaluation of surgical procedures in VR simulators have been restricted to their individual technical aspects, and disregarded the procedures carried out by teams. However, some decision models have been proposed in order to be incorporated and support the collaborative training evaluation process of surgical teams in Collaborative Virtual Environments (CVEs). This paper aims to discuss some possibilities and advantages of using the CVEs for the training process and assessment of surgical teams as well as presenting the steps of planning and development of a new simulator with these features, called SimCEC. The discussion is held in a computational modelling and implementation optic.},
  doi       = {10.1109/SVR.2015.33},
  keywords  = {biomedical education;computer based training;groupware;medical computing;surgery;virtual reality;CVE;SimCEC;collaborative training evaluation process;collaborative virtual environments;education;immersive VR simulator;surgical team assessment;Collaboration;Computational modeling;Real-time systems;Servers;Surgery;Training;SimCEC;VR;collaborative virtual environments;surgical simulation;user's assessment},
}

@InProceedings{7223338,
  author    = {S. de Ribaupierre and R. Armstrong and D. Noltie and M. Kramers and R. Eagleson},
  title     = {VR and AR simulator for neurosurgical training},
  booktitle = {2015 IEEE Virtual Reality (VR)},
  year      = {2015},
  pages     = {147-148},
  month     = {March},
  abstract  = {The placement of an external ventricular drain is one of the most commonly performed neurosurgical procedures, and consequently, is an essential skill to be mastered by neurosurgical trainees. The optimal placement of the drain involves choosing an appropriate burr hole on the skull and blindly placing a catheter through the burr hole to intersect a lateral ventricle in order to drain cerebrospinal fluid and relieve intracranial pressure. Undesirable trajectories lead to multiple tries in order to hit the ventricle, with potential risk of damaging eloquent brain areas. In this paper, we describe the development of a simulation environment to train residents on the acquisition of these targeting skills before attempting the placement on live patients. The platform is coupled with an augmented reality image-guidance tool, developed in our lab, to help with the visualization of the ventricles in the patient's head. Performance is evaluated using Fitts' methodology (Fitts, 1954), which respects the users ability to trade-off speed and accuracy.},
  doi       = {10.1109/VR.2015.7223338},
  issn      = {1087-8270},
  keywords  = {augmented reality;biomedical education;computer based training;data visualisation;medical computing;neurophysiology;surgery;AR simulator;Fitts methodology;VR simulator;augmented reality image-guidance tool;burr hole;catheter;cerebrospinal fluid;eloquent brain areas;external ventricular drain;intracranial pressure;lateral ventricle;live patients;neurosurgical procedures;neurosurgical trainees;neurosurgical training;patient head;targeting skills;ventricles visualization;Accuracy;Augmented reality;Head;Neurosurgery;Solid modeling;Training;Trajectory;Augmented Reality;Surgical Simulation;Virtual Reality},
}

@InProceedings{7016131,
  author    = {T. Tokuyasu and W. Okamura and T. Kusano and M. Inomata and N. Shiraishi and S. Kitanou},
  title     = {Training System for Endoscopic Surgery by Using Augmented Reality and Forceps Control Devices},
  booktitle = {2014 Ninth International Conference on Broadband and Wireless Computing, Communication and Applications},
  year      = {2014},
  pages     = {541-544},
  month     = {Nov},
  abstract  = {Endoscopic surgery is one of minimally invasive medical treatments, which reduces both physical and mental burdens of a patient than conventional abdominal operations. At present, a variety of training environments for endoscopic surgery, such as a dry box, a virtual reality (VR) simulator, and an animal experiment have been prepared to cultivate certifying physicians of endoscopic surgery. Even now, more effective training environments have been requested from the surgeons. Thus, this study proposes an application of augmented reality to establish a novel training environment of endoscopic surgery. We focused on one of Japanese learning styles in writing, in which children repeatedly trace the letters written on a sheet in order to learn how to use a pencil to write neatly without concerning the meanings of the letters. This study applies this learning style to our training system by using a dry box and augmented reality (AR). In our training system, AR Toolkit is utilized to describe a target trajectory which is displayed on a monitor and is refereed by a trainee. And we originally develops the devices which detect the motions of endoscopic forceps while training. This paper introduces the basic constitution of our training system and discusses about the training effects obtained from the experimental results with beginner subjects.},
  doi       = {10.1109/BWCCA.2014.113},
  keywords  = {augmented reality;computer based training;endoscopes;medical computing;surgery;AR Toolkit;Japanese learning styles;animal experiment;augmented reality;conventional abdominal operations;endoscopic forceps;endoscopic surgery;forceps control devices;minimally invasive medical treatments;monitor;target trajectory;training system;virtual reality simulator;Animals;Augmented reality;Cameras;Surgery;Training;Trajectory;Augmented reality;endoscopic surgery;medical education;surgical simulator},
}

@InProceedings{6948504,
  author    = {F. Cutolo and P. D. Parchi and V. Ferrari},
  title     = {Video see through AR head-mounted display for medical procedures},
  booktitle = {2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
  year      = {2014},
  pages     = {393-396},
  month     = {Sept},
  abstract  = {In the context of image-guided surgery (IGS), AR technology appears as a significant development in the field since it complements and integrates the concepts of surgical navigation based on virtual reality. The aim of the project is to optimize and validate an ergonomic, accurate and cheap video see-through AR system as an aid in various typologies of surgical procedures. The system will ideally have to be inexpensive and user-friendly to be successfully introduced in the clinical practice.},
  doi       = {10.1109/ISMAR.2014.6948504},
  keywords  = {Image-guided surgery;Interest point and salient region detections;Medical device validation;Mixed / augmented reality;Object detection;Tracking},
}

@InProceedings{6913104,
  author    = {É. S. Silva and M. A. F. Rodrigues},
  title     = {A Gesture Control System for Aiding Surgical Procedures},
  booktitle = {2014 XVI Symposium on Virtual and Augmented Reality},
  year      = {2014},
  pages     = {287-296},
  month     = {May},
  abstract  = {In this work, we present a gesture control system for aiding surgical procedures using the Kinect device. Several abstractions have been implemented, making this process simpler, at an affordable cost. Procedures for visualization and controlling of medical images, routinely performed in surgical theatres, were also successfully modeled, including gesture control of radiological images. We have also conducted initial usability testings with users to determine performance measures, the influence of gesture control on perceived usefulness and ease of use of the system. The results show that the participants are able to perform the tasks of search, selection and manipulation of 2D images and 3D models, quickly and accurately, demonstrating the usefulness of the system as a possible effective and competitive alternative, to the traditional use of the negatoscope, for gesture based interactive control of medical images.},
  doi       = {10.1109/SVR.2014.34},
  keywords  = {control engineering computing;gesture recognition;medical control systems;medical image processing;radiology;surgery;Kinect device;gesture based interactive control;gesture control system;medical images;negatoscope;radiological images;surgical procedures;surgical theatres;usability testings;Biomedical imaging;Control systems;Solid modeling;Support vector machines;Surgery;Three-dimensional displays;Visualization;Evaluation;Gesture Control System;Interaction;Surgical Procedures},
}

@InProceedings{6913099,
  author    = {D. A. T. Q. Leite and J. C. Duarte and J. C. Oliveira and V. d. A. Thomaz and G. A. Giraldi},
  title     = {A System to Interact with CAVE Applications Using Hand Gesture Recognition from Depth Data},
  booktitle = {2014 XVI Symposium on Virtual and Augmented Reality},
  year      = {2014},
  pages     = {246-253},
  month     = {May},
  abstract  = {Human Computer Interaction (HCI) is a fundamental issue for virtual reality environments due to the need for natural approaches and comfortable devices. Such goals can be achieved using hand gestures to interact with the virtual reality engine. This paper presents a real-time system based on hand gesture recognition (HGR) for interaction with CAVE applications. The whole pipeline can be roughly divided into four steps: segmentation, feature extraction for bag-of-features construction, classification through multiclass support vector machine (SVM), generation of commands to control the application. We build a grammar based on the hand gesture classes to convert the classification results in control commands for an application running in a CAVE. The input is the depth stream data acquired from a Kinect device. The hand gesture recognition and command generation/execution approaches compose a client-server plug in that is part of a CAVE system implemented based on the Instant Reality architecture and the X3D standard. The results show that the implemented plug in is a promising solution. We achieve suitable recognition accuracy and efficient object manipulation in a virtual room representing a surgical environment visualized in the CAVE.},
  doi       = {10.1109/SVR.2014.13},
  keywords  = {feature extraction;gesture recognition;image classification;image segmentation;image sensors;interactive devices;support vector machines;virtual reality;CAVE application;HCI;HGR;Instant Reality architecture;Kinect device;SVM;X3D standard;bag-of-features construction;classification step;client-server plug in;command execution approach;command generation approach;commands generation step;depth data;depth stream data;feature extraction step;hand gesture recognition;human computer interaction;multiclass support vector machine;object manipulation;segmentation step;surgical environment;virtual reality engine;virtual reality environment;virtual room;Cameras;Feature extraction;Gesture recognition;Hardware;Support vector machines;Three-dimensional displays;Training;CAVE;Computer Vision;Hand-Gesture Recognition},
}

@InProceedings{6913106,
  author    = {P. V. d. F. Paiva and L. d. S. Machado and A. M. G. Valença and R. M. d. Moraes},
  title     = {Surgical Simulation: Applying an Architecture for Collaborative Evaluation of Teams},
  booktitle = {2014 XVI Symposium on Virtual and Augmented Reality},
  year      = {2014},
  pages     = {307-310},
  month     = {May},
  abstract  = {Traditionally, the evaluation of medical procedures in VR simulators have been restricted to technical and individual aspects, and procedures performed by teams are normally disregarded. However, some computer architectures have been proposed in order to facilitate the evaluation process of surgical teams supported by Collaborative Virtual Environments (CVEs) training. This paper aims to discuss some possibilities and advantages of CVEs for teams assessment process as well as provide an architecture for collaborative assessment, covering its essential requirements. The discussion is based on the analysis of this evaluation architecture used in a collaborative simulator named SimCEC - Collaborative Simulator for Surgery Education.},
  doi       = {10.1109/SVR.2014.25},
  keywords  = {biomedical education;computer architecture;computer based training;digital simulation;distance learning;groupware;medical computing;surgery;virtual reality;CVE training;SimCEC;VR simulators;collaborative assessment;collaborative simulator for surgery education;collaborative team evaluation;collaborative virtual environment training;computer architectures;distance education;evaluation architecture;medical procedure evaluation;surgical simulation;surgical team evaluation;teams assessment process;Collaboration;Computer architecture;Instruments;Solid modeling;Surgery;Training;Virtual environments;collaborative virtual environments;distance education;skills assessment;surgery simulation},
}

@InProceedings{6734120,
  author    = {M. C. Ke and Y. H. Tseng and C. W. Chen and M. C. Ho and F. L. Lian and J. Y. Yen and W. L. Lin and Y. Y. Chen},
  title     = {Preliminary study of intracorporeal localization for endoscopy tracking},
  booktitle = {2013 CACS International Automatic Control Conference (CACS)},
  year      = {2013},
  pages     = {130-134},
  month     = {Dec},
  abstract  = {An intracorporeal optical tracking system for minimal invasive (MIS) surgery is proposed. MIS has been widely used in laparoscopic surgery since 1980s. Its advantages include small surgical cuts and short recovery time. However, because of the limited workspace and narrow view, MIS is more complicated and costs more time than open surgery. Therefore, the image-guided system plays an important role to give surgeons a better cognition, especially tracking the tip of the instruments. In this study, we proposed a novel approach which is based on intracorporeal optical tracking by using the optical markers with the endoscope. The approach can effectively immune from electromagnetic interference and achieve high tracking accuracy with average error of 1.1 mm.},
  doi       = {10.1109/CACS.2013.6734120},
  keywords  = {biomedical optical imaging;electromagnetic interference;endoscopes;optical tracking;surgery;electromagnetic interference;endoscopy tracking;intracorporeal optical tracking system;minimal invasive surgery;optical markers;tracking accuracy;Accuracy;Adaptive optics;Cameras;Endoscopes;Instruments;Optical imaging;Surgery;Augmented-reality;Electromagnetic interference;Intracorporeal optical tracking},
}

@InProceedings{6703872,
  author    = {Y. S. Chae and S. H. Lee and H. M. Oh and M. Y. Kim},
  title     = {Coordinates tracking and augmented reality system using bipolar X-ray fluoroscopy and stereo vision for image-guided neurosurgery},
  booktitle = {2013 13th International Conference on Control, Automation and Systems (ICCAS 2013)},
  year      = {2013},
  pages     = {107-112},
  month     = {Oct},
  abstract  = {In this paper multiple optic sensor system and algorithms to match coordinate systems for neurosurgery robot is proposed for O-arm or C-arm fluoroscopy. As a specific device mounted on fluoroscopy we make the optical axis of X-ray source and the optical sensor identical. This makes it capable of achieving simultaneously X-ray vision and camera vision of the same area without any other complex mapping between two images. This can be used to make augmented vision to show X-ray vision to surgeons under operation. Also because of the characteristic of neurosurgery the required measurement area is relatively small but neurosurgery is very demanding in terms of accuracy, so the measurement should be within small numbers of micrometers. For accurate measurement specially designed stereo vision is implemented with IR illumination and retro-reflective marker spheres. With a pair of stereo images from the sensor system the three-dimensional coordinates and pose of the markers are calculated. By attaching these markers to patient, surgical instruments, and robot each set of the coordinate systems can be tracked with the proposed sensor system.},
  doi       = {10.1109/ICCAS.2013.6703872},
  issn      = {2093-7121},
  keywords  = {augmented reality;distributed sensors;medical image processing;medical robotics;neurophysiology;optical sensors;radiography;stereo image processing;surgery;C-arm fluoroscopy;IR illumination;O-arm fluoroscopy;X-ray vision;augmented reality system;bipolar X-ray fluoroscopy;camera vision;coordinates tracking;image-guided neurosurgery;markers pose;markers three-dimensional coordinates;multiple optic sensor system;neurosurgery robot;retro-reflective marker spheres;stereo vision;surgical instruments;Cameras;Navigation;Neurosurgery;Robot kinematics;Robot vision systems;augmented reality;medical robot;navigation;retro-reflective marker sphere;stereo X-ray;stereo camera},
}

@InProceedings{6655758,
  author    = {P. V. d. F. Paiva and L. S. Machado and A. M. G. Valença},
  title     = {A Virtual Environment for Training and Assessment of Surgical Teams},
  booktitle = {2013 XV Symposium on Virtual and Augmented Reality},
  year      = {2013},
  pages     = {17-26},
  month     = {May},
  abstract  = {Collaborative Virtual Environments (CVEs) can improve the way remote users interact with one another while learning and training skills on a given task. One CVE's possibility to the health area is the simulation of medical procedures in which a group of remote users can train and interact simultaneously. Health area has been benefited from the advent of Virtual Reality (VR) especially in education area, where these systems present some advantages over traditional teaching methods such as: cost reduction for training, reducing the use of guinea pigs and anatomical specimens in laboratory practices as well the use of interactive teaching approaches. Another VR's feature is the ability to monitor user's actions for assessing training performance. Thus, statistical models are used in order to check whether a group performed the procedure correctly or not. The goal is to allow the formation of teams and the development of individual skills to work together. This work proposes and discusses one CVE's architecture for supporting training and assessment of team skills, during surgery simulation.},
  doi       = {10.1109/SVR.2013.22},
  keywords  = {biomedical education;computer based training;distance learning;groupware;medical computing;surgery;teaching;virtual reality;CVE;VR;anatomical specimen reduction;collaborative virtual environments;cost reduction;guinea pig reduction;health area;remote users;surgery simulation;surgical team assessment;surgical team training;traditional teaching methods;virtual reality;Collaboration;Computer architecture;Solid modeling;Surgery;Training;Virtual environments;collaborative virtual environments;distance education;skills assessment;surgery simulation},
}

@InProceedings{6655793,
  author    = {R. G. S. Correia and L. S. Machado},
  title     = {Methods for Interactive Cut for a Virtual Reality Framework},
  booktitle = {2013 XV Symposium on Virtual and Augmented Reality},
  year      = {2013},
  pages     = {256-259},
  month     = {May},
  abstract  = {Cutting is an important component for the development of applications that simulate surgical procedures. This paper discusses the study of some cutting methods, how they are classified according to their approach, some of these methods' features, their technique and their feasibility of integration in a framework for development of Virtual Reality applications.},
  doi       = {10.1109/SVR.2013.27},
  keywords  = {medical computing;pattern classification;surgery;virtual reality;cutting method classification;interactive cut methods;surgical procedure simulation;virtual reality framework;Augmented reality;Electronic mail;Instruments;Solid modeling;Surgery;Visualization;Virtual Reality;cutting methods;development of applications;framework},
}

@InProceedings{6570203,
  author    = {M. L. Wang and J. J. Wu and P. Y. Lee and M. H. Hu and A. Kumar and L. X. Chen and K. C. Liu and J. Marescaux and S. Nicolau and A. Vemuri and L. Soler},
  title     = {A landmark based registration technique for minimally invasive spinal surgery},
  booktitle = {2013 IEEE International Symposium on Consumer Electronics (ISCE)},
  year      = {2013},
  pages     = {235-236},
  month     = {June},
  abstract  = {This paper proposed a technique for skin curve registration based on landmark detection and calibrated camera-projector system for medical purpose. The technique applies the algorithm which registers a precalculated 3D model for the 2D images of the video frames. The algorithm first calculates the 3D locations of landmarks based upon the calibrated camera projector system and the matching the landmarks in the 3D model using CT-scan. We then generate the image to be projected on the skin curve of a patient from the registered 3D spinal model, and it is created for projecting on the pork or real patient for surgeons to process surgical procedure by utilizing their natural eyes system. The experiments demonstrate the proposed registration method on both animal and real patient and evaluated by several surgeons for spinal surgery.},
  doi       = {10.1109/ISCE.2013.6570203},
  issn      = {0747-668X},
  keywords  = {calibration;computerised tomography;image registration;image sensors;medical image processing;neurophysiology;skin;surgery;video signal processing;2D images;3D locations;calibrated camera-projector system;computerised tomography scans;landmark based registration technique;landmark detection;landmark matching;medical purpose;minimally invasive spinal surgery;natural eyes system;pork;precalculated 3D model;registered 3D spinal model;skin curve registration;surgeons;surgical procedure;video frames;Animals;Augmented reality;Biomedical imaging;Minimally invasive surgery;Skin;Solid modeling},
}

@InProceedings{6473445,
  author    = {M. L. Wang and J. R. Wu and K. C. Liu and P. Y. Lee and Y. Y. Chiang and H. Y. Lin},
  title     = {Innovative 3D augmented reality techniques for spinal surgery applications},
  booktitle = {2012 International Symposium on Intelligent Signal Processing and Communications Systems},
  year      = {2012},
  pages     = {16-20},
  month     = {Nov},
  abstract  = {This paper aims to be an outlet for speeding up workflow of orthopedics surgery and be helpful for surgeons to simply coordinate transformations between several imaging displays. Based on epipolar geometry and simple feature landmarks, a 3-D superimposed imaging approach is developed via the construction of the camera-projector system. The superimposed approach is to rectify the perspective, X-ray and projected image pair using the perspective projection model. The proposed method not only simplifies the computation between surgical instruments and patient for surgeons, but also reduces the radiation exposure. Experimental results for both the synthetic spinal image on dummy and real patient testing have demonstrated the feasibility of our approach.},
  doi       = {10.1109/ISPACS.2012.6473445},
  keywords  = {X-ray imaging;augmented reality;cameras;medical image processing;optical projectors;surgery;three-dimensional displays;3D superimposed imaging approach;X-ray-projected image pair;camera-projector system;epipolar geometry;feature landmarks;imaging displays;innovative 3D augmented reality technique;orthopedics surgery;patient testing;radiation exposure;spinal surgery application;surgeons;surgical instruments;synthetic spinal image;Augmented reality;Biomedical imaging;Cameras;Solid modeling;Surgery;Three-dimensional displays;Visualization;3-D Augmented Reality;Camera-projector system;Medical AR},
}

@InProceedings{6245647,
  author    = {A. De Mauro and J. Mazars and L. Manco and T. Mataj and A. H. Fernandez and C. Cortes and L. T. De Paolis},
  title     = {Intraoperative Navigation System for Image Guided Surgery},
  booktitle = {2012 Sixth International Conference on Complex, Intelligent, and Software Intensive Systems},
  year      = {2012},
  pages     = {486-490},
  month     = {July},
  abstract  = {Surgery is a field in which Human-Computer Interaction design and technical development is a critical success factor. Patient safety and surgical accuracy can take great advantages from a carefully designed user interface technology. The medical world needs easy and fast sharing of information. Prior to the use of any interface, an accurate analysis is required in order to understand if it meets medical needs. Normally, if the innovative concepts proposed relay on the use of existing medical devices it is more probable that new technology is successful. We present the initial development of software for the navigation in different types of stereo tactic surgeries. In this research, conventional medical interfaces for intra-operative visualization purposes are augmented with three-dimensional information provided to the surgeon in order to minimize mistakes.},
  doi       = {10.1109/CISIS.2012.174},
  keywords  = {augmented reality;biomedical equipment;data visualisation;human computer interaction;medical image processing;surgery;user interfaces;augmented reality;human-computer interaction design;image guided surgery;information sharing;intraoperative navigation system;intraoperative visualization purposes;medical devices;medical interfaces;medical needs;patient safety;software development;stereotactic surgeries;surgical accuracy;technical development;three-dimensional information;user interface technology;Augmented reality;Biomedical imaging;Endoscopes;Fasteners;Navigation;Neurosurgery;Augmented Reality;Image Guided Surgery;Medical Simulation;Minimally Invasive Surgery},
}

@InProceedings{6180913,
  author    = {E. Azimi and J. Doswell and P. Kazanzides},
  title     = {Augmented reality goggles with an integrated tracking system for navigation in neurosurgery},
  booktitle = {2012 IEEE Virtual Reality Workshops (VRW)},
  year      = {2012},
  pages     = {123-124},
  month     = {March},
  abstract  = {Precise tumor identification is crucial in image-guided neurosurgical procedures. With existing navigation systems, the surgeon must turn away from the patient to view the imaging data on a separate monitor. In this study, an innovative system is introduced that illustrates the tumor boundaries precisely augmented on the spot where the tumor is located with regard to the patient. Additionally, it allows the surgeon to track the distal end of the tools contextually, where direct visualization is not possible. In this approach, the tracking system is compact and worn by the surgeon, eliminating the need for additional devices that are bulky and typically limited by line of sight constraints.},
  doi       = {10.1109/VR.2012.6180913},
  issn      = {1087-8270},
  keywords  = {augmented reality;medical computing;neurophysiology;surgery;tumours;augmented reality goggles;direct visualization;image-guided neurosurgical procedures;integrated tracking system;navigation systems;neurosurgery;precise tumor identification;tumor boundaries;Adaptive optics;Augmented reality;Biomedical optical imaging;Cameras;Optical sensors;Surgery;Tumors;Augmented reality;HMD;neurosurgery;surgical navigation},
}

@Article{5975145,
  author   = {T. Edmunds and D. K. Pai},
  title    = {Perceptually Augmented Simulator Design},
  journal  = {IEEE Transactions on Haptics},
  year     = {2012},
  volume   = {5},
  number   = {1},
  pages    = {66-76},
  month    = {Jan},
  issn     = {1939-1412},
  abstract = {Training simulators have proven their worth in a variety of fields, from piloting to air-traffic control to nuclear power station monitoring. Designing surgical simulators, however, poses the challenge of creating trainers that effectively instill not only high-level understanding of the steps to be taken in a given situation, but also the low-level “muscle-memory” needed to perform delicate surgical procedures. It is often impossible to build an ideal simulator that perfectly mimics the haptic experience of a surgical procedure, but by focussing on the aspects of the experience that are perceptually salient we can build simulators that effectively instill learning. We propose a general method for the design of surgical simulators that augment the perceptually salient aspects of an interaction. Using this method, we can increase skill-transfer rates without requiring expensive improvements in the capability of the rendering hardware or the computational complexity of the simulation. In this paper, we present our decomposition-based method for surgical simulator design, and describe a user-study comparing the training effectiveness of a haptic-search-task simulator designed using our method versus an unaugmented simulator. The results show that perception-based task decomposition can be used to improve the design of surgical simulators that effectively impart skill by targeting perceptually significant aspects of the interaction.},
  doi      = {10.1109/TOH.2011.42},
  keywords = {augmented reality;computer based training;haptic interfaces;medical computing;surgery;air-traffic control;computational complexity;decomposition-based method;haptic experience;haptic-search-task simulator;low-level muscle-memory;nuclear power station monitoring;perception-based task decomposition;perceptually augmented simulator design;piloting;rendering hardware;salient interaction aspect;skill-transfer rate;surgical procedure;surgical simulator;training simulator;unaugmented simulator;user study;Haptic interfaces;Needles;Rendering (computer graphics);Rough surfaces;Surface roughness;Surgery;Training;Haptic I/O;and virtual realities;artificial;augmented;life and medical sciences;surgical simulation.},
}

@Article{5739103,
  author   = {C. T. Yeo and T. Ungi and P. U-Thainual and A. Lasso and R. C. McGraw and G. Fichtinger},
  title    = {The Effect of Augmented Reality Training on Percutaneous Needle Placement in Spinal Facet Joint Injections},
  journal  = {IEEE Transactions on Biomedical Engineering},
  year     = {2011},
  volume   = {58},
  number   = {7},
  pages    = {2031-2037},
  month    = {July},
  issn     = {0018-9294},
  abstract = {The purpose of this study was to determine if augmented reality image overlay and laser guidance systems can assist medical trainees in learning the correct placement of a needle for percutaneous facet joint injection. The Perk Station training suite was used to conduct and record the needle insertion procedures. A total of 40 volunteers were randomized into two groups of 20. 1) The Overlay group received a training session that consisted of four insertions with image and laser guidance, followed by two insertions with laser overlay only. 2) The Control group received a training session of six classical freehand insertions. Both groups then conducted two freehand insertions. The movement of the needle was tracked during the series of insertions. The final insertion procedure was assessed to determine if there was a benefit to the overlay method compared to the freehand insertions. The Overlay group had a better success rate (83.3% versus 68.4%, p = 0.002), and potential for less tissue damage as measured by the amount of needle movement inside the phantom (3077.6 mm2 versus 5607.9 mm2, p = 0.01). These results suggest that an augmented reality overlay guidance system can assist medical trainees in acquiring technical competence in a percutaneous needle insertion procedure.},
  doi      = {10.1109/TBME.2011.2132131},
  keywords = {augmented reality;biological tissues;cellular biophysics;computerised tomography;laser applications in medicine;learning (artificial intelligence);medical image processing;phantoms;augmented reality image overlay;augmented reality overlay guidance system;augmented reality training effect;classical freehand insertions;laser guidance systems;learning;medical trainees;needle insertion procedures;percutaneous facet joint injection;percutaneous needle placement;perk station training suite;phantom;spinal facet joint injections;tissue damage;Joints;Lasers;Needles;Phantoms;Three dimensional displays;Training;Medical simulation;modeling;skill assessment;surgical training;Computer Simulation;Computer-Assisted Instruction;Equipment Design;Humans;Injections, Spinal;Models, Anatomic;Needles;Orthopedic Procedures;Phantoms, Imaging;Surgery, Computer-Assisted;Zygapophyseal Joint},
}

@InProceedings{5872772,
  author    = {A. Joshi and D. Scheinost and R. Globinsky and K. P. Vives and D. D. Spencer and L. H. Staib and X. Papademetris},
  title     = {Augmented inline-based navigation for stereotactic image guided neurosurgery},
  booktitle = {2011 IEEE International Symposium on Biomedical Imaging: From Nano to Macro},
  year      = {2011},
  pages     = {1869-1872},
  month     = {March},
  abstract  = {Image-guided neurosurgery requires navigation in 3D using a computer-assisted surgery system that tracks surgical tools in realtime and displays their positions with respect to the preoperatively acquired images (e.g. CT, MRI, fMRI etc.) A key problem in image guided procedures is the need to navigate to specific locations highlighted in the images, such as image-derived functional areas, that have no obvious corresponding anatomical landmarks - we refer to such locations as virtual landmarks. To address these issues, we contribute a novel interactive visualization technique to provide improved feedback to surgeons - Augmented inline visualization. Based on the results of an expert evaluation, we found neurosurgeons to be 30% more accurate when using our augmented inline representation.},
  doi       = {10.1109/ISBI.2011.5872772},
  issn      = {1945-7928},
  keywords  = {augmented reality;biomedical MRI;brain;computerised tomography;medical expert systems;medical image processing;neurophysiology;surgery;CT;anatomical landmarks;augmented inline representation;augmented inline-based navigation;computer-assisted surgery system;expert evaluation;fMRI;image guided procedures;image-derived functional areas;interactive visualization technique;stereotactic image guided neurosurgery;Accuracy;Data visualization;Instruments;Navigation;Neurosurgery;Three dimensional displays;Image Guided Navigation;Image-guided surgery;Neurosurgery;Visualization},
}

@InProceedings{5692305,
  author    = {E. D. Lorias and M. A. Minor and S. J. L. Ortiz and P. V. H. Olivares and J. A. Gnecchi},
  title     = {Computer System for the Evaluation of Laparoscopic Skills},
  booktitle = {2010 IEEE Electronics, Robotics and Automotive Mechanics Conference},
  year      = {2010},
  pages     = {19-22},
  month     = {Sept},
  abstract  = {Minimally invasive surgery demands the development and maintenance of the specialized surgeon's psychomotor skills. These skills can be measured objectively through hybrid and virtual training systems. Of which, the most advanced, integrates force feedback systems and augmented reality graphics. However, these systems are inaccessible because of their high cost, limiting the expected academic impact. In this article, we propose a computerized system for an objective evaluation, which can be integrated into any physical trainer at a low cost.},
  doi       = {10.1109/CERMA.2010.11},
  keywords  = {augmented reality;computer graphics;force feedback;medical computing;augmented reality graphics;computerized system;force feedback systems;hybrid training systems;laparoscopic skills evaluation;minimally invasive surgery;specialized surgeon psychomotor skills;virtual training systems;Computers;Laparoscopes;Solid modeling;Surgery;Training;Virtual reality;bioinstrumentation;image processing;surgical evaluation;surgical training computer-assisted},
}

@Article{5454277,
  author   = {A. Widmer and Y. Hu},
  title    = {Effects of the Alignment Between a Haptic Device and Visual Display on the Perception of Object Softness},
  journal  = {IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
  year     = {2010},
  volume   = {40},
  number   = {6},
  pages    = {1146-1155},
  month    = {Nov},
  issn     = {1083-4427},
  abstract = {Virtual reality (VR) has been gaining popularity in surgical planning and simulation. Most VR surgical simulation systems provide haptic (pertinent to the sense of touch) and visual information simultaneously using certain alignments between a haptic device and visual display. A critical aspect of such VR surgical systems is to represent both haptic and visual information accurately to avoid perceptual illusions (e.g., to distinguish the softness of organs/tissues). This study compared three different alignments (same-location alignment, vertical alignment, and horizontal alignment) between a haptic device and visual display that are widely used in VR systems. We conducted three experiments to study the influence of each alignment on the perception of object softness. In each experiment, we tested 15 different human subjects with varying availability of haptic and visual information. During each trial, the task of the subject was to discriminate object softness between two deformable balls in different viewing angles. We analyzed the following dependent measurements: subject perception of object softness and objective measurements of maximum force and maximum pressing depth. The analysis results reveal that all three alignments (independent variables) have similar effect on subjective perception of object softness within the interval of viewing angles from -7.5° to +7.5°. The viewing angle does not affect objective measurements. The same-location alignment requires less physical effort compared with the other two alignments. These observations have implications in creating accurate simulation and interaction for VR surgical systems.},
  doi      = {10.1109/TSMCA.2010.2045370},
  keywords = {computer displays;haptic interfaces;medical computing;surgery;virtual reality;alignment effect;haptic device;maximum force measurement;maximum pressing depth;object softness perception;surgical planning;virtual reality;visual display;Computational modeling;Computer displays;Force measurement;Haptic interfaces;Humans;Medical simulation;Pressing;Surgery;Testing;Virtual reality;Artificial;augmented;human information processing;interaction styles;virtual realities (VRs)},
}

@InProceedings{5542836,
  author    = {N. Fukuda and Y. W. Chen and M. Nakamoto and T. Okada and Y. Sato},
  title     = {A scope cylinder rotation tracking method for oblique-viewing endoscopes without attached sensing device},
  booktitle = {The 2nd International Conference on Software Engineering and Data Mining},
  year      = {2010},
  pages     = {684-687},
  month     = {June},
  abstract  = {Since the view direction of an oblique-viewing endoscope can be changed by rotating the scope cylinder to obtain a larger field of view, the rotation angle of the scope cylinder is one of the parameters in its camera model. In order to perform the augmented reality visualization in the surgery using the oblique-viewing endoscope, tracking of the rotation angle is required to determine the view direction. The optical tracking markers or the rotary-encoder have been used for the rotation tacking in the previous studies. However, the additional components attached to the endoscope can be obstructive during the surgery. We propose the rotation tracking method without any additional components based on detection of the wedge mark in the endoscopie image, which indicates the cylinder angle. In the experiment, the difference of the proposed method from the rotary encoder used as the gold standard was less than 1.5 degrees.},
  keywords  = {augmented reality;endoscopes;medical image processing;surgery;augmented reality visualization;cylinder angle;oblique-viewing endoscope;optical tracking marker;rotary-encoder;rotation angle;scope cylinder rotation tracking method;surgery;Augmented reality;Biomedical optical imaging;Cameras;Educational institutions;Endoscopes;Engine cylinders;Information science;Minimally invasive surgery;Optical sensors;Visualization;arthroscope;augmented reality system;computer assisted surgery;surgical navigation},
}

@Article{4967956,
  author   = {N. Navab and S. M. Heining and J. Traub},
  title    = {Camera Augmented Mobile C-Arm (CAMC): Calibration, Accuracy Study, and Clinical Applications},
  journal  = {IEEE Transactions on Medical Imaging},
  year     = {2010},
  volume   = {29},
  number   = {7},
  pages    = {1412-1423},
  month    = {July},
  issn     = {0278-0062},
  abstract = {Mobile C-arm is an essential tool in everyday trauma and orthopedics surgery. Minimally invasive solutions, based on X-ray imaging and coregistered external navigation created a lot of interest within the surgical community and started to replace the traditional open surgery for many procedures. These solutions usually increase the accuracy and reduce the trauma. In general, they introduce new hardware into the OR and add the line of sight constraints imposed by optical tracking systems. They thus impose radical changes to the surgical setup and overall procedure. We augment a commonly used mobile C-arm with a standard video camera and a double mirror system allowing real-time fusion of optical and X-ray images. The video camera is mounted such that its optical center virtually coincides with the C-arm's X-ray source. After a one-time calibration routine, the acquired X-ray and optical images are coregistered. This paper describes the design of such a system, quantifies its technical accuracy, and provides a qualitative proof of its efficiency through cadaver studies conducted by trauma surgeons. In particular, it studies the relevance of this system for surgical navigation within pedicle screw placement, vertebroplasty, and intramedullary nail locking procedures. The image overlay provides an intuitive interface for surgical guidance with an accuracy of <;1 mm, ideally with the use of only one single X-ray image. The new system is smoothly integrated into the clinical application with no additional hardware especially for down-the-beam instrument guidance based on the anteroposterior oblique view, where the instrument axis is aligned with the X-ray source. Throughout all experiments, the camera augmented mobile C-arm system proved to be an intuitive and robust guidance solution for selected clinical routines.},
  doi      = {10.1109/TMI.2009.2021947},
  keywords = {augmented reality;biomedical optical imaging;diagnostic radiography;image fusion;image registration;medical image processing;orthopaedics;surgery;CAMC;Camera Augmented Mobile C-Arm;biomedical X-ray imaging;biomedical optical imaging;calibration;coregistered external navigation;double mirror system;intramedullary nail locking;minimally invasive solutions;optical tracking system;orthopedics surgery;pedicle screw placement;real-time image fusion;surgical navigation;vertebroplasty;video camera;Biomedical optical imaging;Calibration;Cameras;Hardware;Instruments;Minimally invasive surgery;Mirrors;Navigation;Orthopedic surgery;X-ray imaging;Augmented reality visualization;C-arm navigation;image-guided surgery;Calibration;Equipment Design;Equipment Failure Analysis;Photography;Radiographic Image Enhancement;Radiographic Image Interpretation, Computer-Assisted;Reproducibility of Results;Sensitivity and Specificity;Tomography, X-Ray Computed},
}

@InProceedings{5361782,
  author    = {S. R. Ellis},
  title     = {Latency and User Performance in Virtual Environments and Augmented Reality},
  booktitle = {2009 13th IEEE/ACM International Symposium on Distributed Simulation and Real Time Applications},
  year      = {2009},
  pages     = {69-69},
  month     = {Oct},
  abstract  = {System rendering latency has been recognized by senior researchers, such as Professor Fredrick Brooks of UNC (Turing Award 1999), as a major factor limiting the realism and utility of head-referenced display systems. Latency has been shown to reduce the user's sense of immersion within a virtual environment, to disturb user interaction with virtual objects, and to contribute to motion sickness during some simulation tasks. Latency, however, is not just an issue for external display systems since finite nerve conduction rates and variation in transduction times in the human body's sensors also pose problems for latency management within the nervous system. Some of the phenomena arising from the brain's handling of sensory asynchrony due to latency will be discussed as a prelude to consideration of the effects of latency in interactive displays. The causes and consequences of the erroneous movement that appears in displays due to latency will be illustrated with examples of the user performance impact provided by several experiments. These experiments will review the generality of user sensitivity to latency when users judge either object or environment stability. Hardware and signal processing countermeasures will also be discussed. In particular the tuning of a simple extrapolative predictive filter not using a dynamic movement model will be presented. Results show that it is possible to adjust this filter so that the appearance of some latencies may be hidden without the introduction of perceptual artifacts such as overshoot. Several examples of the effects of user performance will be illustrated by three-dimensional tracking and tracing tasks executed in virtual environments. These experiments demonstrate classic phenomena known from work on manual control and show the need for very responsive systems if they are intended to support precise manipulation. The practical benefits of removing interfering latencies from interactive systems will be emphasized with so- me classic final examples from surgical telerobotics and human-computer interaction.},
  doi       = {10.1109/DS-RT.2009.44},
  issn      = {1550-6525},
  keywords  = {augmented reality;interactive systems;augmented reality;extrapolative predictive filter;interactive systems;motion sickness;nervous system;sensory asynchrony;system rendering latency;user performance;virtual environments;Augmented reality;Brain modeling;Delay;Displays;Filters;Humans;Limiting;Sensor phenomena and characterization;Sensor systems;Virtual environment},
}

@InProceedings{5336474,
  author    = {C. Bichlmeier and S. Holdstock and S. M. Heining and S. Weidert and E. Euler and O. Kutter and N. Navab},
  title     = {Contextual in-situ visualization for port placement in keyhole surgery: Evaluation of three target applications by two surgeons and eighteen medical trainees},
  booktitle = {2009 8th IEEE International Symposium on Mixed and Augmented Reality},
  year      = {2009},
  pages     = {173-174},
  month     = {Oct},
  abstract  = {Port position in minimally invasive surgeries is chosen to minimize the lesion of tissue and maximize the movability for endoscopic instruments. In this study, we present an evaluation of the potential of a 3D contextual in-situ visualization of the anatomic target region to help surgeons for three different surgical procedures decide where best to create ports and incisions to enable the insertion of a specific set of instruments.},
  doi       = {10.1109/ISMAR.2009.5336474},
  keywords  = {augmented reality;data visualisation;medical computing;3D contextual in-situ visualization;contextual in-situ visualization;keyhole surgery;medical trainees;minimally invasive surgeries;port placement;Augmented reality;Cameras;Computer vision;Imaging phantoms;Lesions;Minimally invasive surgery;Surgical instruments;Target tracking;Testing;Visualization;H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities;K.3.1 [Computer Uses in Education]},
}

@InProceedings{5209632,
  author    = {O. Celik and D. Powell and M. K. O'Malley},
  title     = {Impact of visual error augmentation methods on task performance and motor adaptation},
  booktitle = {2009 IEEE International Conference on Rehabilitation Robotics},
  year      = {2009},
  pages     = {793-798},
  month     = {June},
  abstract  = {We hypothesized that augmenting the visual error feedback provided to subjects training in a point-to-point reaching task under visual distortion would improve the amount and speed of adaptation. Previous studies showing that human learning is error-driven and that visual error augmentation can improve the rate at which subjects decrease their trajectory error in such a task provided the motivation for our study. In a controlled experiment, subjects were required to perform point-to-point reaching movements in the presence of a rotational visual distortion. The amount and speed of their adaptation to this distortion were calculated based on two performance measures: trajectory error and hit time. We tested how three methods of error augmentation (error amplification, traditional error offsetting, and progressive error offsetting) affected the amount and speed of adaptation, and additionally propose definitions for ldquoamountrdquo and ldquospeedrdquo of adaptation in an absolute sense that are more practical than definitions used in previous studies. It is concluded that traditional error offsetting promotes the fastest learning, while error amplification promotes the most complete learning. Progressive error offsetting, a novel method, resulted in slower training than the control group, but we hypothesize that it could be improved with further tuning and indicate a need for further study of this method. These results have implications for improvement in motor skill learning across many fields, including rehabilitation after stroke, surgical training, and teleoperation.},
  doi       = {10.1109/ICORR.2009.5209632},
  issn      = {1945-7898},
  keywords  = {augmented reality;medical robotics;patient rehabilitation;robot vision;motor adaptation;motor skill learning;point-to-point reaching task;progressive error offsetting;rehabilitation after stroke;rotational visual distortion;surgical training;task performance;teleoperation;visual distortion;visual error augmentation methods;visual error feedback;Computer errors;Distortion measurement;Error correction;Feedback;Humans;Protocols;Rehabilitation robotics;Testing;Time measurement;Velocity measurement;Error augmentation;motor adaptation;robotic rehabilitation;visual distortion},
}

@InProceedings{4913256,
  author    = {P. U-Thainual and G. S. Fischer and I. Iordachita and S. Vikal and G. Fichtinger},
  title     = {The Perk Station: Systems design for percutaneous intervention training suite},
  booktitle = {2008 IEEE International Conference on Robotics and Biomimetics},
  year      = {2009},
  pages     = {1693-1697},
  month     = {Feb},
  abstract  = {Image-guided percutaneous needle-based surgery has become part of routine clinical practice in performing procedures such as biopsies, injections and therapeutic implants. A novice physician typically performs needle interventions under the supervision of a senior physician; a slow and inherently subjective training process that lacks objective, quantitative assessment of the surgical skill and performance. Current evaluations of needle-based surgery are also rather simplistic: usually only needle tip accuracy and procedure time are recorded, the latter being used as an indicator of economical feasibility. Shortening the learning curve and increasing procedural consistency are critically important factors in assuring high-quality medical care for all segments of society. This paper describes the design and development of a laboratory validation system for measuring operator performance under different assistance techniques for needle-based surgical guidance systems - The perk station. The initial focus of the perk station is to assess and compare three different techniques: the image overlay, bi-plane laser guide, and conventional freehand. The integrated system comprises of a flat display with semi-transparent mirror (image overlay), bi-plane laser guide, a magnetic tracking system, a tracked needle, a phantom, and a stand-alone laptop computer running the planning and guidance software. The prototype Perk Station has been successfully developed, the associated needle insertion phantoms have been built, and the graphic surgical interface has been implemented.},
  doi       = {10.1109/ROBIO.2009.4913256},
  keywords  = {augmented reality;computer vision;medical computing;surgery;biplane laser guide;high-quality medical care;image-guided percutaneous needle-based surgery;laboratory validation system;magnetic tracking system;percutaneous intervention training suite;perk station;quantitative assessment;stand-alone laptop computer;subjective training process;surgical skill;tomntom;tracked needle;Biomedical imaging;Biopsy;Computer displays;Economic indicators;Image segmentation;Imaging phantoms;Implants;Laboratories;Laser surgery;Needles;Augmented Reality;Percutaneous Interventions;Surgical guidance;Training suite},
}

@Article{4579344,
  author   = {J. Ren and R. V. Patel and K. A. McIsaac and G. Guiraudon and T. M. Peters},
  title    = {Dynamic 3-D Virtual Fixtures for Minimally Invasive Beating Heart Procedures},
  journal  = {IEEE Transactions on Medical Imaging},
  year     = {2008},
  volume   = {27},
  number   = {8},
  pages    = {1061-1070},
  month    = {Aug},
  issn     = {0278-0062},
  abstract = {Two-dimensional or 3-D visual guidance is often used for minimally invasive cardiac surgery and diagnosis. This visual guidance suffers from several drawbacks such as limited field of view, loss of signal from time to time, and in some cases, difficulty of interpretation. These limitations become more evident in beating-heart procedures when the surgeon has to perform a surgical procedure in the presence of heart motion. In this paper, we propose dynamic 3-D virtual fixtures (DVFs) to augment the visual guidance system with haptic feedback, to provide the surgeon with more helpful guidance by constraining the surgeon's hand motions thereby protecting sensitive structures. DVFs can be generated from preoperative dynamic magnetic resonance (MR) or computed tomograph (CT) images and then mapped to the patient during surgery. We have validated the feasibility of the proposed method on several simulated surgical tasks using a volunteer's cardiac image dataset. Validation results show that the integration of visual and haptic guidance can permit a user to perform surgical tasks more easily and with reduced error rate. We believe this is the first work presented in the field of virtual fixtures that explicitly considers heart motion.},
  doi      = {10.1109/TMI.2008.917246},
  keywords = {cardiology;medical robotics;patient diagnosis;surgery;virtual reality;beating-heart procedures;cardiac image dataset;computed tomograph images;dynamic virtual fixtures;haptic feedback;heart motion;minimally invasive cardiac diagnosis;minimally invasive cardiac surgery;preoperative dynamic magnetic resonance images;robot-assisted surgery;simulated surgical tasks;three-dimensional virtual fixtures;visual guidance;Computational modeling;Computed tomography;Error analysis;Feedback;Fixtures;Haptic interfaces;Heart;Magnetic resonance;Minimally invasive surgery;Surge protection;Beating heart surgery;dynamic virtual fixtures;haptic feedback;minimally invasive robot-assisted surgery;Cardiovascular Surgical Procedures;Coronary Artery Bypass, Off-Pump;Heart;Humans;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Surgery, Computer-Assisted;Surgical Procedures, Minimally Invasive},
}

@InProceedings{4530106,
  author    = {M. Castro and C. Goksu and P. Haigron and A. Lucas and L. Urbano},
  title     = {Estimation of 2D/3D rigid transformation for Computer-assisted Endovascular Navigation},
  booktitle = {2008 3rd International Conference on Information and Communication Technologies: From Theory to Applications},
  year      = {2008},
  pages     = {1-6},
  month     = {April},
  abstract  = {Minimally invasive vascular interventions require the exact location and orientation of the endovascular tools (i.e., balloon, stent graft) with respect to the patient in the surgical environment. While a 3D image is often available preoperatively (typically an angio-CT scan) the practitioner mainly relies on the 2D images provided by the mobile fluoroscopic device (C-arm) to obtain a real-time feedback of the intravascular tool location as well as of the therapeutic actions. The main objective of this work is the study of an endovascular navigation system that enables the surgeon to improve the accuracy of the procedure along with a minimum use of the intra-operative 2D acquisitions, leading to less radiation exposure and less contrast agent injection. With the aim of exploiting the whole available information at the different observation stages through a cooperative approach, this paper focuses on the definition of the different rigid geometrical transformations implied in the 3D preoperative / 2D intraoperative images matching, as well as on the estimation of the transformation parameters based on the use and combination of different types of data and methods (calibration, 3D tracking, image registration).},
  doi       = {10.1109/ICTTA.2008.4530106},
  keywords  = {image matching;image registration;medical image processing;surgery;2D/3D rigid transformation;computer-assisted endovascular navigation;image matching;minimally invasive vascular interventions;mobile fluoroscopic device;Arteries;Calibration;Catheters;Feedback;Image matching;Image registration;Lead;Minimally invasive surgery;Navigation;Surges;3D localization;Computer-aided surgery;augmented reality;calibration;endovascular navigation;image registration},
}

@InProceedings{4492401,
  author    = {C. Feng and J. W. Rozenblit and A. Hamilton},
  title     = {Fuzzy Logic-Based Performance Assessment in the Virtual, Assistive Surgical Trainer (VAST)},
  booktitle = {15th Annual IEEE International Conference and Workshop on the Engineering of Computer Based Systems (ecbs 2008)},
  year      = {2008},
  pages     = {203-209},
  month     = {March},
  abstract  = {The Virtual Assistive Surgical Trainer (VAST) is an approach developed to train surgeons in minimally invasive procedures. It uses surgical instruments augmented with micro-sensors, and knowledge-based inference techniques to provide objective, data-driven feedback and performance assessment for complex exercises. The assessment is typically based on the expertise of senior surgeons and, thus, a single objective standard is difficult to define. To formulate such a standard, and to provide an accurate scoring method, a fuzzy logic method is proposed in this paper. This makes it easier to mimic tasks that are already successfully performed by human experts. A multi-level fuzzy inference engine and new performance metrics are implemented. Experimental results demonstrate the feasibility of this method and the efficacy of the new performance metrics.},
  doi       = {10.1109/ECBS.2008.51},
  keywords  = {biomedical education;computer based training;fuzzy logic;fuzzy reasoning;knowledge based systems;medical computing;microsensors;surgery;virtual reality;VAST;data-driven feedback;fuzzy logic-based performance assessment;knowledge-based inference technique;micro sensors;minimally invasive procedure;multi level fuzzy inference engine;surgeon training;surgical instruments;virtual assistive surgical trainer;Animals;Conferences;Engines;Fuzzy logic;Fuzzy systems;Instruments;Measurement;Medical diagnosis;Medical simulation;Minimally invasive surgery;Fuzzy Logic;Inference;MIS;Surgical Training},
}

@InProceedings{4479994,
  author    = {M. Mahvash and J. Gwilliam and R. Agarwal and B. Vagvolgyi and L. M. Su and D. D. Yuh and A. M. Okamura},
  title     = {Force-Feedback Surgical Teleoperator: Controller Design and Palpation Experiments},
  booktitle = {2008 Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems},
  year      = {2008},
  pages     = {465-471},
  month     = {March},
  abstract  = {In this paper, we develop and test a 6-degree-of-freedom surgical teleoperator that has four possible modes of operation: (1) direct force feedback, (2) graphical force feedback, (3) direct and graphical force feedback together, and (4) no force feedback. In all cases, visual feedback of the: environment is provided via a head-mounted display. A position-position controller with local dynamic compensators provides the direct force feedback. The graphical force feedback is overlaid on the environment image, and displays a bar whose height and color is related to the environment force estimated using the current applied to the actuators of the patient-side arm. We evaluate the performance of the teleoperator modes in assisting a user to find the location of stiff objects hidden inside a soft material, similar to a calcified artery hidden in heart tissue and a tumor in the prostate. Seven people used the teleoperator t:o perform palpation in these materials. Results showed that direct force feedback mode minimizes palpation task error for the heart model.},
  doi       = {10.1109/HAPTICS.2008.4479994},
  issn      = {2324-7347},
  keywords  = {biological tissues;cardiology;compensation;force feedback;helmet mounted displays;medical robotics;position control;surgery;telerobotics;direct force feedback;environment color image;force-feedback 6-DOF surgical teleoperator;graphical force feedback;head-mounted display;heart tissue;local dynamic compensator;palpation experiment;patient-side arm;position-position controller;Actuators;Arteries;Biological materials;Displays;Force control;Force feedback;Heart;Surgery;Teleoperators;Testing;augmented reality;force feedback;friction compensation;haptics;observer;stability;transparency},
}

@Article{4359067,
  author   = {M. Feuerstein and T. Mussack and S. M. Heining and N. Navab},
  title    = {Intraoperative Laparoscope Augmentation for Port Placement and Resection Planning in Minimally Invasive Liver Resection},
  journal  = {IEEE Transactions on Medical Imaging},
  year     = {2008},
  volume   = {27},
  number   = {3},
  pages    = {355-369},
  month    = {March},
  issn     = {0278-0062},
  abstract = {In recent years, an increasing number of liver tumor indications were treated by minimally invasive laparoscopic resection. Besides the restricted view, two major intraoperative issues in laparoscopic liver resection are the optimal planning of ports as well as the enhanced visualization of (hidden) vessels, which supply the tumorous liver segment and thus need to be divided (e.g., clipped) prior to the resection. We propose an intuitive and precise method to plan the placement of ports. Pre operatively, self-adhesive fiducials are affixed to the patient's skin and a computed tomography (CT) data set is acquired while contrasting the liver vessels. Immediately prior to the intervention, the laparoscope is moved around these fiducials, which are automatically reconstructed to register the patient to its preoperative imaging data set. This enables the simulation of a camera flight through the patient's interior along the laparoscope's or instruments' axes to easily validate potential ports. Intraoperatively, surgeons need to update their surgical planning based on actual patient data after organ deformations mainly caused by application of carbon dioxide pneumoperitoneum. Therefore, preoperative imaging data can hardly be used. Instead, we propose to use an optically tracked mobile C-arm providing cone-beam CT imaging capability intraoperatively. After patient positioning, port placement, and carbon dioxide insufflation, the liver vessels are contrasted and a 3-D volume is reconstructed during patient exhalation. Without any further need for patient registration, the reconstructed volume can be directly augmented on the live laparoscope video, since prior calibration enables both the volume and the laparoscope to be positioned and oriented in the tracking coordinate frame. The augmentation provides the surgeon with advanced visual aid for the localization of veins, arteries, and bile ducts to be divided or sealed.},
  doi      = {10.1109/TMI.2007.907327},
  keywords = {computerised tomography;liver;surgery;tumours;3D volume reconstruction;arteries;bile ducts;carbon dioxide pneumoperitoneum;computed tomography;intraoperative laparoscope augmentation;liver tumor;minimally invasive liver resection;port placement;resection planning;surgical planning;veins;Augmented Reality Visualization;Augmented reality visualization;Image-Guided Surgery;Laparoscopic Surgery;Port Placement;image-guided surgery;laparoscopic surgery;port placement;Animals;Equipment Design;Equipment Failure Analysis;Hepatectomy;Humans;Laparoscopes;Preoperative Care;Surgery, Computer-Assisted;Surgical Procedures, Minimally Invasive;Swine;Tomography, X-Ray Computed;User-Computer Interface},
}

@InProceedings{4352420,
  author    = {J. D. Lee and T. Y. Lan and C. H. Huang and C. T. Wu and S. T. Lee},
  title     = {A Coarse-to-Fine Surface Registration Algorithm for Frameless Brain Surgery},
  booktitle = {2007 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society},
  year      = {2007},
  pages     = {836-839},
  month     = {Aug},
  abstract  = {In this paper, we present a coarse-to-fine approach for surface registration used in frameless brain surgery. The reference point data are extracted from patient's pre-stored CT images; the floating point data are captured from the patient's head by using an electromagnetic tracker. The registration algorithm takes the information of global coarse markers (or feature points) and local fine point cloud into account to avoid the local optimum problem of the iterative closest point (ICP) algorithm. Experimental results show that the registration accuracy of using the proposed registration algorithm is improved than of using other ICP-related algorithms. More specifically, it shows that the target registration error (TRE) is decreased by 50% by using the proposed method.},
  doi       = {10.1109/IEMBS.2007.4352420},
  issn      = {1094-687X},
  keywords  = {brain;computerised tomography;image registration;iterative methods;medical image processing;surgery;CT images;coarse-to-fine surface registration algorithm;electromagnetic tracker;frameless brain surgery;global coarse markers;iterative closest point algorithm;target registration error;Biomedical imaging;Clouds;Computed tomography;Data mining;Head;Iterative algorithms;Iterative closest point algorithm;Navigation;Surgery;Surgical instruments;Algorithms;Brain;Electromagnetic Phenomena;Humans;Stereotaxic Techniques;Tomography, X-Ray Computed},
}

@Article{4354160,
  author   = {G. Lacey and D. Ryan and D. Cassidy and D. Young},
  title    = {Mixed-Reality Simulation of Minimally Invasive Surgeries},
  journal  = {IEEE MultiMedia},
  year     = {2007},
  volume   = {14},
  number   = {4},
  pages    = {76-87},
  month    = {Oct},
  issn     = {1070-986X},
  abstract = {Our mixed-reality platform helps train surgeons in minimally invasive surgery and objectively assesses their performance. The platform uses multicamera stereo inside a patient manikin to measure the 3D positions of unmodified surgical instruments. It uses this information to drive a mixed-reality, computer-mediated learning system and provide objective measures of surgical skill.},
  doi      = {10.1109/MMUL.2007.79},
  keywords = {biomedical education;computer based training;computer vision;medical computing;stereo image processing;surgery;virtual reality;3D position measurement;computer-mediated learning system;minimally invasive surgery;mixed-reality simulation;multicamera stereo;objective performance assessment;patient manikin;surgeon training;surgical instruments;surgical skill measure;Abdomen;Cameras;Discrete event simulation;Feeds;Laparoscopes;Layout;Minimally invasive surgery;Surges;Surgical instruments;Virtual reality;Augmented reality;and psychomotor skills assessment.;surgical training;virtual reality},
}

@Article{4302577,
  author   = {N. Navab and J. Traub and T. Sielhorst and M. Feuerstein and C. Bichlmeier},
  title    = {Action- and Workflow-Driven Augmented Reality for Computer-Aided Medical Procedures},
  journal  = {IEEE Computer Graphics and Applications},
  year     = {2007},
  volume   = {27},
  number   = {5},
  pages    = {10-14},
  month    = {Sept},
  issn     = {0272-1716},
  abstract = {During the past three years, we've tried to develop integrated AR solutions in the context of minimally invasive surgery. We have therefore focused on four main issues: recovery and monitoring of surgical workflow, integrating preoperative and intraoperative anatomic and functional data, improving visual perception in a mixed environment, and developing new user interaction paradigms for taking full advantage of the virtual data, while overlaid onto the real scene. Each of these issues is the subject of many existing and future publications. Here, we provide a brief overview of our activities and current results in regard to each of these issues.},
  doi      = {10.1109/MCG.2007.117},
  keywords = {augmented reality;medical computing;surgery;action-driven augmented reality;computer-aided medical procedures;intraoperative anatomic data;intraoperative functional data;minimally invasive surgery;mixed environment;preoperative anatomic data;preoperative functional data;surgical workflow monitoring;surgical workflow recovery;user interaction paradigm;visual perception;workflow-driven augmented reality;Anatomy;Augmented reality;Biomedical imaging;Biomedical optical imaging;Computerized monitoring;Liver;Optical imaging;Orthopedic surgery;User interfaces;Virtual reality;augmented reality;medical procedures;surgery;virtual reality;1},
}

@Article{1510543,
  author   = {D. Maupu and M. H. Van Horn and S. Weeks and E. Bullitt},
  title    = {3D stereo interactive medical visualization},
  journal  = {IEEE Computer Graphics and Applications},
  year     = {2005},
  volume   = {25},
  number   = {5},
  pages    = {67-71},
  month    = {Sept},
  issn     = {0272-1716},
  abstract = {Our interactive, 3D stereo display helps guide clinicians during endovascular procedures, such as intraoperative needle insertion and stent placement relative to the target organs. We describe a new method of guiding endovascular procedures using interactive 3D stereo visualizations. We use as an example the transjugular intrahepatic portosystemic shunt (TIPS) procedure. Our goal is to increase the speed and safety of endovascular procedures by providing the interventionalist with 3D information as the operation proceeds. Our goal is to provide 3D image guidance of the TIPS procedure so that the interventionalist can readily adjust the needle position and trajectory to reach the target on the first pass. We propose a 3D stereo display of the interventionalist's needle and target vessels. We also add interactivity via head tracking so that the interventionalist gains a better 3D sense of the relationship between the target vessels and the needle during needle advancement.},
  doi      = {10.1109/MCG.2005.94},
  keywords = {blood vessels;computerised tomography;data visualisation;image registration;image segmentation;interactive systems;medical image processing;solid modelling;stereo image processing;three-dimensional displays;3D image guidance;3D information;3D stereo display;endovascular procedures;interactive medical visualization;intraoperative needle insertion;stent placement;transjugular intrahepatic portosystemic shunt procedure;Biomedical imaging;Catheters;Image segmentation;Liver;Medical treatment;Minimally invasive surgery;Needles;Portals;Veins;Visualization;3D and 2D registration;augmented reality;image-guided surgery;medical imaging;minimally invasive surgery;quaternion;Imaging, Three-Dimensional;Photogrammetry;Portacaval Shunt, Surgical;Radiographic Image Interpretation, Computer-Assisted;Surgery, Computer-Assisted;User-Computer Interface;Vascular Surgical Procedures},
}

@InProceedings{1492784,
  author    = {K. Matkovic and T. Psik and I. Wagner and D. Gracanin},
  title     = {Dynamic Texturing of Real Objects in an Augmented Reality System},
  booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
  year      = {2005},
  pages     = {257-260},
  month     = {March},
  abstract  = {The ability to physically change properties of real objects used in augmented reality (AR) applications is limited. Geometrical properties (shape, size) and appearance (color, texture) of a real object remain unchanged during a single application run. However, an AR system can be used to provide a virtual texture for the real object. The texture can be changed dynamically based on user interactions. The developed AR system includes two components, the "3D Table" and the "Texture Painter." The 3D Table is a table where real objects are placed. The tabletop is used as a projection surface, making it possible to add a context to the real object. The Texture Painter makes it possible to paint on the real object, using a real brush and virtual ink (texture). ARToolkit markers are placed on the 3D Table tabletop to augment the environment with the virtual objects. Markers are either physical (printouts on the tabletop) or virtual (projections). The scene is recorded with a camera and the composed video is projected in real time. The projection shows a virtual environment, real objects painted with virtual ink, and virtual objects positioned where real or virtual ARToolkit markers are placed. The developed system is used in architectural design applications where, due to the different qualities of real architectural models and rendered architectural models, real models are still used. The system was tested at the Academy of Fine Arts in Vienna where it is used as a support tool for architecture students.},
  doi       = {10.1109/VR.2005.1492784},
  issn      = {1087-8270},
  keywords  = {augmented reality;force feedback;haptic interfaces;laser applications in medicine;medical computing;medical robotics;surgery;augmented reality;conceptual surgical laser system;haptic feedback;haptic interface;haptic laser system;low-power laser;noncutting laser;surgical robot;visible trajectory;visual feedback;Art;Augmented reality;Brushes;Cameras;Ink;Layout;Paints;Shape;System testing;Virtual environment},
}

@InProceedings{1398527,
  author    = {A. D. Shelton and B. R. Klatzky and C. G. Stetten},
  title     = {Method for assessing augmented reality needle guidance using a virtual biopsy task},
  booktitle = {2004 2nd IEEE International Symposium on Biomedical Imaging: Nano to Macro (IEEE Cat No. 04EX821)},
  year      = {2004},
  pages     = {273-276 Vol. 1},
  month     = {April},
  abstract  = {The Sonic Flashlight is a device that permits real-time in situ visualization of ultrasound images by reflecting calibrated images displayed on a flat-panel monitor from a partially transparent half silvered mirror. This system presents the illusion that the ultrasound slice is "floating" within the patient's body, and we believe it will be a useful visualization technique during ultrasound guided interventional procedures. While our preliminary research indicates that the Sonic Flashlight is practical in a clinical setting, we lack empirical data that demonstrate our hypothesized improvement in needle placement accuracy. To this end, we have designed a system that presents "virtual phantoms" to the operator by tracking a non-scanning Sonic Flashlight and a mock needle with a miniBird™(Ascension Technology) magnetic tracking device. This system allows us to present the correct slice through a stored image volume and compute the error in position between the needle tip and the center of the virtual target. Preliminary data suggest that our device is capable of acquiring data that is consistent with the physical movement observed during the test and is qualitatively similar to data acquired in an earlier analysis of surgical tool movement. While it is not yet possible to draw definitive conclusions about the impact of the Sonic Flashlight on needle guidance, these data suggest that such a study will be possible using the virtual phantom system that we have developed.},
  doi       = {10.1109/ISBI.2004.1398527},
  keywords  = {augmented reality;biomedical ultrasonics;phantoms;surgery;Sonic Flashlight;augmented reality needle guidance;miniBird magnetic tracking device;real-time in situ visualization;surgical tool movement;ultrasound guided interventional procedures;virtual biopsy task;virtual phantoms;Augmented reality;Biopsy;Data visualization;Imaging phantoms;Magnetic devices;Mirrors;Needles;Patient monitoring;Target tracking;Ultrasonic imaging},
}

@InProceedings{1279452,
  author    = {N. L. Lewis and J. Meunier},
  title     = {Intelligent pointer in computer assisted surgery-design and feasibility},
  booktitle = {Proceedings of the 25th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (IEEE Cat. No.03CH37439)},
  year      = {2003},
  volume    = {2},
  pages     = {1148-1151 Vol.2},
  month     = {Sept},
  abstract  = {We present a solution combining the newest technologies in computer vision and preoperativeiy planned surgical intervention to enhanced the efficiency of complex surgery. There have been many solutions proposed to use computer vision in the operating room, but they often depend on annoying and expensive components such as head mounted display (HMD). These systems have proven to be of weak precision and, due to their weight and size, they tend to be really disturbing if worn during long period of time. The goal of this work is to demonstrate the possibility of a system projecting directly on the patient, in real time, during the surgery. The main advantage is to keep the attention of the surgeon focused directly on his patient at all times. The information that can be added to the scene, due to the absence of HMD is evidently restricted to 2D since only one image is projected on the patient (skin, bone, surgery linen etc.) instead of two images for the right and left eyes with HMD, but by using an intelligent pointer to highlight important zones in the line of sight of the surgeon, 3D information can be inferred. This system actually transforms the patient body itself into a visual data source for the surgeon.},
  doi       = {10.1109/IEMBS.2003.1279452},
  issn      = {1094-687X},
  keywords  = {biomedical imaging;data visualisation;helmet mounted displays;medical computing;surgery;3D information;augmented reality;computer assisted surgery;computer vision;head mounted display;intelligent pointer;nonplaner projection;patient body;planned surgical intervention;visual data source;Bones;Computer displays;Computer vision;Eyes;Head;Layout;Real time systems;Skin;Surgery;Surges},
}

@InProceedings{1106959,
  author    = {J. de Siebenthal and F. Langlotz},
  title     = {Use of a new tracking system based on ArToolkit for a surgical simulator: accuracy test and overall evaluation},
  booktitle = {The First IEEE International Workshop Agumented Reality Toolkit,},
  year      = {2002},
  pages     = {6 pp.-},
  abstract  = {Computer assisted surgery (CAS) uses expensive tracking systems, such as Optotrak (Northern Digital, Canada). These cameras use infra-red (IR) light detection and give sub-millimeter accuracy for the tracking of surgical tools in a real surgical context. In simulation, such accuracy is not mandatory. To replace the standard tracking systems used in CAS simulation, this paper promotes the use of video tracking systems that are easy to set up and less expensive. This work was motivated by the 5th European Framework project VOEU that is aiming to produce new training tools for orthopedic surgery. One problems to solve is to provide an autonomous system for supporting surgeons in learning CAS procedures, since new training components are frequently requested. Such training technologies can be used during surgical lessons given to medical students, or are delivered to surgeons for preparing a real CAS procedure. Due to new computer technologies based on PCs, surgical simulators can be built at low cost featuring video tracking. Tracking is a key part of each CAS simulator, since surgical tools are used and need to be located in space. Several test series were carried out according to confidence values given by the ArToolkit library to evaluate its accuracy regarding various different parameters (size of markers, video cameras, volume of interest). The simulator implementation proposes a new interface for ArToolkit displaying a 3D scene without showing the image sequence captured by the video camera. The implemented 3D module is entirely based on an OpenInventor (SGI, USA) engine and can easily be included as a subcomponent of any complex user interface. One to several tools can be displayed in real time allowing the completion of each step of a common CAS procedure.},
  doi       = {10.1109/ART.2002.1106959},
  keywords  = {augmented reality;biomedical education;digital simulation;image sequences;medical image processing;microcomputer applications;optical tracking;orthopaedics;surgery;training;video signal processing;3D scene;ArToolkit;OpenInventor engine;PC;autonomous system;computer assisted surgery;confidence values;image sequence;medical students;orthopedic surgery;surgical simulator;training tools;user interface;video camera;video tracking systems;Biomedical imaging;Cameras;Computational modeling;Content addressable storage;Context-aware services;Infrared detectors;Medical simulation;Orthopedic surgery;Space technology;System testing},
}

@InProceedings{1017288,
  author    = {A. Ozkurt and K. Ozmehmet},
  title     = {Interactive medical volume visualization for surgical operations},
  booktitle = {2001 Conference Proceedings of the 23rd Annual International Conference of the IEEE Engineering in Medicine and Biology Society},
  year      = {2001},
  volume    = {3},
  pages     = {2505-2508 vol.3},
  abstract  = {In this paper, a "Surgeon Assistant System" using Enhanced Reality (ER) approach is presented. The main idea is to use Virtual Reality (VR) device(s), and assist the surgeon with one and two-dimensional information of human body and three-dimensional volumetric surface models, of anatomic tissues, especially in brain operations. This opportunity may increase the concentration of the surgeon with the combinational use of real and computer generated information combination. Therefore, increased concentration and well-organized information improve the success rate of the surgery. This study covers hardware and software implementation of a prototype system recommending requirements of such tools.},
  doi       = {10.1109/IEMBS.2001.1017288},
  issn      = {1094-687X},
  keywords  = {augmented reality;brain;medical image processing;surgery;anatomic tissues;brain operations;computer generated information;interactive medical volume visualization;prototype system;surgeon assistant system;surgeon concentration;surgical operations;virtual reality devices;well-organized information;Biological system modeling;Brain modeling;Erbium;Hardware;Humans;Software tools;Surgery;Surges;Virtual reality;Visualization},
}

@InProceedings{930299,
  author    = {Yong Chong Loh and Ming Yeong Teo and Wan Sing Ng and C. Sim and Qing Song Zou and Tseng Tsai Yeo and Yih Yian Sitoh},
  title     = {Surgical planning system with real-time volume rendering},
  booktitle = {Proceedings International Workshop on Medical Imaging and Augmented Reality},
  year      = {2001},
  pages     = {259-261},
  abstract  = {The system allows surgeons to plan a surgical approach on a set of 2D image slices and visualise them in 3D rapidly. We use neurosurgery as an example. The surgeon could visualize objects of interest like a tumor and surgical path, and verify that the surgical plan avoids the critical features and is optimal},
  doi       = {10.1109/MIAR.2001.930299},
  keywords  = {data visualisation;medical image processing;planning;real-time systems;rendering (computer graphics);surgery;2D image slices;3D visualisation;neurosurgery;real-time volume rendering;surgeons;surgical planning system;three dimensional visualisation;tumor;Biomedical imaging;Diseases;Neoplasms;Production engineering;Production planning;Real time systems;Shape;Surgery;Technology planning;Visualization},
}

@InProceedings{930276,
  author    = {E. Gladilin and S. Zachow and P. Deuflhard and H. C. Hege},
  title     = {A biomechanical model for soft tissue simulation in craniofacial surgery},
  booktitle = {Proceedings International Workshop on Medical Imaging and Augmented Reality},
  year      = {2001},
  pages     = {137-141},
  abstract  = {Soft tissue prediction is an important additional criterion in surgical planning. Especially in craniofacial surgery the realistic prediction of a patient's postoperative appearance is of utmost importance. A physically based modeling approach seems to be most suitable for the achievement of realistic results. However the properties of biological structures are highly complex and scarcely investigated, thus simplified models are often applied. In this paper we describe our FE based modeling approach for static soft tissue prediction and muscle muscle simulation},
  doi       = {10.1109/MIAR.2001.930276},
  keywords  = {finite element analysis;medical image processing;surgery;FE based modeling approach;biomechanical model;craniofacial surgery;muscle muscle simulation;physically based modeling approach;postoperative appearance;soft tissue prediction;soft tissue simulation;Biological system modeling;Biological tissues;Capacitive sensors;Deformable models;Iron;Muscles;Predictive models;Solid modeling;Surgery;Tensile stress},
}

@InProceedings{930256,
  author    = {J. Shah and A. Darzi},
  title     = {Simulation and skills assessment},
  booktitle = {Proceedings International Workshop on Medical Imaging and Augmented Reality},
  year      = {2001},
  pages     = {5-9},
  abstract  = {Being a competent surgeon involves many factors such as knowledge of anatomy, physiology, pathology operative theory decision-making and communicative skills to name but a few. Manual dexterity and the actual skill required to operate has recently come under increasing scrutiny, and there is growing pressure for surgeons to demonstrate that they can operate well, and maintain this high level of performance throughout their careers. In the changing climate of surgical training, we discuss the use of various techniques of surgical simulation for teaching technical skills outside the operating theatre, and consider the advantages and disadvantages of each. Current methods of assessment that are in place both in the research setting and in practice are also presented},
  doi       = {10.1109/MIAR.2001.930256},
  keywords  = {biomedical education;computer based training;digital simulation;medical computing;surgery;virtual reality;career;manual dexterity;medical skills assessment;surgeon skills;surgical simulation;surgical training;teaching;technical skills;virtual reality;Anatomy;Animal structures;Biological system modeling;Diseases;Education;Medical simulation;Minimally invasive surgery;Skin;Surges;Virtual reality},
}

@InProceedings{930271,
  author    = {Tianmiao Wang and Da Liu and Lei Hu and Hong Lv and Zigang Wang and Zesheng Tang},
  title     = {A simulation and training system of robot assisted surgery based on virtual reality},
  booktitle = {Proceedings International Workshop on Medical Imaging and Augmented Reality},
  year      = {2001},
  pages     = {103-107},
  abstract  = {In this paper, a simulation and training system of robot assisted surgery based on virtual reality is proposed setting up from a concrete practical background of frameless stereotactic neurosurgery and a connecting home-made surgical robot. This system combines with 3D display devices and home-made man-machine conversation devices. This system can also improve the speed of displaying interaction of the 3D operation model and establish a robot assisted surgical simulation and training demo system oriented clinical applications},
  doi       = {10.1109/MIAR.2001.930271},
  keywords  = {computer based training;digital simulation;medical computing;medical robotics;surgery;virtual reality;3D display devices;3D operation model;clinical applications;frameless stereotactic neurosurgery;man-machine conversation devices;robot assisted surgery;surgical simulation;training system;virtual reality;Application software;Computational modeling;Computer simulation;Hardware;Instruments;Man machine systems;Minimally invasive surgery;Neurosurgery;Robots;Virtual reality},
}

@InProceedings{930260,
  author    = {C. Sim and Wan Sing Ng and Ming Yeong Teo and Yong-Chong Loh and Tseng Tsai Yeo},
  title     = {Image-guided manipulator compliant surgical planning methodology for robotic skull-base surgery},
  booktitle = {Proceedings International Workshop on Medical Imaging and Augmented Reality},
  year      = {2001},
  pages     = {26-29},
  abstract  = {In robotic skull-base surgery, a surgical navigation and planning station is used to plan the path to be taken by the bone removal tool which is carried by a robotic manipulator. In planning for such a path, considerations have to be taken in terms of the manipulator characteristics and the geometric properties of the bone removal tool used. Presented in this paper is a methodology which takes into account the required efficacy of surgical planning. Differing from other surgical planning methodologies, the final output of the planned surgery can be directly executed by a surgical robot (NeuRobot). In doing so, the surgeon has only to specify the region or features within the skull-base which are to be avoided and the general direction of action which specifies the eventual path that the surgery is to be carried out. A novel approach is taken by the use of Voronoi maps to identify “safe” cavities on each image slice},
  doi       = {10.1109/MIAR.2001.930260},
  keywords  = {computational geometry;manipulators;medical image processing;medical robotics;path planning;robot vision;surgery;NeuRobot;Voronoi maps;bone removal tool;geometric properties;image slice;image-guided manipulator;medical image processing;path planning;robotic skull-base surgery;surgical navigation system;surgical planning;surgical robot;Biomedical imaging;Bones;Manipulators;Medical robotics;Path planning;Production planning;Robots;Surgery;Surges;Technology planning},
}

@InProceedings{930268,
  author    = {X. Pennec and N. Ayache and A. Roche and P. Cachier},
  title     = {Non-rigid MR/US registration for tracking brain deformations},
  booktitle = {Proceedings International Workshop on Medical Imaging and Augmented Reality},
  year      = {2001},
  pages     = {79-86},
  abstract  = {During a neuro-surgical intervention, the brain tissues shift and warp. In order to keep an accurate positioning of the surgical instruments, one has to estimate this deformation from intra-operative images. 3D ultrasound (US) imaging is an innovative and low-cost modality which appears to be suited for such computer-assisted surgery tools. We present a new image-based technique to register intra-operative 3D US with pre-operative magnetic resonance (MR) data. A first automatic rigid registration is achieved by the maximisation of a similarity measure that generalises the correlation ratio. Then, brain deformations are tracked in the 3D US time-sequence using a “demon's” like algorithm. Experiments show that a registration accuracy of the MR voxel size is achieved for the rigid part, and a qualitative accuracy of a few millimetres could be obtained for the complete tracking system},
  doi       = {10.1109/MIAR.2001.930268},
  keywords  = {biomedical MRI;biomedical ultrasonics;brain;image registration;medical image processing;surgery;3D ultrasound imaging;automatic rigid registration;brain deformation tracking;computer-assisted surgery;experiments;image registration;magnetic resonance imaging;medical image processing;neuro-surgical intervention;surgical instrument positioning;Computed tomography;Magnetic resonance;Magnetic resonance imaging;Registers;Skull;Spline;Surface fitting;Surgery;Surgical instruments;Ultrasonic imaging},
}

@Article{826855,
  author   = {P. Dario and M. C. Carrozza and M. Marcacci and S. D'Attanasio and B. Magnami and O. Tonet and G. Megali},
  title    = {A novel mechatronic tool for computer-assisted arthroscopy},
  journal  = {IEEE Transactions on Information Technology in Biomedicine},
  year     = {2000},
  volume   = {4},
  number   = {1},
  pages    = {15-29},
  month    = {March},
  issn     = {1089-7771},
  abstract = {Describes a mechatronic tool for arthroscopy, which is both a smart tool for traditional arthroscopy and the main component of a system for computer-assisted arthroscopy. The mechatronic arthroscope has a cable-actuated servomotor-driven multi-joint mechanical structure, is equipped with a position sensor for measuring the orientation of the tip and a force sensor for detecting possible contact with delicate tissues in the knee, and incorporates an embedded microcontroller for sensor signal processing, motor driving and interfacing with the surgeon and/or the system control unit. The computer-assisted arthroscopy system comprises an image processing module for the segmentation and 3D reconstruction of pre-operative CT or MR images, a registration module for measuring the position of the knee joint, tracking the trajectory of the operating tools and matching pre-operative and intra-operative images, and a human-machine interface that displays the enhanced reality scenario and data from the mechatronic arthroscope in a friendly and intuitive manner. By integrating pre-operative and intra-operative images and information provided by the mechatronic arthroscope, the system allows virtual navigation in the knee joint during the planning phase and computer guidance by augmented reality during the intervention. This paper describes in detail the characteristics of the mechatronic arthroscope and of the system for computer-assisted arthroscopy and discusses experimental results obtained with a preliminary version of the tool and of the system.},
  doi      = {10.1109/4233.826855},
  keywords = {augmented reality;force sensors;image matching;image reconstruction;image registration;image segmentation;mechatronics;medical image processing;microcontrollers;optical tracking;surgery;3D image reconstruction;augmented reality;cable-actuated servomotor-driven multi-joint mechanical structure;computer guidance;computer-assisted arthroscopy;delicate tissue contact;embedded microcontroller;enhanced reality scenario;force sensor;human-machine interface;image matching;image processing module;image registration module;image segmentation;intra-operative images;knee joint position measurement;mechatronic arthroscope;motor driving;planning phase;position sensor;pre-operative images;sensor signal processing;surgeon;surgical intervention;system control unit;tip orientation measurement;trajectory tracking;virtual navigation;Computer displays;Force measurement;Intelligent sensors;Knee;Mechanical cables;Mechanical sensors;Mechanical variables measurement;Mechatronics;Position measurement;Sensor systems;Arthroscopes;Arthroscopy;Biomedical Engineering;Data Display;Equipment Design;Humans;Image Processing, Computer-Assisted;Infrared Rays;Knee Joint;Magnetic Resonance Imaging;Man-Machine Systems;Optics;Patient Care Planning;Robotics;Signal Processing, Computer-Assisted;Surface Properties;Therapy, Computer-Assisted;Tomography, X-Ray Computed;User-Computer Interface},
}

@InProceedings{803994,
  author    = {P. S. Jensen and P. K. Gupta and E. de Juan},
  title     = {Quantification of microsurgical tactile perception},
  booktitle = {Proceedings of the First Joint BMES/EMBS Conference. 1999 IEEE Engineering in Medicine and Biology 21st Annual Conference and the 1999 Annual Fall Meeting of the Biomedical Engineering Society (Cat. N},
  year      = {1999},
  volume    = {2},
  pages     = {839 vol.2-},
  month     = {Oct},
  abstract  = {Microsurgery involves the manipulation of delicate membranes and vessels with a required accuracy often on the order of tens of microns, a scale at or near the limit of human positional ability. In addition, forces imposed by the tissue on the surgical tool during these manipulations are exceedingly small. Manual tasks performed using visual feedback alone (i.e., without tactile sensation) have been shown to take longer and be less accurate than tasks performed utilizing both visual and tactile information. Here we validate the hypothesis that retinal surgery is primarily vision based by measuring the magnitude of forces generated during simulated retinal surgery in cadaveric porcine eyes and comparing the results with the magnitude of forces discernable by retinal surgeons. Our goal is to improve microsurgical performance by developing robotic augmentation devices capable of enhancing the surgeon's ability to both position surgical tools and sense the surgical environment. The results of this study are an important guideline for such systems currently under development},
  doi       = {10.1109/IEMBS.1999.803994},
  issn      = {1094-687X},
  keywords  = {augmented reality;force feedback;medical robotics;surgery;touch (physiological);cadaveric porcine eyes;generated forces;microsurgical performance;microsurgical tactile perception;retinal surgery;robotic augmentation devices;surgical environment sensing;surgical tool positioning;visual feedback;Biomembranes;Eyes;Feedback;Force measurement;Humans;Microsurgery;Retina;Robot sensing systems;Surgery;Surges},
}

@InProceedings{7405915,
  author    = {S. Lu and X. Lin and X. Han},
  title     = {Virtual-Real Registration of Augmented Reality Technology Used in the Cerebral Surgery Lesion Localization},
  booktitle = {2015 Fifth International Conference on Instrumentation and Measurement, Computer, Communication and Control (IMCCC)},
  year      = {2015},
  pages     = {620-625},
  month     = {Sept},
  abstract  = {In the traditional cerebral surgery operation, the location of the lesion is difficult to calculate accurately. This paper designed a medical augmented reality system to locate the lesion via virtual-real registration technology. Firstly, a four-ball bracket was designed and fixed on the real skull. The virtual skull model with the virtual four-ball bracket was got by CT data reconstruct. The virtual lesion was added in the virtual skull model and the four-ball bracket was used for spatial position calibration of virtual model and real skull in the process of virtual-real registration. The whole virtual-real registration process was displayed on the computer in real-time. Finally, the application of virtual-real registration technology in the lesion localization of cerebral surgery was realized.},
  doi       = {10.1109/IMCCC.2015.136},
  keywords  = {augmented reality;computerised tomography;image reconstruction;image registration;medical image processing;surgery;CT data reconstruction;augmented reality technology;cerebral surgery lesion localization;medical augmented reality system;spatial position calibration;virtual four-ball bracket;virtual lesion;virtual skull model;virtual-real registration technology;Cameras;Computed tomography;Image reconstruction;Isosurfaces;Lesions;Surgery;Three-dimensional displays;augmented reality;lesions location;virtual-reality registration},
}

@InProceedings{7348441,
  author    = {M. Heydarzadeh and M. Nourani and J. Park},
  title     = {An augmented reality platform for CABG surgery},
  booktitle = {2015 IEEE Biomedical Circuits and Systems Conference (BioCAS)},
  year      = {2015},
  pages     = {1-4},
  month     = {Oct},
  abstract  = {In this paper, we introduce a new platform for in CABG surgery based on marker-less augmented reality technology. Our real-time system captures the surgery scene by a camera, extracts SURF features and matches them in video sequence to track the patient's heart. It estimates the camera's pose robustly using RANSAC and highlights the clogged vessel to guide the surgeon during the surgery.},
  doi       = {10.1109/BioCAS.2015.7348441},
  keywords  = {augmented reality;biomedical optical imaging;blood vessels;cameras;feature extraction;medical image processing;surgery;CABG surgery;RANSAC;SURF feature extraction;camera;clogged vessel;coronary artery bypass graft surgery;feature matching;heart;marker-less augmented reality technology;real-time system;video sequence;Cameras;Feature extraction;Heart;Solid modeling;Surgery;Three-dimensional displays},
}

@InProceedings{7319352,
  author    = {E. P. Ong and J. A. Lee and J. Cheng and B. H. Lee and G. Xu and A. Laude and S. Teoh and T. H. Lim and D. W. K. Wong and J. Liu},
  title     = {An augmented reality assistance platform for eye laser surgery},
  booktitle = {2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
  year      = {2015},
  pages     = {4326-4329},
  month     = {Aug},
  abstract  = {This paper presents a novel augmented reality assistance platform for eye laser surgery. The aims of the proposed system are for the application of assisting eye doctors in pre-planning as well as providing guidance and protection during laser surgery. We developed algorithms to automatically register multi-modal images, detect macula and optic disc regions, and demarcate these as protected areas from laser surgery. The doctor will then be able to plan the laser treatment pre-surgery using the registered images and segmented regions. Thereafter, during live surgery, the system will automatically register and track the slit lamp video frames on the registered retina images, send appropriate warning when the laser is near protected areas, and disable the laser function when it points into the protected areas. The proposed system prototype can help doctors to speed up laser surgery with confidence without fearing that they may unintentionally fire laser in the protected areas.},
  doi       = {10.1109/EMBC.2015.7319352},
  issn      = {1094-687X},
  keywords  = {augmented reality;eye;image registration;image segmentation;laser applications in medicine;medical image processing;surgery;video signal processing;augmented reality assistance platform;automatic multimodal image registration;eye laser surgery;laser function;laser treatment presurgery;macula region detection;optic disc region detection;protected areas;registered retina images;segmented regions;slit lamp video frame registration;slit lamp video frame tracking;Biomedical optical imaging;Diabetes;Image registration;Optical imaging;Retina;Surgery},
}

@Article{6987340,
  author   = {N. Haouchine and S. Cotin and I. Peterlik and J. Dequidt and M. S. Lopez and E. Kerrien and M. O. Berger},
  title    = {Impact of Soft Tissue Heterogeneity on Augmented Reality for Liver Surgery},
  journal  = {IEEE Transactions on Visualization and Computer Graphics},
  year     = {2015},
  volume   = {21},
  number   = {5},
  pages    = {584-597},
  month    = {May},
  issn     = {1077-2626},
  abstract = {This paper presents a method for real-time augmented reality of internal liver structures during minimally invasive hepatic surgery. Vessels and tumors computed from pre-operative CT scans can be overlaid onto the laparoscopic view for surgery guidance. Compared to current methods, our method is able to locate the in-depth positions of the tumors based on partial three-dimensional liver tissue motion using a real-time biomechanical model. This model permits to properly handle the motion of internal structures even in the case of anisotropic or heterogeneous tissues, as it is the case for the liver and many anatomical structures. Experimentations conducted on phantom liver permits to measure the accuracy of the augmentation while real-time augmentation on in vivo human liver during real surgery shows the benefits of such an approach for minimally invasive surgery.},
  doi      = {10.1109/TVCG.2014.2377772},
  keywords = {augmented reality;biological tissues;computerised tomography;liver;medical image processing;surgery;internal liver structures;liver surgery;minimally invasive hepatic surgery;partial three-dimensional liver tissue motion;pre-operative CT scans;real-time augmentation;real-time augmented reality;real-time biomechanical model;soft tissue heterogeneity;Biological system modeling;Biomechanics;Computational modeling;Deformable models;Liver;Surgery;Three-dimensional displays;Biomechanical Modeling;Computer Assisted Surgery;Image-guided Simulation;Image-guided simulation;Real-Time Augmented Reality;biomechanical modeling;computer assisted surgery;real-time augmented reality;0},
}

@InProceedings{6907458,
  author    = {N. Haouchine and J. Dequidt and I. Peterlik and E. Kerrien and M. O. Berger and S. Cotin},
  title     = {Towards an accurate tracking of liver tumors for augmented reality in robotic assisted surgery},
  booktitle = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
  year      = {2014},
  pages     = {4121-4126},
  month     = {May},
  abstract  = {This article introduces a method for tracking the internal structures of the liver during robot-assisted procedures. Vascular network, tumors and cut planes, computed from pre-operative data, can be overlaid onto the laparoscopic view for image-guidance, even in the case of large motion or deformation of the organ. Compared to current methods, our method is able to precisely propagate surface motion to the internal structures. This is made possible by relying on a fast yet accurate biomechanical model of the liver combined with a robust visual tracking approach designed to properly constrain the model. Augmentation results are demonstrated on in-vivo sequences of a human liver during robotic surgery, while quantitative validation is performed on an ex-vivo porcine liver experimentation. Validation results show that our approach gives an accurate surface registration with an error of less than 6mm on the position of the tumor.},
  doi       = {10.1109/ICRA.2014.6907458},
  issn      = {1050-4729},
  keywords  = {augmented reality;control engineering computing;image motion analysis;image registration;liver;medical image processing;medical robotics;object tracking;surgery;tumours;augmented reality;ex-vivo porcine liver experimentation;image-guidance;laparoscopic view;liver biomechanical model;liver tumor tracking;robotic assisted surgery;surface motion propagation;surface registration;vascular network;visual tracking approach;Biological system modeling;Biomechanics;Liver;Surface reconstruction;Surgery;Three-dimensional displays;Tumors},
}

@InProceedings{6671780,
  author    = {N. Haouchine and J. Dequidt and I. Peterlik and E. Kerrien and M. O. Berger and S. Cotin},
  title     = {Image-guided simulation of heterogeneous tissue deformation for augmented reality during hepatic surgery},
  booktitle = {2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
  year      = {2013},
  pages     = {199-208},
  month     = {Oct},
  abstract  = {This paper presents a method for real-time augmentation of vascular network and tumors during minimally invasive liver surgery. Internal structures computed from pre-operative CT scans can be overlaid onto the laparoscopic view for surgery guidance. Compared to state-of-the-art methods, our method uses a real-time biomechanical model to compute a volumetric displacement field from partial three-dimensional liver surface motion. This permits to properly handle the motion of internal structures even in the case of anisotropic or heterogeneous tissues, as it is the case for the liver and many anatomical structures. Real-time augmentation results are presented on in vivo and phantom data and illustrate the benefits of such an approach for minimally invasive surgery.},
  doi       = {10.1109/ISMAR.2013.6671780},
  keywords  = {augmented reality;computerised tomography;liver;medical image processing;phantoms;real-time systems;solid modelling;surgery;tumours;anatomical structures;anisotropic tissue;augmented reality;hepatic surgery;heterogeneous tissue deformation;image-guided simulation;internal structures;laparoscopic view;minimally invasive liver surgery;minimally invasive surgery;partial three-dimensional liver surface motion;phantom data;preoperative CT scans;real-time augmentation;real-time biomechanical model;state-of-the-art method;surgery guidance;tumors;vascular network;volumetric displacement field;Biological system modeling;Biomechanics;Computational modeling;Deformable models;Liver;Surgery;Three-dimensional displays},
}

@InProceedings{6680063,
  author    = {L. Hu and M. Wang and Z. Song},
  title     = {A Convenient Method of Video See-Through Augmented Reality Based on Image-Guided Surgery System},
  booktitle = {2013 Seventh International Conference on Internet Computing for Engineering and Science},
  year      = {2013},
  pages     = {100-103},
  month     = {Sept},
  abstract  = {Augmented Reality (AR) is a technology which is widely used in medical area, especially in surgery room. In many cases, cumbersome equipments or complicated algorithm are introduced to establish the whole system, which would make it inefficient and unsuitable for surgery environment. This paper presents a convenient method for video see-through augmented reality on the basis of existing image-guided surgery system developed by our research center. We describe the prototype of the system, the registration of virtual objects in a marker coordinate system, and the motion tracking of the Head Mounted Display (HMD) using A Toolkit. Finally, we present the experiment results which show that the proposed AR technology can provide augmented information for the surgeons and work well with existing image-guided surgery system.},
  doi       = {10.1109/ICICSE.2013.27},
  issn      = {2330-9857},
  keywords  = {augmented reality;helmet mounted displays;image motion analysis;image registration;medical image processing;object tracking;surgery;video signal processing;AR technology;HMD;head mounted display;image-guided surgery system;marker coordinate system;medical area;motion tracking;surgery room;video see-through augmented reality;virtual objects registration;Augmented reality;Biomedical optical imaging;Calibration;Cameras;Optical imaging;Surgery;Tracking;Augmented reality;Image-guided surgery;Tracking system},
}

@InProceedings{6116211,
  author    = {F. López-Mir and V. Naranjo and J. Angulo and E. Villanueva and M. Alcañiz and S. López-Celada},
  title     = {Aorta segmentation using the watershed algorithm for an augmented reality system in laparoscopic surgery},
  booktitle = {2011 18th IEEE International Conference on Image Processing},
  year      = {2011},
  pages     = {2649-2652},
  month     = {Sept},
  abstract  = {This paper presents an algorithm for a 3D segmentation of the aorta artery in magnetic resonance images (MRI). The purpose is to project the 3D segmented aorta in the patient's abdomen with an augmented reality (AR) system to help the surgeon in laparoscopic interventions. In order to obtain accurate results in the segmentation process a marker-controlled watershed algorithm is used. Since this method requires a robust gradient image and two marker sets, a preprocessing step is carried out in each image. The algorithm is automatic and the results are promising with a Jaccard coefficient (JC) of 0.8107 ± 0.0228.},
  doi       = {10.1109/ICIP.2011.6116211},
  issn      = {1522-4880},
  keywords  = {augmented reality;biomedical MRI;blood vessels;image segmentation;medical image processing;surgery;3D segmentation;Jaccard coefficient;MRI;abdomen;aorta artery;aorta segmentation;augmented reality;laparoscopic surgery;magnetic resonance image;marker-controlled watershed algorithm;Biomedical imaging;Image segmentation;Magnetic resonance imaging;Morphology;Noise;Surgery;Three dimensional displays;aorta segmentation;augmented reality;mathematical morphology;medical image processing;watershed algorithm},
}

@InProceedings{5524421,
  author    = {R. Wen and L. Yang and C. K. Chui and K. B. Lim and S. Chang},
  title     = {Intraoperative visual guidance and control interface for augmented reality robotic surgery},
  booktitle = {IEEE ICCA 2010},
  year      = {2010},
  pages     = {947-952},
  month     = {June},
  abstract  = {We proposed an augmented reality (AR) robotic system equipped with intraoperative visual guidance and gesture based control interface. The proposed feature is an enhancement of our AR robotic system. AR robotic system was introduced to the field of interventional medicine to assist surgeons in implementing medical operations under the augmented reality environment. The technique combining dextrous robot and AR guidance provides a new operational mode for surgeons. The introduction of robotic modules is to compliment surgeon's dexterity and to perform specific tasks defined during the surgery. Augmented reality provides additional visualization and interaction absence in a typical clinical environment. The interfacing system includes intraoperative tracking of surgical tools and virtual reconstruction of the visually occluded tool segment, and an intuitive gesture-based human-computer interaction centered on the projected organ. Preliminary experiments show that this novel human-machine interface is effective for surgical intervention.},
  doi       = {10.1109/ICCA.2010.5524421},
  issn      = {1948-3449},
  keywords  = {augmented reality;dexterous manipulators;human computer interaction;medical robotics;surgery;user interfaces;visual servoing;AR robotic system;P;augmented reality;dextrous robot;gesture based control interface;human-computer interaction;human-machine interface;interfacing system;intraoperative visual guidance;robotic surgery;surgical tools;Augmented reality;Automatic control;Control systems;Medical diagnostic imaging;Medical robotics;Minimally invasive surgery;Needles;Neoplasms;Robotics and automation;Robots},
}

@Article{5415615,
  author   = {H. Liao and T. Inomata and I. Sakuma and T. Dohi},
  title    = {3-D Augmented Reality for MRI-Guided Surgery Using Integral Videography Autostereoscopic Image Overlay},
  journal  = {IEEE Transactions on Biomedical Engineering},
  year     = {2010},
  volume   = {57},
  number   = {6},
  pages    = {1476-1486},
  month    = {June},
  issn     = {0018-9294},
  abstract = {A 3-D augmented reality navigation system using autostereoscopic images was developed for MRI-guided surgery. The 3-D images are created by employing an animated autostereoscopic image, integral videography (IV), which provides geometrically accurate 3-D spatial images and reproduces motion parallax without using any supplementary eyeglasses or tracking devices. The spatially projected 3-D images are superimposed onto the surgical area and viewed via a half-slivered mirror. A fast and accurate spatial image registration method was developed for intraoperative IV image-guided therapy. Preliminary experiments showed that the total system error in patient-to-image registration was 0.90 ± 0.21 mm, and the procedure time for guiding a needle toward a target was shortened by 75%. An animal experiment was also conducted to evaluate the performance of the system. The feasibility studies showed that augmented reality of the image overlay system could increase the surgical instrument placement accuracy and reduce the procedure time as a result of intuitive 3-D viewing.},
  doi      = {10.1109/TBME.2010.2040278},
  keywords = {biomedical MRI;image registration;medical image processing;surgery;video recording;3D augmented reality;3D spatial images;MRI-guided surgery;animated autostereoscopic image;half-slivered mirror;integral videography;integral videography autostereoscopic image overlay;intraoperative IV image-guided therapy;motion parallax;patient-to-image registration;spatial image registration method;supplementary eyeglasses;surgical area;surgical instrument placement accuracy;tracking devices;Image overlay;integral photography (IP);integral videography (IV);registration;surgical navigation;three-dimensional image;Equipment Design;Equipment Failure Analysis;Imaging, Three-Dimensional;Magnetic Resonance Imaging;Photogrammetry;Sensitivity and Specificity;Subtraction Technique;Surgery, Computer-Assisted;User-Computer Interface;Video Recording},
}

@InProceedings{4649094,
  author    = {R. U. Thoranaghatte and J. G. Giraldez and G. Zheng},
  title     = {Landmark based augmented reality endoscope system for sinus and skull-base surgeries},
  booktitle = {2008 30th Annual International Conference of the IEEE Engineering in Medicine and Biology Society},
  year      = {2008},
  pages     = {74-77},
  month     = {Aug},
  abstract  = {Endoscopic video stream during sinus and skull base surgeries can be augmented with the preoperatively chosen landmark to provide effective navigation for the operating surgeon. Currently available systems try to augment with CT or MR image slices. This will be of not much help since there is too much information overlaid. We have developed a simplified landmark based Augmented Reality (AR) system for endoscopic sinus/skull-base surgeries. Results are presented from the experiments with plastic skull and cadaver specimen. Subjective evaluation from a experienced surgeon confirms the effectiveness of the system.},
  doi       = {10.1109/IEMBS.2008.4649094},
  issn      = {1094-687X},
  keywords  = {Augmented reality;Cadaver;Computed tomography;Endoscopes;Plastics;Skull;Streaming media;Surgery;Surges;Videoconference;Computer Simulation;Endoscopy;Humans;Models, Anatomic;Models, Biological;Paranasal Sinuses;Skull Base;Surgery, Computer-Assisted;User-Computer Interface},
}

@InProceedings{4381718,
  author    = {C. Jing and W. Yongtian and L. Yue and W. Dongdong},
  title     = {Navigating System for Endoscopic Sinus Surgery Based on Augmented Reality},
  booktitle = {2007 IEEE/ICME International Conference on Complex Medical Engineering},
  year      = {2007},
  pages     = {185-188},
  month     = {May},
  abstract  = {Medical image processing has led to a major improvement of patient care. The 3D modeling of patients from their CT scan or MRI provides an improved surgical planning and simulation allows training of the surgical gesture before carrying it out. These two preoperative steps can be used intraoperatively with the development of augmented reality. We present a navigating system for endoscope sinus surgery based on augmented reality, which can superimpose the virtual model onto the surgeon's view of the patient, providing accurate spatial information of the patient's sinus structure. The system is comfortable and easy to use in clinical employment.},
  doi       = {10.1109/ICCME.2007.4381718},
  keywords  = {augmented reality;biomedical MRI;computerised tomography;endoscopes;medical image processing;surgery;3D modeling;CT scan;MRI;augmented reality;endoscopic sinus surgery;medical image processing;navigating system;surgical gesture;Augmented reality;Biomedical image processing;Computed tomography;Employment;Endoscopes;Magnetic resonance imaging;Medical simulation;Navigation;Surgery;Surges},
}

@InProceedings{1383075,
  author    = {L. Soler and S. Nicolau and J. Schmid and C. Koehl and J. Marescaux and X. Pennec and N. Ayache},
  title     = {Virtual reality and augmented reality in digestive surgery},
  booktitle = {Third IEEE and ACM International Symposium on Mixed and Augmented Reality},
  year      = {2004},
  pages     = {278-279},
  month     = {Nov},
  abstract  = {Medical image processing led to a major improvement of patient care: the 3D modeling of patients from their CT-scan or MRI provides an improved surgical planning and simulation allows to train the surgical gesture before carrying it out. These two preoperative steps can be used intra-operatively with the development of augmented reality (AR). In this paper, we present the tools we developed to provide our first prototypal AR guiding system for abdominal surgery.},
  doi       = {10.1109/ISMAR.2004.64},
  keywords  = {augmented reality;medical image processing;patient care;surgery;3D modeling;AR guiding system;augmented reality;digestive surgery;medical image processing;patient care;surgical planning;surgical simulation;virtual reality;Abdomen;Augmented reality;Biomedical imaging;Liver neoplasms;Magnetic resonance imaging;Medical simulation;Pathology;Surgery;Virtual reality;Visualization},
}

@InProceedings{1320143,
  author    = {C. Paloc and E. Carrasco and I. Macia and R. Gomez and I. Barandiaran and J. M. Jimenez and O. Rueda and J. Ortiz de Urbina and A. Valdivieso and G. Sakas},
  title     = {Computer-aided surgery based on auto-stereoscopic augmented reality},
  booktitle = {Proceedings. Eighth International Conference on Information Visualisation, 2004. IV 2004.},
  year      = {2004},
  pages     = {189-193},
  month     = {July},
  abstract  = {Although augmented reality (AR) promises to provide valuable means for computer-aided surgery; the underlying technologies often create a cumbersome work environment that is inadequate for clinical employment. A great deal of research is still needed to develop comfortable and easy-to-use tools providing an augmented view of the patient and its main internal structures. We propose to develop an AR system for enhanced visualization of the liver that involves minimal annoyance for both the surgeon and the patient. The ultimate application of our system is to assist the surgeon in oncological liver surgery.},
  doi       = {10.1109/IV.2004.1320143},
  file      = {:ref_downloads/1320143 - Computer-aided surgery based on auto-stereoscopic augmented reality.pdf:PDF},
  issn      = {1093-9547},
  keywords  = {augmented reality;data visualisation;liver;medical image processing;surgery;auto-stereoscopic augmented reality;clinical employment;computer-aided surgery;medical image segmentation;oncological liver surgery;visualization;Anatomy;Augmented reality;Biomedical imaging;Content addressable storage;Displays;Employment;Image segmentation;Liver;Oncological surgery;Surges},
}

@InProceedings{1279797,
  author    = {J. P. Tardif and S. Roy and J. Meunier},
  title     = {Projector-based augmented reality in surgery without calibration},
  booktitle = {Proceedings of the 25th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (IEEE Cat. No.03CH37439)},
  year      = {2003},
  volume    = {1},
  pages     = {548-551 Vol.1},
  month     = {Sept},
  abstract  = {Augmented reality (AR) is becoming an important tool in surgery to support the surgeon and improve operation quality, safety and duration. However the AR setup with head-mounted display (HMD) and other equipments is often considered cumbersome by surgeons and limits its wide use in the operating room. To reduce this burden, we introduce a new approach to display undistorted image data directly on the patient (skin, bone, surgery linen etc.) without explicit camera and projector calibration. With a single camera used to capture the surgeon's field of view, the calibration is implicitly represented as a mapping establishing the correspondence of each pixel of a camera to a pixel from a projector. After this mapping has been carried out, one can display an image corrected for the surgeon. Results are presented showing the simplicity and potential of the method for an operating room.},
  doi       = {10.1109/IEMBS.2003.1279797},
  issn      = {1094-687X},
  keywords  = {augmented reality;medical image processing;surgery;bone;head-mounted display;image correction;operating room;projector-based augmented reality;skin;surgery linen;undistorted image data;Augmented reality;Bones;Calibration;Cameras;Computer displays;Computer science;Geometry;Skin;Surgery;Surges},
}

@InProceedings{1218010,
  author    = {D. Stoyanov and M. ElHelw and B. P. Lo and A. Chung and F. Bello and Guang-Zhong Yang},
  title     = {Current issues of photorealistic rendering for virtual and augmented reality in minimally invasive surgery},
  booktitle = {Proceedings on Seventh International Conference on Information Visualization, 2003. IV 2003.},
  year      = {2003},
  pages     = {350-358},
  month     = {July},
  abstract  = {In surgery, virtual and augmented reality are increasingly being used as new ways of training, preoperative planning, diagnosis and surgical navigation. Further development of virtual and augmented reality in medicine is moving towards photorealistic rendering and patient specific modeling, permitting high fidelity visual examination and user interaction. This coincides with the current development in computer vision and graphics where image information is used directly to render novel views of a scene. These techniques require extensive use of geometric information about the scene and provide a comprehensive review of the underlying techniques required for building patient specific models with photorealstic rendering. It also highlights some of the opportunities that image based modeling and rendering techniques can offer in the context of minimally invasive surgery.},
  doi       = {10.1109/IV.2003.1218010},
  keywords  = {medical computing;patient diagnosis;realistic images;rendering (computer graphics);surgery;user interfaces;virtual reality;augmented reality;computer graphics;computer vision;geometric information;high fidelity visual examination;image based modeling;image information;minimally invasive surgery;patient specific model;photorealistic rendering;preoperative planning;surgical navigation;user interaction;virtual reality;Augmented reality;Biomedical imaging;Computer graphics;Computer vision;Layout;Medical diagnostic imaging;Minimally invasive surgery;Navigation;Rendering (computer graphics);Solid modeling},
}

@InProceedings{1240716,
  author    = {R. J. Lapeer and A. C. Tan and A. Linney and G. Alusi},
  title     = {Stereo depth assessment experiment for microscope-based surgery},
  booktitle = {The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.},
  year      = {2003},
  pages     = {272-273},
  month     = {Oct},
  abstract  = {We present experimental data on the use of autostereoscopic displays as complementary visualization aids to the surgical stereo microscope for augmented reality surgery. Five experts in the use of the microscope, and five non-experts, performed a depth experiment to assess stereo cues as provided by two autostereoscopic displays (DTI 2015XLS Virtual Window and SHARP micro-optic twin), the surgical microscope and the "naked" eye.},
  doi       = {10.1109/ISMAR.2003.1240716},
  keywords  = {augmented reality;computer displays;medical computing;optical microscopes;stereo image processing;surgery;DTI 2015XLS Virtual Window;SHARP microoptic twin;augmented reality surgery;autostereoscopic displays;experimental data;microscope-based surgery;naked eye;stereo cues;stereo depth assessment;surgical microscope;surgical stereo microscope;visualization aids;Augmented reality;Biomedical optical imaging;Computer displays;Control systems;Diffusion tensor imaging;Liquid crystal displays;Optical crosstalk;Optical microscopy;Surgery;Visualization},
}

@InProceedings{7966908,
  author    = {R. Cziva and D. P. Pezaros},
  title     = {On the Latency Benefits of Edge NFV},
  booktitle = {2017 ACM/IEEE Symposium on Architectures for Networking and Communications Systems (ANCS)},
  year      = {2017},
  pages     = {105-106},
  month     = {May},
  abstract  = {Next-generation networks are expected to support low-latency, context-aware and user-specific services in a highly flexible and efficient manner. Proposed applications include high-definition, low-latency video streaming, remote surgery, as well as applications for tactile Internet, virtual or augmented reality that demand network side data processing (such as image recognition, transformation or head/eye motion aware rendering). One approach to support these use cases is to introduce virtualized network services at the edge of the network, in close proximity of the end users to reduce end-to-end latency, time-to-response and unnecessary utilization of the core network, while providing flexibility for resource allocation. While many research projects including our previous work on Glasgow Network Functions have proposed running virtual network functions (vNF)s at the network edge, a latency-optimal placement allocation has not been presented before for the network edge and therefore the impact on user-to-vNF latency has not been investigated. In this paper, we formulate a simple vNF placement problem that minimizes end-to-end latency from users to their network functions. We have implemented the problem using Integer Linear Programming (ILP) with the Gurobi solver, and evaluated it with a real topology of a network provider. We use our solution to compare two vNF deployment scenarios over an emulation of a national backbone network: a two-tier edge deployment and a cloud-only deployment. We show that, in our example, using edge servers can deliver up to 70% improvement in user-to-vNF latency.},
  doi       = {10.1109/ANCS.2017.23},
  keywords  = {integer programming;linear programming;next generation networks;resource allocation;telecommunication network topology;virtualisation;Edge NFV;Glasgow Network Function;Gurobi solver;ILP;augmented reality;cloud-only deployment;context-aware services;core network utilization;edge servers;end-to-end latency minimization;end-to-end latency reduction;eye motion aware rendering;head motion aware rendering;high-definition low-latency video streaming;image recognition;image transformation;integer linear programming;latency-optimal placement allocation;low-latency services;national backbone network;network side data processing;network topology;next-generation networks;remote surgery;resource allocation;tactile Internet;time-to-response reduction;two-tier edge deployment;user-specific services;vNF placement problem;virtual network function;virtual reality;virtualized network services;Cloud computing;Memory management;Network function virtualization;Resource management;Servers;Topology;Edge Network;Network Function Virtualization},
}

@InProceedings{7759099,
  author    = {N. Haouchine and F. Roy and L. Untereiner and S. Cotin},
  title     = {Using contours as boundary conditions for elastic registration during minimally invasive hepatic surgery},
  booktitle = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year      = {2016},
  pages     = {495-500},
  month     = {Oct},
  abstract  = {We address in this paper the ill-posed problem of initial alignment of pre-operative to intra-operative data for augmented reality during minimally invasive hepatic surgery. This problem consists of finding the rigid transformation that relates the scanning reference and the endoscopic camera pose, and the non-rigid transformation undergone by the liver w.r.t its scanned state. Most of the state-of-the-art methods assume a known initial registration. Here, we propose a method that permits to recover the deformation undergone by the liver while simultaneously finding the rotational and translational parts of the transformation. Our formulation considers the boundaries of the liver with its surrounding tissues as hard constraints directly encoded in an energy minimization process. We performed experiments on real in-vivo data of human hepatic surgery and synthetic data, and compared our method with related works.},
  doi       = {10.1109/IROS.2016.7759099},
  keywords  = {augmented reality;biological tissues;cameras;endoscopes;image registration;liver;medical image processing;minimisation;surgery;augmented reality;boundary conditions;contours;deformation recovery;elastic registration;endoscopic camera pose;energy minimization process;ill-posed problem;intra-operative data alignment;liver boundary;minimally invasive hepatic surgery;nonrigid transformation;pre-operative data alignment;scanning reference;tissues;Biological system modeling;Cameras;Finite element analysis;Liver;Surface reconstruction;Surgery;Three-dimensional displays},
}

@InProceedings{7743367,
  author    = {E. A. Larrarte and A. V. Alban},
  title     = {Virtual markers in virtual laparoscopy surgery},
  booktitle = {2016 XXI Symposium on Signal Processing, Images and Artificial Vision (STSIVA)},
  year      = {2016},
  pages     = {1-6},
  month     = {Aug},
  abstract  = {Augmented Reality or AR is a technology characterized by virtual content adding on the perception of reality using the existing environment and overlaying additional information, this digital information is shown in a device display in real time. The aim of this study is to implement some tests for a mobile augmented reality system (MAR) and test the potential of AR development tools in a mobile application for a surgery training system to assist and train laparoscopic or minimal invasive surgery. These tests are the base for the first application to be developed, it will use 3D visualization, depth extraction from medical images and tracking to display information about the video surgery. Augmented reality apps are written in special 3D programs that allow the developer to tie animation or contextual digital information in a computer program to an augmented reality “marker” in the real world. When a computing device AR app or browser plug-in receives digital information from a known marker, it begins to execute the marker's code and layer the correct image or images.},
  doi       = {10.1109/STSIVA.2016.7743367},
  keywords  = {augmented reality;computer animation;data visualisation;medical image processing;mobile computing;surgery;video signal processing;3D programs;3D visualization;AR development tools;MAR;animation;augmented reality apps;augmented reality marker code;browser plug-in;computer program;contextual digital information;depth extraction;laparoscopic surgery;medical images;minimal invasive surgery;mobile application;mobile augmented reality system;surgery training system;video surgery;virtual content;Augmented reality;Cameras;Engines;Games;Minimally invasive surgery;Three-dimensional displays},
}

@InProceedings{7591700,
  author    = {M. A. Ghaderi and M. Heydarzadeh and M. Nourani and G. Gupta and L. Tamil},
  title     = {Augmented reality for breast tumors visualization},
  booktitle = {2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
  year      = {2016},
  pages     = {4391-4394},
  month     = {Aug},
  abstract  = {3D visualization of breast tumors are shown to be effective by previous studies. In this paper, we introduce a new augmented reality application that can help doctors and surgeons to have a more accurate visualization of breast tumors; this system uses a marker-based image-processing technique to render a 3D model of the tumors on the body. The model can be created using a combination of breast 3D mammography by experts. We have tested the system using an Android smartphone and a head-mounted device. This proof of concept can be useful for oncologists to have a more effective screening, and surgeons to plan the surgery.},
  doi       = {10.1109/EMBC.2016.7591700},
  issn      = {1557-170X},
  keywords  = {augmented reality;mammography;medical image processing;smart phones;tumours;3D model;3D visualization;Android smartphone;augmented reality;breast 3D mammography;breast tumor visualization;head-mounted device;marker-based image-processing technique;oncologists;surgery;Breast cancer;Cameras;Solid modeling;Three-dimensional displays;Visualization;augmented reality;breast cancer;tumor visualization},
}

@InProceedings{7591901,
  author    = {K. He and Y. Mao and J. Ye and Y. An and S. Jiang and C. Chi and J. Tian},
  title     = {A novel wireless wearable fluorescence image-guided surgery system},
  booktitle = {2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
  year      = {2016},
  pages     = {5208-5211},
  month     = {Aug},
  abstract  = {Segmentectomy using indocyanine green (ICG) has become a primary treatment option to achieve a complete resection and preserve lung function in early-stage lung cancer. However, owing to a lack of appropriate intraoperative imaging systems, it is a huge challenge for surgeons to identify the intersegmental plane during the operation, leading to poor prognosis. Thus, we developed a novel wireless wearable fluorescence image-guided surgery system (LIGHTEN) for fast and accurate identification of intersegmental planes in human patients. The system consists of a handle, light source, Google glass and laptop. Application software is written to capture clear real-time images and Google glass is adopted to display with augmented reality. Twelve in vivo studies of pulmonary segmentectomy in swine by intravenous injection of ICG were conducted to test the performance of the system. A distinct black-and-white transition zone image was observed and displayed simultaneously on the Google glass in all swine. The results demonstrated that surgeons using LIGHTEN can effortlessly and quickly discern intersegmental planes during the operation. Our system has enormous potential in helping surgeons to precisely identify intersegmental planes with mobility and high-sensitivity.},
  doi       = {10.1109/EMBC.2016.7591901},
  issn      = {1557-170X},
  keywords  = {augmented reality;biomedical optical imaging;body sensor networks;cancer;fluorescence;lung;medical image processing;surgery;Google glass;ICG;LIGHTEN;augmented reality;black-and-white transition zone image;early-stage lung cancer;human patients;indocyanine green;intersegmental planes;intraoperative imaging systems;intravenous injection;laptop;light source;lung function;primary treatment option;pulmonary segmentectomy;real-time images;software;wireless wearable fluorescence image-guided surgery system;Cancer;Fluorescence;Glass;Google;Imaging;Lungs;Surgery},
}

@InProceedings{7504375,
  author    = {W. Park and H. J. Yang},
  title     = {On Spectral Efficiency of Asynchronous GFDMA and SC-FDMA in Frequency Selective Channels},
  booktitle = {2016 IEEE 83rd Vehicular Technology Conference (VTC Spring)},
  year      = {2016},
  pages     = {1-5},
  month     = {May},
  abstract  = {In the fifth-generation mobile communications, futuristic wireless services such as remote surgery, streaming gaming, tactile internet, and augmented reality are envisioned, which require real- time end-to-end communication with near-zero latency. One of the essential enablers of near-zero latency may be asynchronous multiple-access for uplink, in which each user transmits instantaneously without waiting for the next frame to start. Generalized frequency division multiple-access (GFDMA) has been studied as a promising candidate for asynchronous multiple-access, however, conventional single-carrier FDMA (SC-FDMA) can also be adapted in an asynchronous manner for low latency. In this paper, via extensive numerical simulations, the spectral efficiency of GFDMA and conventional SC-FDMA is investigated in the asynchronous scenario with the presence of inter-user interference due to non-zero out-of-band emission of the spectrum. Simulation results show that the superiority between the two techniques is determined depending on the system parameters such as user density, number of guard band subcarriers, signal-to-noise ratio, and etc.},
  doi       = {10.1109/VTCSpring.2016.7504375},
  keywords  = {5G mobile communication;frequency division multiple access;SC-FDMA;asynchronous GFDMA;asynchronous SC-FDMA;asynchronous multiple-access;augmented reality;fifth-generation mobile communications;frequency selective channels;generalized frequency division multiple-access;guard band subcarriers;inter-user interference;near-zero latency;nonzero out-of-band emission;real-time end-to-end communications;remote surgery;signal-to-noise ratio;single-carrier FDMA;streaming gaming;tactile internet;uplink;user density;wireless services;Frequency division multiaccess;Interference;OFDM;Quadrature amplitude modulation;Signal to noise ratio;Uplink},
}

@InProceedings{7375310,
  author    = {D. P. Kaur and A. Mantri},
  title     = {Computer vision and sensor fusion for efficient hybrid tracking in augmented reality systems},
  booktitle = {2015 IEEE 3rd International Conference on MOOCs, Innovation and Technology in Education (MITE)},
  year      = {2015},
  pages     = {176-181},
  month     = {Oct},
  abstract  = {Augmented Reality is one of the most fundamental enabling technologies of this era which modifies our perception to such an extent that we are able to see, hear and feel the ordinary everyday objects in a new and enriched way. This technology is an amalgamation of Real and Virtual Worlds which aims to enhance the physical information by exactly super-imposing the computer generated content on it. The technology can prove helpful to surgeons and doctors to visualize and train a surgery, can instruct a mechanic for repair of an unknown piece of equipment, can serve as a source of entertainment, can help soldiers for spotting enemy snipers, and can be used as an efficient teaching aid for education. A typical Augmented Reality (AR) system requires the integration of hardware and software with factors such as Tracking, Registration, Interaction and Display playing an important role in the system design. The focus of this paper is specifically towards surveying the available Tracking Techniques for various Augmented Reality applications. Based on the existing methods of sensor based, vision based and hybrid tracking, a new method for efficient tracking is proposed.},
  doi       = {10.1109/MITE.2015.7375310},
  keywords  = {augmented reality;computer vision;sensor fusion;AR system;Virtual Worlds amalgamation;augmented reality system;computer generated content;computer vision;hybrid tracking;real world amalgamation;sensor fusion;surgery;Augmented reality;Cameras;Education;Global Positioning System;Mobile communication;Robustness;Tracking;Augmented Reality;Augmented Reality for Education;Hybrid Tracking;Mobile AR;Sensor Based Tracking;Vision Based Tracking},
}

@Article{7150416,
  author   = {N. Haouchine and J. Dequidt and M. O. Berger and S. Cotin},
  title    = {Monocular 3D Reconstruction and Augmentation of Elastic Surfaces with Self-Occlusion Handling},
  journal  = {IEEE Transactions on Visualization and Computer Graphics},
  year     = {2015},
  volume   = {21},
  number   = {12},
  pages    = {1363-1376},
  month    = {Dec},
  issn     = {1077-2626},
  abstract = {This paper focuses on the 3D shape recovery and augmented reality on elastic objects with self-occlusions handling, using only single view images. Shape recovery from a monocular video sequence is an underconstrained problem and many approaches have been proposed to enforce constraints and resolve the ambiguities. State-of-the art solutions enforce smoothness or geometric constraints, consider specific deformation properties such as inextensibility or resort to shading constraints. However, few of them can handle properly large elastic deformations. We propose in this paper a real-time method that uses a mechanical model and able to handle highly elastic objects. The problem is formulated as an energy minimization problem accounting for a non-linear elastic model constrained by external image points acquired from a monocular camera. This method prevents us from formulating restrictive assumptions and specific constraint terms in the minimization. In addition, we propose to handle self-occluded regions thanks to the ability of mechanical models to provide appropriate predictions of the shape. Our method is compared to existing techniques with experiments conducted on computer-generated and real data that show the effectiveness of recovering and augmenting 3D elastic objects. Additionally, experiments in the context of minimally invasive liver surgery are also provided and results on deformations with the presence of self-occlusions are exposed.},
  doi      = {10.1109/TVCG.2015.2452905},
  keywords = {augmented reality;image reconstruction;image sequences;3D shape recovery;augmented reality;elastic objects;elastic surfaces augmentation;energy minimization problem;external image points;mechanical model;monocular 3D reconstruction;monocular video sequence;nonlinear elastic model;self-occlusion handling;single view images;Augmented reality;Computational modeling;Context modeling;Deformable models;Mathematical model;Shape analysis;Three-dimensional displays;Virtual reality;Computer Assisted Surgery;Elastic Augmented Reality;Image-guided Simulation;Image-guided simulation;Non-rigid Registration;Physics-based Modeling;computer assisted surgery;elastic augmented reality;non-rigid registration;physics-based modeling;0},
}

@InProceedings{7163851,
  author    = {M. Yigitsoy and V. Belagiannis and A. Djurka and A. Katouzian and S. Ilic and E. Pernuš and A. Eslami and N. Navab},
  title     = {Random ferns for multiple target tracking in microscopic retina image sequences},
  booktitle = {2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)},
  year      = {2015},
  pages     = {209-212},
  month     = {April},
  abstract  = {Accurate and robust tracking of retina in operating microscope images is critical for an augmented reality assistance system for retinal surgery. Most retinal surgeries such as the peeling are performed using hand-held intraocular light and hence the tool and its shadow have two different motions, independent from the motion of the retina. In this paper, we propose multi-object motion estimation in high definition operating microscopic images by using a parallel network of random ferns, followed by RANSAC in order to achieve a simultaneous and robust tracking of the retina, the tool, and the tool shadow. Thanks to the separate tracking of each object, the number of outliers is dramatically reduced and the extracted motions are more accurate and reliable even in complex scenes which are considerably occluded by the tool and its shadow. The proposed method is evaluated on several challenging sequences in comparison with SIFT tracking, direct visual tracking, and single random ferns tracking of the retina. The experimental results show that the proposed method has a significantly higher success rate in comparison to the other three approaches with the accuracy of 4 pixels in tractable frames which is comparable with the intra- and inter-observer error of manual tracking (3.4 and 8.5 pixels, respectively).},
  doi       = {10.1109/ISBI.2015.7163851},
  issn      = {1945-7928},
  keywords  = {augmented reality;biomedical optical imaging;eye;gaze tracking;image sequences;medical image processing;motion estimation;object tracking;surgery;target tracking;RANSAC;SIFT tracking;augmented reality assistance system;complex scenes;direct visual tracking;hand-held intraocular light;high definition operating microscopic images;interobserver error;intraobserver error;manual tracking;microscopic retina image sequences;multiobject motion estimation;multiple target tracking;object tracking;peeling;retina motion;retina tracking;retinal surgery;robust tracking;shadow motions;simultaneous tracking;single random ferns tracking;tool shadow;tractable frames;Retina;Robustness;Surgery;Target tracking;Training;Visualization},
}

@InProceedings{7067098,
  author    = {J. G. Grandi and A. Maciel and H. G. Debarba and D. J. Zanchet},
  title     = {Spatially aware mobile interface for 3D visualization and interactive surgery planning},
  booktitle = {2014 IEEE 3nd International Conference on Serious Games and Applications for Health (SeGAH)},
  year      = {2014},
  pages     = {1-8},
  month     = {May},
  abstract  = {While medical images are fundamental in the surgery planning procedure, the process of analysis of such images slice-by-slice is still tedious and inefficient. In this work we introduce a system for exploration of the internal anatomy structures directly on the surface of the real body using a mobile display device as a window to the interior of the patient's body. The method is based on volume visualization of standard computed tomography datasets and augmented reality for interactive visualization of the generated volume. It supports our liver surgery planner method in the analysis of the segmented liver and in the color classification of the vessels. We present a set of experiments showing the system's ability to operate on mobile devices. Quantitative performance results are detailed, and applications in teaching anatomy and doctor-patient communication are discussed.},
  doi       = {10.1109/SeGAH.2014.7067098},
  keywords  = {augmented reality;computer aided instruction;computerised tomography;data visualisation;graphical user interfaces;image classification;image colour analysis;image segmentation;interactive systems;liver;medical image processing;mobile computing;surgery;teaching;3D visualization;anatomy teaching;augmented reality;doctor-patient communication;interactive surgery planning;interactive visualization;internal anatomy structures;liver segmentation;liver surgery planner method;medical images;mobile devices;mobile display device;patient body;quantitative performance;real body surface;spatially aware mobile interface;standard computed tomography datasets;vessel color classification;volume visualization;Biomedical imaging;Cameras;Liver;Mobile communication;Planning;Surgery;Three-dimensional displays},
}

@InProceedings{6907577,
  author    = {M. K. Ackerman and A. Cheng and E. Boctor and G. Chirikjian},
  title     = {Online ultrasound sensor calibration using gradient descent on the Euclidean Group},
  booktitle = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
  year      = {2014},
  pages     = {4900-4905},
  month     = {May},
  abstract  = {Ultrasound imaging can be an advantageous imaging modality for image guided surgery. When using ultrasound imaging (or any imaging modality), calibration is important when more advanced forms of guidance, such as augmented reality systems, are used. There are many different methods of calibration, but the goal of each is to recover the rigid body transformation relating the pose of the probe to the ultrasound image frame. This paper presents a unified algorithm that can solve the ultrasound calibration problem for various calibration methodologies. The algorithm uses gradient descent optimization on the Euclidean Group. It can be used in real time, also serving as a way to update the calibration parameters on-line. We also show how filtering, based on the theory of invariants, can further improve the online results. Focusing on two specific calibration methodologies, the AX = XB problem and the BX-1p problem, we demonstrate the efficacy of the algorithm in both simulation and experimentation.},
  doi       = {10.1109/ICRA.2014.6907577},
  issn      = {1050-4729},
  keywords  = {calibration;computerised instrumentation;filtering theory;optimisation;ultrasonic imaging;ultrasonic transducers;Euclidean group;filtering;gradient descent optimization;image guided surgery;online ultrasound sensor calibration;rigid body transformation;ultrasound imaging;Bismuth;Calibration;Cost function;Imaging;Robot sensing systems;Ultrasonic imaging},
}

@InProceedings{6860086,
  author    = {C. Ledermann and H. Alagi and H. Woern and R. Schirren and S. Reiser},
  title     = {Biomimetic tactile sensor based on Fiber Bragg Gratings for tumor detection #x2014; Prototype and results},
  booktitle = {2014 IEEE International Symposium on Medical Measurements and Applications (MeMeA)},
  year      = {2014},
  pages     = {1-6},
  month     = {June},
  abstract  = {Robot assisted minimally invasive surgery features many advantages, such as augmented reality or more comfort for the surgeon during the operation. On the downside, the surgeon loses all aspects of haptic perception from the operation situ. Tactile sensors are necessary to restore some of the haptic perception. In this paper, a tactile sensor is presented that mimics the human finger and is based on Fiber Bragg Gratings. The idea is a comparison of strain measurements of two materials of different stiffnesses. The principle is described in detail, as well as the fabrication of our first prototype. First experiments have been carried out, showing the potential of the tactile sensor to detect tumors within healthy tissue by looking at the progression of the strain-strain-curve.},
  doi       = {10.1109/MeMeA.2014.6860086},
  keywords  = {Bragg gratings;biomechanics;biomedical measurement;biomimetics;fibre optic sensors;strain measurement;tactile sensors;tumours;biomimetic tactile sensor;fiber bragg gratings;healthy tissue;human finger;strain measurements;strain-strain-curve;tumor detection;Bragg gratings;Gratings;Haptic interfaces;Phantoms;Tactile sensors;Tumors},
}

@InProceedings{1191126,
  author    = {U. Bockholt and A. Bisler and M. Becker and W. Muller-Wittig and G. Voss},
  title     = {Augmented reality for enhancement of endoscopic interventions},
  booktitle = {IEEE Virtual Reality, 2003. Proceedings.},
  year      = {2003},
  pages     = {97-101},
  month     = {March},
  abstract  = {Computer assisted operation planning systems win more and more recognition in the field of surgery. These systems offer new possibilities to prepare an intervention with the goal to shorten the expansive time in the operation room required for the intervention. The safest and most effective surgical approach should be selected. But often, it is difficult to transfer the output of the planning system to the intra-operative situation and so to consider the planning results in the real intervention. At the Fraunhofer Institute for Computer Graphics (IGD) in Darmstadt and the Centre for Advanced Media Technology (CAMTech) in Singapore, methods are developed to bridge the gap between the external planning session and the intra-operative case: augmented reality (AR) techniques are used to overlap preoperative scanned image data as well as results of the planning session to the operation field.},
  doi       = {10.1109/VR.2003.1191126},
  file      = {:1191126 - Augmented reality for enhancement of endoscopic interventions.pdf:PDF},
  issn      = {1087-8270},
  keywords  = {augmented reality;endoscopes;medical image processing;planning;surgery;augmented reality;computer assisted operation planning systems;endoscopic intervention enhancement;intra-operative case;preoperative scanned image data;surgery;Augmented reality;Biological tissues;Biomedical imaging;Deformable models;Displays;Endoscopes;Neurosurgery;Orthopedic surgery;Shape;Technology planning},
}

@InProceedings{913788,
  author    = {P. J. Passmore and C. F. Nielsen and W. J. Cosh and A. Darzi},
  title     = {Effects of viewing and orientation on path following in a medical teleoperation environment},
  booktitle = {Proceedings IEEE Virtual Reality 2001},
  year      = {2001},
  pages     = {209-215},
  month     = {March},
  abstract  = {The use of virtual and augmented reality techniques in medicine is rapidly increasing particularly in the area of minimal access surgery. Such surgery is a form of teleoperation in which accurate perception of depth and orientation, navigation, and interaction with the operative space are vital. Virtual and augmented reality techniques will allow us to produce new views of the operative site and introduce extra information into the scene such as safe paths for instruments to follow etc. A path following task is developed and human factors issues are addressed by varying viewing conditions (standard mono, stereo, multiple views and tool-linked view), presence or absence of haptic feedback, and orientation of the task. The results show that performance is improved with haptic feedback, but not by the various viewing conditions and is significantly worse for side aligned orientations.},
  doi       = {10.1109/VR.2001.913788},
  keywords  = {augmented reality;force feedback;haptic interfaces;human factors;medical computing;surgery;telemedicine;virtual reality;accurate depth perception;accurate interaction perception;accurate navigation perception;accurate orientation perception;augmented reality techniques;haptic feedback;human factors;medical teleoperation environment;medicine;minimal access surgery;operative site;orientation;path following;side aligned orientations;viewing;virtual reality techniques;Augmented reality;Feedback;Haptic interfaces;Human factors;Instruments;Layout;MONOS devices;Navigation;Standards development;Surgery},
}

@InProceedings{900419,
  author    = {J. H. Ku and D. P. Jang and H. J. Jo and I. Y. Kim and S. I. Kim},
  title     = {An augmented reality visualization in IGS system},
  booktitle = {Proceedings of the 22nd Annual International Conference of the IEEE Engineering in Medicine and Biology Society (Cat. No.00CH37143)},
  year      = {2000},
  volume    = {3},
  pages     = {1741-1742 vol.3},
  abstract  = {Augmented reality is a technology in which a computer-generated image is superimposed onto the user's vision of the real world. This gives the user additional information generated from the computer generated model. Especially, in the medical field, this technique can be used for understanding the spatial position of a patient's organ during an operation. We applied this technique to an image guided surgery (IGS) system and tested it with a skull phantom},
  doi       = {10.1109/IEMBS.2000.900419},
  issn      = {1094-687X},
  keywords  = {augmented reality;image registration;image segmentation;medical image processing;surgery;CT images;MRI;augmented reality visualization;computer-generated image;image guided surgery system;image registration;image segmentation;marching cube algorithm;neurosurgery;organ spatial position;point matching algorithm;skull phantom;virtual model;Augmented reality;Biomedical imaging;Calibration;Cameras;Computer vision;Equations;Surges;System testing;Transforms;Visualization},
}

@Article{8003599,
  author   = {D. Williams},
  title    = {Time for a 5G Technology Assessment and Road Map? [President's Column]},
  journal  = {IEEE Microwave Magazine},
  year     = {2017},
  volume   = {18},
  number   = {6},
  pages    = {10-14},
  month    = {Sept},
  issn     = {1527-3342},
  abstract = {Perhaps what many of us find most exciting are the outlines of next-generation (5G) application spaces that would be feasible if we could unlock the great open spaces in the millimeter-wave (mmW) bands for mobile communications, allowing real-time transfer of not only conventional data but also high-definition video and haptic (tactile and kinesthetic) information on millisecond time scales. Those spaces include applications in augmented reality, which relies on telecommunications to transfer high-definition video to a server where it is processed, augmented, and then sent back to the user in real time. The automotive industry is not just betting on systems that allow cars to negotiate the streets based on sensing the immediate surroundings; they envision globally aware autonomous automobiles that leverage immediate feedback from all the surrounding autonomous systems to maximize throughput and increase speeds, while driving more closely together and more safely than human drivers or their locally aware counterparts. And, of course, the public safety and other sectors are eagerly awaiting advances in communications that will allow robots to transfer not only real-time, high-definition video but haptic feedback as they fight fires, safely perform surgery, and carry out other functions to protect their human counterparts in the field. These applications are expected to create a US$3 trillion per year global market, and the companies and countries that can successfully unlock the mmW spectrum will immediately gain a critical advantage in deployment over their competitors.},
  doi      = {10.1109/MMM.2017.2716558},
  keywords = {5G mobile communication;next generation networks;5G technology assessment;millimeter-wave bands;mobile communications;next-generation application spaces},
}

@InProceedings{1196293,
  title     = {Conference Proceedings. 1st International IEEE EMBS Conference on Neural Engineering 2003 (Cat. No.03EX606)},
  booktitle = {First International IEEE EMBS Conference on Neural Engineering, 2003. Conference Proceedings.},
  year      = {2003},
  month     = {March},
  abstract  = {The following topics are dealt with: neural engineering; brain and neurons; artificial implants/neural prostheses; biological neural networks; neurological systems control; neural signal processing; neural informatics; brain imaging; brain-computer interface; and virtual and augmented reality in brain surgery, diagnosis and treatment.},
  doi       = {10.1109/CNE.2003.1196293},
  keywords  = {biomedical imaging;brain;medical signal processing;neural nets;neurophysiology;prosthetics;user interfaces;virtual reality;artificial implants;augmented reality;biological neural networks;brain imaging;brain surgery;brain-computer interface;neural engineering;neural informatics;neural prostheses;neural signal processing;neurological systems control;virtual reality},
}

@Article{8026164,
  author   = {F. J. Detmer and J. Hettig and D. Schindele and M. Schostak and C. Hansen},
  title    = {Virtual and Augmented Reality Systems for Renal Interventions: A Systematic Review},
  journal  = {IEEE Reviews in Biomedical Engineering},
  year     = {2017},
  volume   = {PP},
  number   = {99},
  pages    = {1-1},
  issn     = {1937-3333},
  abstract = {Purpose: Many virtual and augmented reality systems have been proposed to support renal interventions. This paper reviews such systems employed in the treatment of renal cell carcinoma and renal stones.},
  doi      = {10.1109/RBME.2017.2749527},
  keywords = {Augmented reality;Kidney;Laparoscopes;Minimally invasive surgery;Solid modeling;Tumors;Augmented Reality;Image-Guided Surgery;Nephrectomy;Renal Interventions;Virtual Reality},
}

@Article{7588033,
  author   = {F. Clemente and S. Dosen and L. Lonini and M. Markovic and D. Farina and C. Cipriani},
  title    = {Humans Can Integrate Augmented Reality Feedback in Their Sensorimotor Control of a Robotic Hand},
  journal  = {IEEE Transactions on Human-Machine Systems},
  year     = {2017},
  volume   = {47},
  number   = {4},
  pages    = {583-589},
  month    = {Aug},
  issn     = {2168-2291},
  abstract = {Tactile feedback is pivotal for grasping and manipulation in humans. Providing functionally effective sensory feedback to prostheses users is an open challenge. Past paradigms were mostly based on vibro- or electrotactile stimulations. However, the tactile sensitivity on the targeted body parts (usually the forearm) is greatly less than that of the hand/fingertips, restricting the amount of information that can be provided through this channel. Visual feedback is the most investigated technique in motor learning studies, where it showed positive effects in learning both simple and complex tasks; however, it was not exploited in prosthetics due to technological limitations. Here, we investigated if visual information provided in the form of augmented reality (AR) feedback can be integrated by able-bodied participants in their sensorimotor control of a pick-and-lift task while controlling a robotic hand. For this purpose, we provided visual continuous feedback related to grip force and hand closure to the participants. Each variable was mapped to the length of one of the two ellipse axes visualized on the screen of wearable single-eye display AR glasses. We observed changes in behavior when subtle (i.e., not announced to the participants) manipulation of the AR feedback was introduced, which indicated that the participants integrated the artificial feedback within the sensorimotor control of the task. These results demonstrate that it is possible to deliver effective information through AR feedback in a compact and wearable fashion. This feedback modality may be exploited for delivering sensory feedback to amputees in a clinical scenario.},
  doi      = {10.1109/THMS.2016.2611998},
  keywords = {augmented reality;control engineering computing;dexterous manipulators;augmented reality feedback;clinical scenario;electrotactile stimulation;feedback modality;robotic hand;sensorimotor control;tactile feedback;vibrotactile stimulation;visual feedback;wearable single-eye display AR glasses;Glass;Indexes;Prosthetics;Robot sensing systems;Thumb;Visualization;Augmented reality (AR);motor learning;sensorimotor control;sensory substitution;visual system;wearable systems},
}

@Article{6847730,
  author   = {P. Miraldo and H. Araujo},
  title    = {Direct Solution to the Minimal Generalized Pose},
  journal  = {IEEE Transactions on Cybernetics},
  year     = {2015},
  volume   = {45},
  number   = {3},
  pages    = {404-415},
  month    = {March},
  issn     = {2168-2267},
  abstract = {Pose estimation is a relevant problem for imaging systems whose applications range from augmented reality to robotics. In this paper we propose a novel solution for the minimal pose problem, within the framework of generalized camera models and using a planar homography. Within this framework and considering only the geometric elements of the generalized camera models, an imaging system can be modeled by a set of mappings associating image pixels to 3-D straight lines. This mapping is defined in a 3-D world coordinate system. Pose estimation performs the computation of the rigid transformation between the original 3-D world coordinate system and the one in which the camera was calibrated. Using synthetic data, we compare the proposed minimal-based method with the state-of-the-art methods in terms of numerical errors, number of solutions and processing time. From the experiments, we conclude that the proposed method performs better, especially because there is a smaller variation in numerical errors, while results are similar in terms of number of solutions and computation time. To further evaluate the proposed approach we tested our method with real data. One of the relevant contributions of this paper is theoretical. When compared to the state-of-the-art approaches, we propose a completely new parametrization of the problem that can be solved in four simple steps. In addition, our approach does not require any predefined transformation of the dataset, which yields a simpler solution for the problem.},
  doi      = {10.1109/TCYB.2014.2326970},
  keywords = {pose estimation;augmented reality;generalized camera models;geometric elements;image pixel;imaging systems;minimal generalized pose estimation;minimal pose problem;minimal-based method;planar homography;robotics;three-dimensional world coordinate system;Cameras;Computational modeling;Estimation;Polynomials;Vectors;Absolute pose;generalized camera models;homography matrix;minimal problems in computer vision},
}

@Article{7247759,
  author   = {C. G. Lee and I. Oakley and E. S. Kim and J. Ryu},
  title    = {Impact of Visual-Haptic Spatial Discrepancy on Targeting Performance},
  journal  = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  year     = {2016},
  volume   = {46},
  number   = {8},
  pages    = {1098-1108},
  month    = {Aug},
  issn     = {2168-2216},
  abstract = {This paper presents a comprehensive study of the impact of visual-haptic spatial discrepancies on human performance in a targeting task conducted in a visual-haptic virtual and augmented environment. Moreover, it explores whether the impact of this effect varies with two additional variables: 1) haptic wall stiffness and 2) visual cursor diameter. Finally, we discuss the relative dominance of visual and haptic cues during a targeting task. The results indicate that while the spatial discrepancies studied exerted a small effect on the time required to perform targeting, they impacted the absolute errors considerably. Additionally, we report that haptic wall stiffness has a significant effect on absolute errors while the visual cursor diameter has a significant effect on movement time. Finally, we conclude that while both visual and haptic cues are important during targeting tasks, haptic cues played a more dominant role than visual cues. The results of this paper can be used to predict how human targeting performance will vary between systems, such as those using haptically enabled virtual reality or augmented reality technologies that feature visual-haptic spatial discrepancies.},
  doi      = {10.1109/TSMC.2015.2468675},
  keywords = {augmented reality;feature extraction;haptic interfaces;haptic cues;haptic wall stiffness;haptically enabled augmented reality;haptically enabled virtual reality;human performance;visual cues;visual cursor diameter;visual-haptic augmented environment;visual-haptic spatial discrepancy;visual-haptic virtual environment;Atmospheric measurements;Force;Haptic interfaces;Solid modeling;Surgery;Training;Visualization;Augmented reality (AR);force feedback;haptic interfaces;performance evaluation;surgery;virtual reality (VR)},
}

@InProceedings{7516351,
  author    = {F. Mohamed and S. C. C. Tong and B. Tomi and M. K. Mokhtar and Y. A. Yusoff},
  title     = {Heart Care Augmented Reality Mobile Simulation (heARt)},
  booktitle = {2015 4th International Conference on Interactive Digital Media (ICIDM)},
  year      = {2015},
  pages     = {1-6},
  month     = {Dec},
  abstract  = {Heart Care Augmented Reality Mobile Simulation (heARt) is an edutainment mobile based application that will helps in gaining such awareness of maintaining a healthy heart among the society. This application are augmented reality application that will combined both reality world and virtual reality world in order to give the users an interactive content of information. This application are developed to replace a traditional pamphlets or brochures that have no interactive content within it. HeARt, allows users to interact with the information that presented virtually on their phone screen. This kind of features are believed to be a lot more interesting in contrast with traditional brochures and pamphlets.},
  doi       = {10.1109/IDM.2015.7516351},
  keywords  = {augmented reality;cardiology;digital simulation;graphical user interfaces;human computer interaction;medical computing;mobile computing;HeARt;edutainment mobile based application;healthy heart maintenance awareness;heart care augmented reality mobile simulation;interactive information content;phone screen;virtual reality world;Augmented reality;Biomedical imaging;Heart;Mobile communication;Operating systems;Surgery;Augmented Reality;Medical;heARt},
}

@InProceedings{7328061,
  author    = {N. Leucht and S. Habert and P. Wucherer and S. Weidert and N. Navab and P. Fallavollita},
  title     = {[POSTER] Augmented Reality for Radiation Awareness},
  booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
  year      = {2015},
  pages     = {60-63},
  month     = {Sept},
  abstract  = {C-arm fluoroscopes are frequently used during surgeries for intraoperative guidance. Unfortunately, due to X-ray emission and scattering, increased radiation exposure occurs in the operating theatre. The objective of this work is to sensitize the surgeon to their radiation exposure, enable them to check on their exposure over time, and to help them choose their best position related to the C-arm gantry during surgery. First, we aim at simulating the amount of radiation that reaches the surgeon using the Geant4 software, a toolkit developed by CERN. Using a flexible setup in which two RGB-D cameras are mounted to the mobile C-arm, the scene is captured and modeled respectively. After the simulation of particles with specific energies, the dose at the surgeon's position, determined by the depth cameras, can be measured. The validation was performed by comparing the simulation results to both theoretical values from the C-arms user manual and real measurements made with a QUART didoSVM dosimeter. The average error was 16.46% and 16.39%, respectively. The proposed flexible setup and high simulation precision without a calibration with measured dosimeter values, has great potential to be directly used and integrated intraoperatively for dose measurement.},
  doi       = {10.1109/ISMAR.2015.21},
  keywords  = {augmented reality;diagnostic radiography;medical computing;surgery;C-arm fluoroscopes;C-arm gantry;Geant4 software;QUART didoSVM dosimeter;RGB-D camera;augmented reality;dose measurement;intraoperative surgery guidance;radiation awareness;radiation exposure;red-green-blue-depth camera;Calibration;Cameras;Computational modeling;Manuals;Photonics;Surgery;X-ray imaging;C-arm fluoroscopy;augmented reality;dose measurement;radiation exposure;visualization},
}

@InProceedings{7328089,
  author    = {A. Saito and M. Nakao and Y. Uranishi and T. Matsuda},
  title     = {[POSTER] Deformation Estimation of Elastic Bodies Using Multiple Silhouette Images for Endoscopic Image Augmentation},
  booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
  year      = {2015},
  pages     = {170-171},
  month     = {Sept},
  abstract  = {This study proposes a method to estimate elastic deformation using silhouettes obtained from multiple endoscopic images. Our method can estimate the intraoperative deformation of organs using a volumetric mesh model reconstructed from preoperative CT data. We use this elastic body silhouette information of elastic bodies not to model the shape but to estimate the local displacements. The model shape is updated to satisfy the silhouette constraint while preserving the shape as much as possible. The result of the experiments showed that the proposed methods could estimate the deformation with root mean square (RMS) errors of 5.0&ndash;10 mm.},
  doi       = {10.1109/ISMAR.2015.49},
  keywords  = {computerised tomography;endoscopes;image reconstruction;mean square error methods;medical image processing;RMS errors;elastic bodies deformation estimation;endoscopic image augmentation;multiple silhouette images;preoperative CT data reconstruction;root mean square error;volumetric mesh model;Augmented reality;Computational modeling;Computed tomography;Deformable models;Estimation;Shape;Surgery;Computer-assisted surgery;Deformation estimation;Shape matching},
}

@InProceedings{7328060,
  author    = {C. J. Paulus and N. Haouchine and D. Cazier and S. Cotin},
  title     = {Augmented Reality during Cutting and Tearing of Deformable Objects},
  booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
  year      = {2015},
  pages     = {54-59},
  month     = {Sept},
  abstract  = {Current methods dealing with non-rigid augmented reality only provide an augmented view when the topology of the tracked object is not modified, which is an important limitation. In this paper we solve this shortcoming by introducing a method for physics-based non-rigid augmented reality. Singularities caused by topological changes are detected by analyzing the displacement field of the underlying deformable model. These topological changes are then applied to the physics-based model to approximate the real cut. All these steps, from deformation to cutting simulation, are performed in real-time. This significantly improves the coherence between the actual view and the model, and provides added value.},
  doi       = {10.1109/ISMAR.2015.19},
  keywords  = {augmented reality;object tracking;augmented view;cutting simulation;deformable model;deformable object cutting;deformable object tearing;object tracking;physics-based model;physics-based nonrigid augmented reality;Augmented reality;Computational modeling;Deformable models;Feature extraction;Finite element analysis;Real-time systems;Surgery;Augmented Reality;Cutting;Deformation;Tearing},
}

@InProceedings{7328092,
  author    = {M. Nakao and S. Endo and K. Imanishi and T. Matsuda},
  title     = {[POSTER] Endoscopic Image Augmentation Reflecting Shape Changes in Cutting Procedures},
  booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
  year      = {2015},
  pages     = {176-177},
  month     = {Sept},
  abstract  = {This paper introduces a concept of endoscopic image augmentation that overlays shape changes to support cutting procedures. This framework handles the history of measured drill tip's location as a volume label, and visualizes the remains to be cut overlaid on the endoscopic image in real time. We performed a cutting experiment, and the efficacy of the cutting aid was evaluated among shape similarity, total moved distance of a cutting tool, and the required cutting time. The results of the experiments showed that cutting performance was significantly improved by the proposed framework.},
  doi       = {10.1109/ISMAR.2015.52},
  keywords  = {endoscopes;medical image processing;shape recognition;cutting aid;cutting procedures;endoscopic image augmentation;measured drill tip location;shape changes;shape similarity;Augmented reality;Cameras;Computed tomography;Shape;Surgery;Three-dimensional displays;Visualization;Endoscopic image augmentation;computer assisted surgery;volumetric cutting model},
}

@InProceedings{7286666,
  author    = {M. A. Faudzi and R. W. O. K. Rahmat and P. S. Sulaiman and M. Z. Dimon},
  title     = {Image Stitching of Textures for Augmented Reality Medical Training},
  booktitle = {2014 International Conference on Computer Assisted System in Health},
  year      = {2014},
  pages     = {32-37},
  month     = {Dec},
  abstract  = {Augmented reality is an area that requires the participants to be fully immersed in the environment. As such, the appearance of the object must be as close as possible with the real object, which is really challenging. In achieving a realistic model, a camera capture images are mapped onto the model. The main phase in fulfilling these criteria are seamless image registration and ensuring the mapping of the images onto the model. The virtual model, in this case, the virtual heart, is going to be created by capturing multiple images of the real heart from different angles. The images are then going to be stitched before they can be mapped onto the 3D model of a heart. A preliminary experiment on the texture mapping is implemented using two well-known methods, Scale Invariant Feature Transform and Speeded-up Robust Feature.},
  doi       = {10.1109/CASH.2014.20},
  keywords  = {augmented reality;biomedical education;computer based training;image registration;image texture;medical computing;transforms;SIFT;SURF;augmented reality medical training;image registration;scale invariant feature transform;speeded-up robust feature;texture image stitching;texture mapping;virtual model;Augmented reality;Biomedical imaging;Feature extraction;Heart;Image registration;Surgery;Training;Image Registration;Image Stitching;SIFT;SURF},
}

@Article{7164348,
  author   = {J. E. Swan and G. Singh and S. R. Ellis},
  title    = {Matching and Reaching Depth Judgments with Real and Augmented Reality Targets},
  journal  = {IEEE Transactions on Visualization and Computer Graphics},
  year     = {2015},
  volume   = {21},
  number   = {11},
  pages    = {1289-1298},
  month    = {Nov},
  issn     = {1077-2626},
  abstract = {Many compelling augmented reality (AR) applications require users to correctly perceive the location of virtual objects, some with accuracies as tight as 1 mm. However, measuring the perceived depth of AR objects at these accuracies has not yet been demonstrated. In this paper, we address this challenge by employing two different depth judgment methods, perceptual matching and blind reaching, in a series of three experiments, where observers judged the depth of real and AR target objects presented at reaching distances. Our experiments found that observers can accurately match the distance of a real target, but when viewing an AR target through collimating optics, their matches systematically overestimate the distance by 0.5 to 4.0 cm. However, these results can be explained by a model where the collimation causes the eyes' vergence angle to rotate outward by a constant angular amount. These findings give error bounds for using collimating AR displays at reaching distances, and suggest that for these applications, AR displays need to provide an adjustable focus. Our experiments further found that observers initially reach ~4 cm too short, but reaching accuracy improves with both consistent proprioception and corrective visual feedback, and eventually becomes nearly as accurate as matching.},
  doi      = {10.1109/TVCG.2015.2459895},
  keywords = {augmented reality;display devices;augmented reality target;blind reaching;collimating AR display;collimating optics;constant angular amount;depth judgment method;observer;perceptual matching;real target;virtual object;visual feedback;Accuracy;Augmented reality;Observers;Servers;Surgery;Target tracking;Visualization;Depth judgment;accommodation;augmented reality;blind reaching;perceptual matching;vergence},
}

@InProceedings{7223384,
  author    = {J. Morita and S. Shimamura and M. Kanegae and Y. Uema and M. Takahashi and M. Inami and T. Hayashida and M. Sugimoto},
  title     = {MRI overlay system using optical see-through for marking assistance},
  booktitle = {2015 IEEE Virtual Reality (VR)},
  year      = {2015},
  pages     = {239-240},
  month     = {March},
  abstract  = {In this paper we propose an augmented reality system that superimposes MRI onto the patient model. We use a half-silvered mirror and a handheld device to superimpose the MRI onto the patient model. By tracking the coordinates of the patient model and the handheld device using optical markers, we are able to transform the images to the correlated position. Voxel data of the MRI are made so that the user is able to view the MRI from many different angles.},
  doi       = {10.1109/VR.2015.7223384},
  issn      = {1087-8270},
  keywords  = {augmented reality;biomedical MRI;medical image processing;patient treatment;MRI overlay system;augmented reality system;marking assistance;optical see-through;patient model;Biomedical optical imaging;Data models;Handheld computers;Magnetic resonance imaging;Mirrors;Optical imaging;Solid modeling;Augmented Reality;MRI;Optical See-Through;breast cancer surgery;half-silvered mirror},
}

@InProceedings{7216873,
  author    = {H. C. Lee and J. D. Lee and S. T. Lee and C. T. Wu},
  title     = {Medical augmented reality for craniofacial application using surface deformation correction},
  booktitle = {2015 IEEE International Conference on Consumer Electronics - Taiwan},
  year      = {2015},
  pages     = {234-235},
  month     = {June},
  abstract  = {The paper presents a novel scheme that uses a projector to project a corrected CT image on the craniofacial surface for augmented reality visualization. The deformation of the projected image due to curved surface can be successful recovered via homography projection correction. The experimental result that shows the superior performance of this work is also included.},
  doi       = {10.1109/ICCE-TW.2015.7216873},
  keywords  = {augmented reality;computerised tomography;data visualisation;medical image processing;augmented reality visualization;corrected CT image;craniofacial application;homography projection correction;medical augmented reality;surface deformation correction;Augmented reality;Biomedical imaging;Cameras;Computed tomography;Surgery;Visualization},
}

@Article{6915708,
  author   = {B. Diotte and P. Fallavollita and L. Wang and S. Weidert and E. Euler and P. Thaller and N. Navab},
  title    = {Multi-Modal Intra-Operative Navigation During Distal Locking of Intramedullary Nails},
  journal  = {IEEE Transactions on Medical Imaging},
  year     = {2015},
  volume   = {34},
  number   = {2},
  pages    = {487-495},
  month    = {Feb},
  issn     = {0278-0062},
  abstract = {The interlocking of intramedullary nails is a technically demanding procedure which involves a considerable amount of X-ray acquisitions; one study lists as many as 48 to successfully complete the procedure and fix screws into 4-6 mm distal holes of the nail. We propose to design an augmented radiolucent drill to assist surgeons in completing the distal locking procedure without any additional X-ray acquisitions. Using an augmented reality fluoroscope that coregisters optical and X-ray images, we exploit solely the optical images to detect the augmented radiolucent drill and estimate its tip position in real-time. Consequently, the surgeons will be able to maintain the down the beam positioning required to drill the screws into the nail holes successfully. To evaluate the accuracy of the proposed augmented drill, we perform a preclinical study involving six surgeons and ask them to perform distal locking on dry bone phantoms. Surgeons completed distal locking 98.3% of the time using only a single X-ray image with an average navigation time of 1.4 ± 0.9 min per hole.},
  doi      = {10.1109/TMI.2014.2361155},
  keywords = {augmented reality;biomedical optical imaging;bone;data acquisition;diagnostic radiography;image registration;medical image processing;orthopaedics;phantoms;prosthetics;surgery;X-ray acquisitions;X-ray image coregistration;augmented radiolucent drill;augmented reality fluoroscope;beam positioning;distal holes;distal locking procedure;dry bone phantoms;intramedullary nails;multimodal intra-operative navigation;optical image coregistration;screws;size 4 mm to 6 mm;surgeons;Bones;Cameras;Fasteners;Nails;Surgery;Visualization;X-ray imaging;Augmented reality;C-arm fluoroscopy;intraoperative navigation;multi-modal imaging;orthopedic and trauma surgery;video guidance;visualization;0},
}

@InProceedings{6948442,
  author    = {V. Ferrari and F. Cutolo and E. M. Calabrò and M. Ferrari},
  title     = {[Poster] HMD Video see though AR with unfixed cameras vergence},
  booktitle = {2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
  year      = {2014},
  pages     = {265-266},
  month     = {Sept},
  abstract  = {Stereoscopic video see though AR systems permit accurate marker video based registration. To guarantee accurate registration, cameras are normally rigidly blocked while the user could require changing their vergence. We propose a solution working with lightweight hardware that, without the need for a new calibration of the cameras relative pose after each vergence adjustment, guarantees registration accuracy using pre-determined calibration data.},
  doi       = {10.1109/ISMAR.2014.6948442},
  keywords  = {augmented reality;cameras;helmet mounted displays;image registration;stereo image processing;video signal processing;HMD video see though AR;augmented reality;helmet mounted display;marker video based registration;pose calibration;registration accuracy;stereoscopic video;unfixed cameras vergence;vergence adjustment;Augmented reality;Calibration;Cameras;Fasteners;Hardware;Stereo image processing;Surgery;HMD;Vergence;Video See Though},
}

@InProceedings{6948432,
  author    = {N. Haouchine and J. Dequidt and M. O. Berger and S. Cotin},
  title     = {Single view augmentation of 3D elastic objects},
  booktitle = {2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
  year      = {2014},
  pages     = {229-236},
  month     = {Sept},
  abstract  = {This paper proposes an efficient method to capture and augment highly elastic objects from a single view. 3D shape recovery from a monocular video sequence is an underconstrained problem and many approaches have been proposed to enforce constraints and resolve the ambiguities. State-of-the art solutions enforce smoothness or geometric constraints, consider specific deformation properties such as inextensibility or ressort to shading constraints. However, few of them can handle properly large elastic deformations. We propose in this paper a real-time method which makes use of a mechanical model and is able to handle highly elastic objects. Our method is formulated as a energy minimization problem accounting for a non-linear elastic model constrained by external image points acquired from a monocular camera. This method prevents us from formulating restrictive assumptions and specific constraint terms in the minimization. The only parameter involved in the method is the Young's modulus where we show in experiments that a rough estimate of its value is sufficient to obtain a good reconstruction. Our method is compared to existing techniques with experiments conducted on computer-generated and real data that show the effectiveness of our approach. Experiments in the context of minimally invasive liver surgery are also provided.},
  doi       = {10.1109/ISMAR.2014.6948432},
  keywords  = {elastic deformation;image reconstruction;image sequences;minimisation;video signal processing;3D elastic object;3D shape recovery;Young modulus;deformation property;elastic deformation;energy minimization problem;minimally invasive liver surgery;monocular video sequence;shading constraint;single view augmentation;Computational modeling;Deformable models;Shape;Strain;Three-dimensional displays;Young's modulus;Computational Geometry and Object Modeling — Physically based modeling;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial;I.3.5 [Computer Graphics];augmented;virtual realities},
}
{6948510,
  author    = {Y. Oyamada},
  title     = {A } # x2018;Look Into #{x2019; Medical augmented reality},
  booktitle = {2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
  year      = {2014},
  pages     = {1-1},
  month     = {Sept},
  abstract  = {The concept of augmented reality (AR) has been introduced to variety of felds in the last decade. Recent development of portable devices such as smart phone and tablet PC provides the community a lot of possible applications in AR systems. Even in the medical feld, various AR systems have recently been proposed: systems for education, pre-planning, and those in the operating room. The aim of this tutorial is to bridge the expertise between the researchers in ISMAR community and medical doctors so that researchers can contribute to the medical domain with their specialty more than one can do right now. This tutorial aims to make a bridge between researchers in augmented reality feld and medical doctors. We target an audience interested in medical augmented reality systems.},
  doi       = {10.1109/ISMAR.2014.6948510},
  keywords  = {Augmented reality;Biomedical imaging;Bridges;Educational institutions;Surgery;Tutorials},
}

@InProceedings{6913103,
  author    = {A. C. M. T. G. d. Oliveira and R. Tori and J. L. Bernardes and R. S. Torres and F. L. S. Nunes},
  title     = {Simulation of Deformation in Models of Human Organs Using Physical Parameters},
  booktitle = {2014 XVI Symposium on Virtual and Augmented Reality},
  year      = {2014},
  pages     = {277-286},
  month     = {May},
  abstract  = {Simulating the deformation of human tissue during medical procedures that employ needles is important for representing the tissue's elastic behavior, thus providing visual realism to simulated medical training. This article presents a method for simulating the deformation of three-dimensional objects that represent human organs through many tissue layers, in which the following physical parameters are included: modulus of elasticity, girth and density. The use of layers allows for the attribution of physical parameters for each type of tissue that composes a specific organ. Furthermore, it is possible to employ some layers to simulate visual appearance of the deformation and others just to simulate tissue resistance when the needle is inserted. Experiments designed to validate this method were performed in a Virtual Reality Framework for puncture procedures. The results were satisfactory compared to the time of response, which was found to be suitable for haptic interaction. Consequently, it was possible to simulate human tissue deformation with the appropriate visual realism and haptic realism for medical training.},
  doi       = {10.1109/SVR.2014.24},
  keywords  = {biological organs;biological tissues;biomedical education;computer based training;digital simulation;elastic moduli;medical computing;virtual reality;3D object deformation;density;girth;haptic interaction;haptic realism;human organ representation;human tissue deformation simulation;medical procedure;modulus of elasticity;needle insertion;physical parameters;puncture procedure;simulated medical training;tissue elastic behavior representation;tissue layers;tissue resistance simulation;virtual reality framework;visual appearance simulation;visual realism;Computational modeling;Deformable models;Face;Materials;Solid modeling;Three-dimensional displays;Visualization;Three-dimensional interaction;deformable models;human tissue;surgery simulation},
}

@InProceedings{6902927,
  author    = {M. Jeevan and R. Jebaraj and R. Krishnakumar},
  title     = {In-vitro Validation of Image Guided Surgery System with 3D Pre-Operative Visualization for Atrial Transseptal Puncture},
  booktitle = {2014 18th International Conference on Information Visualisation},
  year      = {2014},
  pages     = {342-345},
  month     = {July},
  abstract  = {The left atrium (LA) is the most difficult cardiac chamber to access percutaneously. The route through the systemic venous system across the interatrial septum is mostly preferred to the more retrograde arterial route as larger catheters and devices could be manipulated safely. The transseptal (TS) puncture permits this direct route to the LA through the interatrial septum which is necessary in patients with a trial fibrillation (cardiac ablation), patent fossa ovalis (PFO), a trial septal defect (ASD) repair, left atrium appendage closure, balloon mitral valvuoloplasty, pulmonary vein stenos is intervention, Ante grade ventricular septal defect closure, stent implantation in the right internal carotid artery. For a safe TS puncture, one requires a delivery system and medical imaging software. At present doctors use biplanar fluoroscopic images during navigation and TS puncture. The two-dimensional echocardiography aids the doctor during the TS puncture. A complete three-dimensional visualization is yet to be established. We propose an efficient method for target localization pre-operatively and three-dimensional visualization with respect to catheter tip of the scene during the procedure. This technique eliminates fluoroscopy. The technique proposed in this work has been validated in-vitro using an a trial phantom, used to train doctors in electrophysiology (EP) labs. The pre-operative image obtained using MRI is registered with the phantom and the catheter tip location inside the phantom and is visualized in three-dimensions.},
  doi       = {10.1109/IV.2014.11},
  issn      = {1550-6037},
  keywords  = {biomedical MRI;echocardiography;image registration;medical image processing;surgery;3D preoperative visualization;EP;LA;TS puncture;atrial transseptal puncture;biplanar fluoroscopic images;cardiac MRI;cardiac chamber;echocardiography;electrophysiology;image guided surgery system;image registration;in-vitro validation;left atrium;Catheterization;Catheters;Heart;Phantoms;Solid modeling;Visualization;Atrial transseptal puncture;Cardiac MRI;Image guided surgery;Left heart catheterization;Virtual augmented reality},
}

@InProceedings{6802061,
  author    = {C. Hua and J. Edward Swan},
  title     = {The effect of an occluder on near field depth matching in optical see-through augmented reality},
  booktitle = {2014 IEEE Virtual Reality (VR)},
  year      = {2014},
  pages     = {81-82},
  month     = {March},
  abstract  = {We have conducted an experiment to study the effect of an occluding surface on the accuracy of near field depth matching in augmented reality (AR). Our experiment was based on replicating a similar experiment conducted by Edwards et al. [2]. We used an AR haploscope, which allows us to independently manipulate accommodative demand and vergence angle. Sixteen observers matched the perceived depth of an AR-presented virtual object with a physical pointer. Overall, observers overestimated depth by 6 mm or less with or without the presence of the occluder. The data from Edwards et al. [2] is normalized, and when we performed the same normalization procedure on our own data, our results do not agree with Edwards et al. [2]. We suspect that eye vergence explains these results.},
  doi       = {10.1109/VR.2014.6802061},
  issn      = {1087-8270},
  keywords  = {augmented reality;pattern matching;AR haploscope;AR-presented virtual object;accommodative demand;near field depth matching;normalization procedure;occluder;occluding surface;optical see-through augmented reality;perceived depth;vergence angle;Augmented reality;Biomedical optical imaging;Context;Educational institutions;Head;Observers;Surgery;Depth perception;augmented reality},
}

@InProceedings{6783230,
  author    = {G. Hamarneh and A. Amir-Khalili and M. S. Nosrati and I. Figueroa and J. Kawahara and O. Al-Alao and J. M. Peyrat and J. Abi-Nahed and A. Al-Ansari and R. Abugharbieh},
  title     = {Towards multi-modal image-guided tumour identification in robot-assisted partial nephrectomy},
  booktitle = {2nd Middle East Conference on Biomedical Engineering},
  year      = {2014},
  pages     = {159-162},
  month     = {Feb},
  abstract  = {Tumour identification is a critical step in robot-assisted partial nephrectomy (RAPN) during which the surgeon determines the tumour localization and resection margins. To help the surgeon in achieving this step, our research work aims at leveraging both pre- and intra-operative imaging modalities (CT, MRI, laparoscopic US, stereo endoscopic video) to provide an augmented reality view of kidney-tumour boundaries with uncertainty-encoded information. We present herein the progress of this research work including segmentation of preoperative scans, biomechanical simulation of deformations, stereo surface reconstruction from stereo endoscopic camera, pre-operative to intra-operative data registration, and augmented reality visualization.},
  doi       = {10.1109/MECBME.2014.6783230},
  issn      = {0018-9294},
  keywords  = {augmented reality;biomechanics;biomedical MRI;biomedical optical imaging;computerised tomography;deformation;endoscopes;image reconstruction;image registration;image segmentation;kidney;medical image processing;medical robotics;stereo image processing;surgery;tumours;CT;MRI;augmented reality visualization;biomechanical simulation;computed tomography;deformations;intraoperative data registration;kidney-tumour boundaries;laparoscopic US;magnetic resonance imaging;multimodal image-guided tumour identification;preoperative data registration;preoperative scan segmentation;robot-assisted partial nephrectomy;stereo endoscopic camera;stereo endoscopic video;stereo surface reconstruction;uncertainty-encoded information;Biological system modeling;Computed tomography;Kidney;Stereo image processing;Surface reconstruction;Surgery;Tumors},
}

@InProceedings{6775469,
  author    = {L. B. Porquis and D. Maemori and N. Nagaya and M. Konyo and S. Tadokoro},
  title     = {Presenting virtual stiffness by modulating the perceived force profile with suction pressure},
  booktitle = {2014 IEEE Haptics Symposium (HAPTICS)},
  year      = {2014},
  pages     = {289-294},
  month     = {Feb},
  abstract  = {This paper reports a study on modulating the perceived stiffness by controlling the perceived force evoked from suction pressure stimuli. It demonstrates an early attempt of using suction pressure stimuli for augmenting the perceived stiffness of a spring. The purpose of this work is twofold; 1) to validate a requirement needed for the device in force enhancement applications, 2) to tentatively explore the effect of suction pressure stimuli on stiffness perception. In this study, we used physical springs for the stiffness stimuli, and a tool (tactile interface) was used for stiffness exploration. Human subjects were requested to explore and estimate the stiffness of a spring sample. Suction pressure stimuli were applied on the contact areas between the finger the tool during stiffness exploration. The amount of suction stimuli adjusts correspondingly with the measured force, but it is regulated by a psychophysical function. We introduced the gain to scale the measured force, thereby adjusting the profile of the pressure stimuli. We found that the perceived stiffness of the spring appears to increase with higher gain. The result seems to suggest that stiffness augmentation is feasible by modulating the stiffness perception using multipoint suction pressure stimuli.},
  doi       = {10.1109/HAPTICS.2014.6775469},
  issn      = {2324-7347},
  keywords  = {augmented reality;force sensors;haptic interfaces;psychology;springs (mechanical);tactile sensors;contact areas;finger;force enhancement applications;multipoint suction pressure stimuli;perceived force control;perceived force profile modulation;physical springs;psychophysical function;stiffness augmentation;stiffness estimation;stiffness exploration;stiffness perception;stiffness perception modulation;stiffness stimuli;tactile interface;virtual stiffness;Estimation;Force;Grasping;Hoses;Skin;Springs;Surgery},
}

@InProceedings{6746949,
  author    = {C. Zhu and Y. Xiong and K. Xu and P. Shi},
  title     = {Fast cutting simulations with underlying lattices},
  booktitle = {2013 6th International Conference on Biomedical Engineering and Informatics},
  year      = {2013},
  pages     = {283-289},
  month     = {Dec},
  abstract  = {We present a method for real-time simulations of the arbitrary cutting of a deformable object. Our method augments the surface mesh with underlying lattices for fast and robust numerical simulations. The algorithm contains two phases: cutting and reconstruction. During the cutting, the underlying lattice is split approximately according to the blade and the soft body simulation is conducted on the lattices. Simulating the cutting and deformation on lattices rather than on the surface mesh avoids degenerating elements caused by cutting, which may lead to numerical instability. In the next, inspired by the fracture modeling, the surface mesh is split in response to the change of material stress tolerance caused by the blade in the simulation domain. In the reconstruction phase, we propose an improved Delaunay triangulation algorithm to reconstruct the inner incision surface, to alleviate the problems such as poor mesh quality and redundant faces. Due to the regularity of the lattice mesh, as well as the avoidance of the subdivision, our approach is fast and stable. The application of our method in a virtual training system for the liver surgery demonstrates its practical effectiveness.},
  doi       = {10.1109/BMEI.2013.6746949},
  issn      = {1948-2914},
  keywords  = {biomechanics;deformation;fracture mechanics;liver;medical computing;mesh generation;stress analysis;surgery;virtual reality;Delaunay triangulation algorithm;arbitrary cutting;blade;deformable object;fast cutting simulations;fast numerical simulations;fracture modeling;inner incision surface;lattice mesh regularity;liver surgery;material stress tolerance;mesh quality;numerical instability;real-time simulations;reconstruction phase;redundant faces;robust numerical simulations;simulation domain;soft body simulation;surface mesh;two-phase algorithm;virtual training system;Blades;Computational modeling;Lattices;Real-time systems;Surface cracks;Surface reconstruction;Surface treatment},
}

@InProceedings{6655797,
  author    = {A. C. M. T. G. Oliveira and R. Tori and W. Brito and J. Santos and H. H. Bíscaro and F. L. S. Nunes},
  title     = {Realistic Simulation of Deformation for Medical Training Applications},
  booktitle = {2013 XV Symposium on Virtual and Augmented Reality},
  year      = {2013},
  pages     = {272-275},
  month     = {May},
  abstract  = {Many computational applications for virtual medical training manipulate 3D objects that represent organs and human tissues. These representations, in function of the training requirements, may include parameters such as shape, topology, color, volume, texture and, in certain cases, physical properties such as elasticity and stiffness. An approach is presented in this paper, combining methods and models that are efficient to simulate elastic deformation, obtaining equilibrium between visual and haptic realism. Based on this model, visual and/or haptic outputs are generated for users in order to provide to learner sensations close enough to those they would have if the training were provided with real objects. The results regarding deformation processing time are compatible with those required for haptic interaction and provide adequate visual feedback from using meshes composed of a large number of polygons.},
  doi       = {10.1109/SVR.2013.44},
  keywords  = {biological tissues;biomedical education;computer based training;digital simulation;elastic deformation;haptic interfaces;medical computing;3D object manipulation;computational applications;deformation processing time;elastic deformation simulation;haptic interaction;haptic realism;human tissues representation;medical training application deformation;organ representation;polygons;realistic simulation;training requirements;virtual medical training;visual feedback;visual realism;Computational modeling;Deformable models;Haptic interfaces;Surgery;Three-dimensional displays;Training;Visualization;Deformable Object;Deformation Models;Virtual Reality;Visual Realism},
}

@InProceedings{6402569,
  author    = {T. Blum and R. Stauder and E. Euler and N. Navab},
  title     = {Superman-like X-ray vision: Towards brain-computer interfaces for medical augmented reality},
  booktitle = {2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
  year      = {2012},
  pages     = {271-272},
  month     = {Nov},
  abstract  = {This paper describes first steps towards a Superman-like X-ray vision where a brain-computer interface (BCI) device and a gaze-tracker are used to allow the user controlling the augmented reality (AR) visualization. A BCI device is integrated into two medical AR systems. To assess the potential of this technology first feedback from medical doctors is gathered. While in this pilot study not the full range of available signals but only electromyographic signals are used, the medical doctors provided very positive feedback on the use of BCI for medical AR.},
  doi       = {10.1109/ISMAR.2012.6402569},
  keywords  = {augmented reality;brain-computer interfaces;data visualisation;electromyography;medical signal processing;object tracking;visual evoked potentials;AR visualization;BCI device;Superman-like X-ray vision;brain-computer interface;electromyographic signal;gaze-tracker;medical AR system;medical augmented reality;Augmented reality;Biomedical imaging;Electromyography;Monitoring;Surgery;Visualization;X-ray imaging;H.5.1 [Information Interfaces and Presentation]: Artificial, augmented, and virtual realities—},
}

@InProceedings{6402565,
  author    = {J. Wang and P. Fallavollita and L. Wang and M. Kreiser and N. Navab},
  title     = {Augmented reality during angiography: Integration of a virtual mirror for improved 2D/3D visualization},
  booktitle = {2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
  year      = {2012},
  pages     = {257-264},
  month     = {Nov},
  abstract  = {Visualizing 2D and 3D anatomical information simultaneously within an X-ray image is challenging. Multiple monitors are required in the operating room to enable visualization of anatomical data by the surgeon. Consequently, this results in an interruption during the operation for proper assessment of information. In this paper, we introduce an interactive visualization of the 3D data from new perspectives including visualizing a virtual mirror from the same viewpoint as the X-ray source. The main contribution is our development of a complete angiographic visualization system that displays simultaneous 2D X-ray and 3D anatomical information in a common monitor for the surgeon in the operating room. No previous works have conceived the integration of the virtual mirror into the projection geometry of a C-arm fluoroscope. In total, 24 participants were asked to assess the benefits of the angiographic virtual mirror with different colour-depth encodings. The results of our feasibility study show a clear improvement when deciphering the true positions of aneurysms in X-ray. Lastly, color depth encoding improves correspondence between the 3D vasculature displayed in the virtual mirror to their projection images in X-ray.},
  doi       = {10.1109/ISMAR.2012.6402565},
  keywords  = {augmented reality;data visualisation;diagnostic radiography;image coding;image colour analysis;medical image processing;surgery;three-dimensional displays;2D anatomical information visualization;3D anatomical information visualization;3D data interactive visualization;3D vasculature;C-arm fluoroscope projection geometry;X-ray aneurysms;X-ray image;X-ray source;angiographic visualization system;augmented reality;color depth encoding;colour-depth encoding;deciphering;multiple monitors;operating room;simultaneous 2D X-ray anatomical information displays;simultaneous 3D anatomical information displays;surgeon;virtual mirror;Aneurysm;Data visualization;Encoding;Image color analysis;Mirrors;Rendering (computer graphics);X-ray imaging;2D/3D Registration;Angiography;Augmented Reality;Virtual Mirror;Visualization;X-ray},
}

@InProceedings{6263864,
  author    = {J. Puig and A. Perkis and F. Lindseth and T. Ebrahimi},
  title     = {Towards an efficient methodology for evaluation of quality of experience in Augmented Reality},
  booktitle = {2012 Fourth International Workshop on Quality of Multimedia Experience},
  year      = {2012},
  pages     = {188-193},
  month     = {July},
  abstract  = {The goal of this paper is to survey existing quality assessment methodologies for Augmented Reality (AR) visualization and to introduce a methodology for subjective quality assessment. Methodologies to assess the quality of AR systems have existed since these technologies appeared. The existing methodologies typically take an approach from the fields they are used in, such as ergonomics, usability, psychophysics or ethnography. Each field utilizes different methods, looking at different aspects of AR quality such as physical limitations, tracking loss or jitter, perceptual issues or feedback issues, just to name a few. AR systems are complex experiences, involving a mix of user interaction, visual perception, audio, haptic or other types of multimodal interactions as well. This paper focuses on the quality assessment of AR visualization, with a special interest on applications for neuronavigation.},
  doi       = {10.1109/QoMEX.2012.6263864},
  keywords  = {augmented reality;data visualisation;graphical user interfaces;neurophysiology;AR systems;augmented reality visualization;multimodal interactions;neuronavigation;quality assessment methodologies;quality-of-experience;user interaction;visual perception;Augmented reality;Ergonomics;Human computer interaction;Quality assessment;Surgery;Usability;Visualization;Augmented Reality;Objective Measurement;Quality of Experience;Subjective Quality Assessment},
}

@Article{6165241,
  author   = {N. Navab and T. Blum and L. Wang and A. Okur and T. Wendler},
  title    = {First Deployments of Augmented Reality in Operating Rooms},
  journal  = {Computer},
  year     = {2012},
  volume   = {45},
  number   = {7},
  pages    = {48-55},
  month    = {July},
  issn     = {0018-9162},
  abstract = {Researchers are developing augmented reality visualization systems to provide accessible and user-friendly interfaces for medical intervention and patient information systems. A related video can be seen here: http://youtu.be/kifj0ZP4Mos. It shows how augmented reality visualization systems can provide accessible and user-friendly interfaces for medical intervention and patient information systems.},
  doi      = {10.1109/MC.2012.75},
  keywords = {augmented reality;medical computing;user interfaces;augmented reality visualization systems;first deployments;medical intervention;operating rooms;patient information systems;user-friendly interfaces;Augmented reality;Computer aided diagnosis;Image reconstruction;Medical diagnostic imaging;Single photon emission computed tomography;Surgery;Three dimensional displays;User interfaces;Visualization;augmented reality;computer-assisted intervention},
}

@InProceedings{6180934,
  author    = {T. Blum and V. Kleeberger and C. Bichlmeier and N. Navab},
  title     = {mirracle: Augmented Reality in-situ visualization of human anatomy using a magic mirror},
  booktitle = {2012 IEEE Virtual Reality Workshops (VRW)},
  year      = {2012},
  pages     = {169-170},
  month     = {March},
  abstract  = {The mirracle system extends the concept of an Augmented Reality (AR) magic mirror to the visualization of human anatomy on the body of the user. Using a medical volume renderer a CT dataset is augmented onto the user. By a slice based user interface, slice from the CT and an additional photographic dataset can be selected.},
  doi       = {10.1109/VR.2012.6180934},
  issn      = {1087-8270},
  keywords  = {augmented reality;CT dataset;augmented reality;human anatomy visualization;magic mirror;medical volume renderer;mirracle system;photographic dataset;user interface;Biomedical imaging;Cameras;Computed tomography;Skeleton;Surgery;Visualization;X-ray imaging;H.5.1 [Information Interfaces and Presentation]: Artificial, augmented, and virtual realities},
}

@Article{5728803,
  author   = {M. Kersten-Oertel and P. Jannin and D. L. Collins},
  title    = {DVV: A Taxonomy for Mixed Reality Visualization in Image Guided Surgery},
  journal  = {IEEE Transactions on Visualization and Computer Graphics},
  year     = {2012},
  volume   = {18},
  number   = {2},
  pages    = {332-352},
  month    = {Feb},
  issn     = {1077-2626},
  abstract = {Mixed reality visualizations are increasingly studied for use in image guided surgery (IGS) systems, yet few mixed reality systems have been introduced for daily use into the operating room (OR). This may be the result of several factors: the systems are developed from a technical perspective, are rarely evaluated in the field, and/or lack consideration of the end user and the constraints of the OR. We introduce the Data, Visualization processing, View (DVV) taxonomy which defines each of the major components required to implement a mixed reality IGS system. We propose that these components be considered and used as validation criteria for introducing a mixed reality IGS system into the OR. A taxonomy of IGS visualization systems is a step toward developing a common language that will help developers and end users discuss and understand the constituents of a mixed reality visualization system, facilitating a greater presence of future systems in the OR. We evaluate the DVV taxonomy based on its goodness of fit and completeness. We demonstrate the utility of the DVV taxonomy by classifying 17 state-of-the-art research papers in the domain of mixed reality visualization IGS systems. Our classification shows that few IGS visualization systems' components have been validated and even fewer are evaluated.},
  doi      = {10.1109/TVCG.2011.50},
  keywords = {augmented reality;data visualisation;medical computing;surgery;DVV;IGS visualization systems;data-visualization processing-view taxonomy;image guided surgery system;mixed reality visualization;operating room;Brain;Data models;Data visualization;Surgery;Taxonomy;Virtual reality;Visualization;Taxonomy;augmented reality;augmented virtuality;image guided surgery.;mixed reality;visualization;Classification;Computer Graphics;Humans;Image Processing, Computer-Assisted;Models, Theoretical;Programming Languages;Reproducibility of Results;Surgery, Computer-Assisted;Terminology as Topic;User-Computer Interface},
}

@InProceedings{6094809,
  author    = {C. Shi and C. Tercero and S. Ikeda and T. Fukuda and K. Komori and K. Yamamoto},
  title     = {In-vitro three dimensional vasculature modeling based on sensor fusion between intravascular ultrasound and magnetic tracker},
  booktitle = {2011 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  year      = {2011},
  pages     = {2115-2120},
  month     = {Sept},
  abstract  = {This paper presents a sensor fusion between intravascular ultrasound (IVUS) and magnetic trackers for constructing the virtual reality three dimensional models of the blood vessels. We propose this approach for vasculature modeling as part of a guidance system relying on augmented reality for assistance during aortic stent graft deploy. This guidance will facilitate the alignment of the holes on the stent graft walls with the renal and mesenteric arteries ramifications. First we studied the disturbances on the magnetic tracker measurements induced by IVUS emitter after assembling the two sensors together. Then we performed a scan with the hybrid probe inside a blood silicone model submerged into a water tank, captured and fused data from both sensors. The dispersion of samples increased less than 1mm in the evaluated locations while the IVUS was activated. This enabled the construction of a three-dimensional model in virtual reality of the blood vessel model relying on the sensor fusion.},
  doi       = {10.1109/IROS.2011.6094809},
  issn      = {2153-0858},
  keywords  = {augmented reality;biomedical ultrasonics;blood vessels;medical signal processing;sensor fusion;aortic stent graft deploy;augmented reality;blood vessels;guidance system;in-vitro three dimensional vasculature modeling;intravascular ultrasound;magnetic tracker;mesenteric arteries;renal arteries;sensor fusion;three dimensional models;virtual reality;Biomedical imaging;Blood vessels;Dispersion;Magnetic resonance imaging;Probes;Solid modeling;Three dimensional displays},
}

@InProceedings{6093412,
  author    = {C. H. Hsieh and C. H. Huang and J. D. Lee and S. T. Lee},
  title     = {Estimating brain-surface deformation using stereo-camera and thin-plate spline},
  booktitle = {The 5th International Conference on New Trends in Information Science and Service Science},
  year      = {2011},
  volume    = {1},
  pages     = {159-164},
  month     = {Oct},
  abstract  = {This study proposes a method to estimate brain-surface deformation, which usually happens during cranium removal for brain surgery. The deformation estimation method utilizes a stereo-vision camera system to estimate brain surface structure and adopts thin-plate spline algorithm (TPS) to model the brain deformation. Before brain deformation, the full brain surface needs to be reconstructed by using pre-operative medical images or surface digitizing devices. Next, feature points on the surface are detected and reconstructed into 3-D shape from the images obtained by the stereo-vision cameras. While the deformation occurs, we estimate the corresponding points of the feature points in the current images. By associating the feature points before and after brain deformation, a globally smooth transformation can be estimated by TPS. The transformation is then applied to the full surface data, and the deformed brain surface is thus obtained. In the experiment, a porcine brain is used as the phantom to test the proposed method, and the result shows the average mean error is below 1.1 mm, which proves the superiority of this approach.},
  keywords  = {brain;cameras;deformation;feature extraction;image reconstruction;medical image processing;phantoms;splines (mathematics);stereo image processing;surgery;3D shape detection;3D shape reconstruction;brain surface structure;brain surgery;brain-surface deformation estimation;cranium removal;porcine brain;preoperative medical image;smooth transformation;stereo-vision camera system;thin-plate spline algorithm;Biomedical imaging;Brain;Cameras;Feature extraction;Image reconstruction;Surface reconstruction;Surgery},
}

@Article{5783434,
  author   = {G. Ferrigno and G. Baroni and F. Casolo and E. D. Momi and G. Gini and M. Matteucci and A. Pedrocchi},
  title    = {Medical Robotics},
  journal  = {IEEE Pulse},
  year     = {2011},
  volume   = {2},
  number   = {3},
  pages    = {55-61},
  month    = {May},
  issn     = {2154-2287},
  abstract = {Although the field of research in medical robotics and computer-aided therapy is diverse, it is very widely explored at Politecnico di Milano in several departments. Robotics, in particular, is exploited in both therapeutic and assistive fields, showing its great potential as an effective personal aid. Given the saturation and marginal cost reduction going on in the automotive market, health care represents one of the potential investments in the fields of service and professional robotics that will boost the research in mechatronics, augmented reality, and intelligence augmentation in the next years. Potential benefits, especially for the elderly, are straightforward in the field of robot-based assistive systems, which will allow for a better quality of life (daily life activities and mobility) even for severely disabled patients.},
  doi      = {10.1109/MPUL.2011.941523},
  keywords = {artificial intelligence;handicapped aids;medical diagnostic computing;medical robotics;patient treatment;Politecnico di Milano;computer aided therapy;computer-aided therapy;daily life activities;disabled patients;elderly patients;health care;intelligence augmentation;medical robotics;personal aid;professional robotics;robot-based assistive systems;Electromyography;Mobile robots;Prosthetics;Robot kinematics;Surgery;Artificial Limbs;Humans;Neurosurgical Procedures;Robotics;Therapy, Computer-Assisted},
}

@InProceedings{5336476,
  author    = {T. Blum and S. M. Heining and O. Kutter and N. Navab},
  title     = {Advanced training methods using an Augmented Reality ultrasound simulator},
  booktitle = {2009 8th IEEE International Symposium on Mixed and Augmented Reality},
  year      = {2009},
  pages     = {177-178},
  month     = {Oct},
  abstract  = {Ultrasound (US) is a medical imaging modality which is extremely difficult to learn as it is user-dependent, has low image quality and requires much knowledge about US physics and human anatomy. For training US we propose an Augmented Reality (AR) ultrasound simulator where the US slice is simulated from a CT volume. The location of the US slice inside the body is visualized using contextual in-situ techniques. We also propose advanced methods how to use an AR simulator for training.},
  doi       = {10.1109/ISMAR.2009.5336476},
  keywords  = {augmented reality;biomedical ultrasonics;computer based training;data visualisation;medical image processing;teaching;CT volume;US physics-and-human anatomy;US slice;augmented reality ultrasound simulator;contextual in-situ visualization;image quality;medical imaging;teaching;training method;Augmented reality;Biological system modeling;Biomedical imaging;Computed tomography;Human anatomy;Image quality;Medical simulation;Physics;Ultrasonic imaging;Visualization;H.5.1 [Information Interfaces and Presentation]: Artificial, augmented, and virtual realities;J.3 [Life and Medical Sciences]: —},
}

@InProceedings{5336459,
  author    = {A. Martin-Gonzalez and S. M. Heining and N. Navab},
  title     = {Head-mounted virtual loupe with sight-based activation for surgical applications},
  booktitle = {2009 8th IEEE International Symposium on Mixed and Augmented Reality},
  year      = {2009},
  pages     = {207-208},
  month     = {Oct},
  abstract  = {This work presents the development of an augmented reality magnification system, termed virtual loupe, implemented in a head-mounted display for surgical applications. The system provides a magnified view with a novel control based on tracked sight orientation. The system was evaluated by measuring the completion time of a suturing task performed by surgeons. The magnifying approach implemented proved to be useful by providing global context of the operating field. The sight-based activation was widely accepted by surgeons as a useful functionality to control viewing modalities.},
  doi       = {10.1109/ISMAR.2009.5336459},
  keywords  = {augmented reality;helmet mounted displays;medical computing;surgery;augmented reality magnification system;head-mounted virtual loupe;sight-based activation;surgical application;Application software;Augmented reality;Biomedical imaging;Cameras;Computer displays;Medical control systems;Microscopy;Surgery;Target tracking;Visualization;Augmented reality;medical visualization;user interaction},
}

@InProceedings{5336485,
  author    = {A. Kotranza and D. Scott Lind and C. M. Pugh and B. Lok},
  title     = {Real-time in-situ visual feedback of task performance in mixed environments for learning joint psychomotor-cognitive tasks},
  booktitle = {2009 8th IEEE International Symposium on Mixed and Augmented Reality},
  year      = {2009},
  pages     = {125-134},
  month     = {Oct},
  abstract  = {This paper proposes an approach to mixed environment training of manual tasks requiring concurrent use of psychomotor and cognitive skills. To train concurrent use of both skill sets, the learner is provided real-time generated, in-situ presented visual feedback of her performance. This feedback provides reinforcement and correction of psychomotor skills concurrently with guidance in developing cognitive models of the task. The general approach is presented: 1) Sensors placed in the physical environment detect in real-time a learner's manipulation of physical objects. 2) Sensor data is input to models of task performance which output quantitative measures of the learner's performance. 3) Pre-defined rules are applied to transform the learner's performance data into visual feedback presented in realtime and in-situ with the physical objects being manipulated. With guidance from medical education experts, we have applied this approach to a mixed environment for learning clinical breast exams (CBEs). CBE belongs to a class of tasks that require learning multiple cognitive elements and task-specific psychomotor skills. Traditional approaches to learning CBEs and other joint psychomotor-cognitive tasks rely on extensive one-onone training with an expert providing subjective feedback. By integrating real-time visual feedback of learners' quantitatively measured CBE performance, a mixed environment for learning CBEs provides on-demand learning opportunities with more objective, detailed feedback than available with expert observation. The proposed approach applied to learning CBEs was informally evaluated by four expert medical educators and six novice medical students. This evaluation highlights that receiving real-time in-situ visual feedback of their performance provides students an advantage, over traditional approaches to learning CBEs, in developing correct psychomotor and cognitive skills.},
  doi       = {10.1109/ISMAR.2009.5336485},
  keywords  = {augmented reality;biomedical education;computer based training;feedback;medical computing;patient diagnosis;clinical breast exam;cognitive skills;detailed feedback;learner manipulation;learning;medical education;medical students;mixed environment training;output quantitative measure;physical objects;psychomotor skills;real-time in-situ visual feedback;skills correction;skills reinforcement;task performance;Assembly;Breast;Computer applications;Education;Educational institutions;Object detection;Output feedback;Psychology;Virtual reality;Visualization;information visualization;mixed reality},
}

@InProceedings{5334085,
  author    = {R. Buttin and F. Zara and B. Shariat and T. Redarce},
  title     = {A biomechanical model of the female reproductive system and the fetus for the realization of a childbirth virtual simulator},
  booktitle = {2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society},
  year      = {2009},
  pages     = {5263-5266},
  month     = {Sept},
  abstract  = {Our main work consists in modeling of the female pelvis and uterus, as well as the human fetus. The goal of this work is to recover the different forces generated during the delivery. These forces will be input to the haptic obstetric training tool BirthSim which has already been developed by the Ampe're Laboratory at the INSA of Lyon. This modeling process will permit us to develop a new training device to take into account different anatomies and different types of delivery. In this paper, we will firstly show the different existing haptic and virtual simulators in the obstetric world with their advantages and drawbacks. After, we will present our approach based on a biomechanical modeling of concerned organs. To obtain interactive time performance, we proceed by the simplification of the organs anatomy. Then, we present some results showing that FEM analysis can be used to model forces during childbirth. In the future, we plan to use this work to more accurately control a childbirth simulator.},
  doi       = {10.1109/IEMBS.2009.5334085},
  issn      = {1094-687X},
  keywords  = {biological organs;finite element analysis;graphical user interfaces;haptic interfaces;obstetrics;physiological models;pneumatic actuators;pneumatic systems;training;BirthSim;FEM analysis;biomechanical model;childbirth virtual simulator;delivery;electro pneumatic system;female reproductive system;graphic user interface;haptic obstetric training tool;haptic simulators;human fetus;organs;pelvis;pneumatic actuator;uterus;Anatomy;Augmented reality;Computational modeling;Fetus;Haptic interfaces;Laboratories;Medical simulation;Orthopedic surgery;Pediatrics;Pelvis;0},
}

@Article{5184927,
  author   = {V. Ferrari* and G. Megali and E. Troia and A. Pietrabissa and F. Mosca},
  title    = {A 3-D Mixed-Reality System for Stereoscopic Visualization of Medical Dataset},
  journal  = {IEEE Transactions on Biomedical Engineering},
  year     = {2009},
  volume   = {56},
  number   = {11},
  pages    = {2627-2633},
  month    = {Nov},
  issn     = {0018-9294},
  abstract = {We developed a simple, light, and cheap 3-D visualization device based on mixed reality that can be used by physicians to see preoperative radiological exams in a natural way. The system allows the user to see stereoscopic ldquoaugmented images,rdquo which are created by mixing 3-D virtual models of anatomies obtained by processing preoperative volumetric radiological images (computed tomography or MRI) with real patient live images, grabbed by means of cameras. The interface of the system consists of a head-mounted display equipped with two high-definition cameras. Cameras are mounted in correspondence of the user's eyes and allow one to grab live images of the patient with the same point of view of the user. The system does not use any external tracker to detect movements of the user or the patient. The movements of the user's head and the alignment of virtual patient with the real one are done using machine vision methods applied on pairs of live images. Experimental results, concerning frame rate and alignment precision between virtual and real patient, demonstrate that machine vision methods used for localization are appropriate for the specific application and that systems based on stereoscopic mixed reality are feasible and can be proficiently adopted in clinical practice.},
  doi      = {10.1109/TBME.2009.2028013},
  keywords = {computer vision;medical image processing;stereo image processing;3-D mixed-reality system;3-D virtual models;3-D visualization device;MRI;alignment precision;augmented images;computed tomography;head-mounted display;high-definition cameras;machine vision methods;medical dataset;preoperative radiological exams;preoperative volumetric radiological image processing;stereoscopic visualization;Anatomy;Biomedical imaging;Cameras;Computed tomography;Displays;Eyes;Machine vision;Magnetic resonance imaging;Virtual reality;Visualization;Biomedical image processing;biomedical imaging;machine vision;stereo vision;virtual reality;Artificial Intelligence;Computer Graphics;Humans;Image Processing, Computer-Assisted;Imaging, Three-Dimensional;Magnetic Resonance Imaging;Phantoms, Imaging;Surgery, Computer-Assisted;Tomography, X-Ray Computed;User-Computer Interface;Video Recording},
}

@InProceedings{5171238,
  author    = {H. Qing-xi and L. Tao and Y. Yuan},
  title     = {An Easy System of Spatial Points Collection Based On ARtoolKit},
  booktitle = {2009 WRI World Congress on Computer Science and Information Engineering},
  year      = {2009},
  volume    = {1},
  pages     = {582-586},
  month     = {March},
  abstract  = {In this paper, we propose and develop an interactive system based on ARtoolKit, which can be employed to pick up points in spatial environment. This system is composed of four parts: Detection Pen (DP), single marker, webcam and computer. It is tracked using markers and the system translates the center position of DPpsila s leading marker to the location of pen-point within the webcam coordinate system. Therefore, the positions in physical world can be detected by acquiring the pen-point coordinate within the single marker coordinate system. As the markers-based image segmentation for a small region is swift, the process is a real-time. The experimental result indicates that it is able to pick-up points steadily. In addition, the system is easy and quite convenient to operate. So it has good potential in 3D interaction design.},
  doi       = {10.1109/CSIE.2009.680},
  keywords  = {biology computing;computers;image segmentation;medical image processing;3D interaction design;ARtoolKit;computer;detection pen;marker-based image segmentation;pen-point coordinate;single marker coordinate system;spatial points collection;webcam coordinate system;Bones;Computer aided manufacturing;Computer science;Costs;Focusing;Image segmentation;Interactive systems;Pulp manufacturing;Surgery;Systems engineering and theory;3D design;ARtoolKit;Augmented Reality;interactive},
}

@InProceedings{5166184,
  author    = {K. A. Everett and R. E. Exon and S. H. Rosales and G. J. Gerling},
  title     = {A virtual reality interface to provide point interaction and constriction to the finger},
  booktitle = {2009 Systems and Information Engineering Design Symposium},
  year      = {2009},
  pages     = {203-207},
  month     = {April},
  abstract  = {Virtual reality (VR) simulation of tube thoracostomy may improve the procedural training for medical and nursing students. Current VR simulators, however, do not provide tactile feedback, which is essential for enabling certain tasks (e.g., surface palpation to identify rib location, blunt dissection for access to pleural space surrounding the lungs, and finger sweep to confirm location in the pleural space). This work develops a physical apparatus that provides users with point feedback at the fingertip when palpating an external surface and a sensation of constriction around the finger during insertion into a body. The physical apparatus is composed of two components that separately control the constriction on the tip and middle of the finger. Each constriction component is made of two nylon casings coated with a silicone-elastomer that enclose about the top and bottom of the finger. DC gearhead motors control the magnitude of pressure in proportion to feedback from force transducers embedded in the silicone-elastomer. The device is intended to communicate with a virtual environment (written in H3D). The apparatus augments traditional stick-based force feedback and should enhance the learning of tactile tasks in tube thoracostomy.},
  doi       = {10.1109/SIEDS.2009.5166184},
  keywords  = {computer based training;force feedback;medical computing;virtual reality;force transducers;gearhead motors;nylon casings;physical apparatus;point feedback;point interaction;procedural training;silicone-elastomer;stick-based force feedback;tube thoracostomy;virtual reality interface;Art;Computational modeling;Fingers;Force feedback;Lungs;Medical services;Medical simulation;Surgery;Virtual environment;Virtual reality},
}

@Article{4781567,
  author   = {M. Feuerstein and T. Reichl and J. Vogel and J. Traub and N. Navab},
  title    = {Magneto-Optical Tracking of Flexible Laparoscopic Ultrasound: Model-Based Online Detection and Correction of Magnetic Tracking Errors},
  journal  = {IEEE Transactions on Medical Imaging},
  year     = {2009},
  volume   = {28},
  number   = {6},
  pages    = {951-967},
  month    = {June},
  issn     = {0278-0062},
  abstract = {Electromagnetic tracking is currently one of the most promising means of localizing flexible endoscopic instruments such as flexible laparoscopic ultrasound transducers. However, electromagnetic tracking is also susceptible to interference from ferromagnetic material, which distorts the magnetic field and leads to tracking errors. This paper presents new methods for real-time online detection and reduction of dynamic electromagnetic tracking errors when localizing a flexible laparoscopic ultrasound transducer. We use a hybrid tracking setup to combine optical tracking of the transducer shaft and electromagnetic tracking of the flexible transducer tip. A novel approach of modeling the poses of the transducer tip in relation to the transducer shaft allows us to reliably detect and significantly reduce electromagnetic tracking errors. For detecting errors of more than 5 mm, we achieved a sensitivity and specificity of 91% and 93%, respectively. Initial 3-D rms error of 6.91 mm were reduced to 3.15 mm.},
  doi      = {10.1109/TMI.2008.2008954},
  keywords = {biomedical ultrasonics;endoscopes;error correction;error detection;magneto-optical sensors;optical tracking;ultrasonic transducers;error correction;laparoscopic ultrasound transducers;magnetic tracking error correction;magneto-optical tracking;online error detection;transducer shaft;Electromagnetic fields;Electromagnetic interference;Error correction;Instruments;Laparoscopes;Magnetic susceptibility;Optical distortion;Shafts;Ultrasonic imaging;Ultrasonic transducers;Electromagnetic tracking;hybrid tracking;image-guided surgery;laparoscopic surgery;optical tracking;Calibration;Electromagnetic Fields;Equipment Failure;Humans;Laparoscopy;Online Systems;Optics and Photonics;Phantoms, Imaging;ROC Curve;Reproducibility of Results;Surgery, Computer-Assisted;Transducers;Ultrasonography},
}

@InProceedings{4810890,
  author    = {T. Edmunds and D. K. Pai},
  title     = {Perceptually augmented simulator design through decomposition},
  booktitle = {World Haptics 2009 - Third Joint EuroHaptics conference and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems},
  year      = {2009},
  pages     = {505-510},
  month     = {March},
  abstract  = {We approach the problem of determining a general method for augmenting haptic simulators to amplify the perceptually salient aspects of the interaction that induce effective skill transfer. Using such a method, we seek to simplify the design of haptic simulators that can improve training effectiveness without requiring expensive improvements in the capability of the rendering hardware. We present a decomposition approach to the automated design of perceptually augmented simulations, and we describe a user-study of the training effectiveness of a search-task simulator designed using our approach vs. an un-augmented simulator. The results indicate that our decomposition approach allows existing psychophysical findings to be leveraged in the design of haptic simulators that effectively impart skill by targeting perceptually significant aspects of the interaction.},
  doi       = {10.1109/WHC.2009.4810890},
  keywords  = {augmented reality;haptic interfaces;rendering (computer graphics);haptic simulators augmentation;perceptually augmented simulator design;rendering hardware;search-task simulator;Analytical models;Engines;Haptic interfaces;Hardware;Jamming;Shape;Surface texture;Surgery;Teleoperators;Virtual environment},
}

@Article{4670084,
  author   = {J. Traub and T. Sielhorst and S. M. Heining and N. Navab},
  title    = {Advanced Display and Visualization Concepts for Image Guided Surgery},
  journal  = {Journal of Display Technology},
  year     = {2008},
  volume   = {4},
  number   = {4},
  pages    = {483-490},
  month    = {Dec},
  issn     = {1551-319X},
  abstract = {Thanks to its rapid development in the last decades, image guided surgery (IGS) has been introduced successfully in many modern operating rooms. Current IGS systems provide their navigation information on a standard computer monitor. Alternatively, one could enhance the direct sight of the physician by an overlay of the virtual data onto the real patient view. Such in situ visualization methods have been proposed in the literature for providing a more intuitive visualization, improving the ergonomics as well as the hand-eye coordination. In this paper, we first discuss the fundamental issues and the recent endeavors in advanced display and visualization for IGS. We then present some of our recent work comparing two navigation systems: 1) a classical monitor based navigation and 2) a new navigation system we had developed based on in situ visualization. As both solutions reveal shortcomings as well as complementary advantages, we introduce a new solution that combines both concepts into one hybrid user interface. Finally, experimental results report on the performance of several surgeons using an external monitor as well as a stereo video see-through head-mounted display (HMD). The experiments consist of drilling into a phantom in order to reach planted deep-seated targets only visible in computed tomography (CT) data. We evaluate several visualization techniques, including the new hybrid solution, and study their influence on the performance of the participant surgeons.},
  doi      = {10.1109/JDT.2008.2006510},
  keywords = {augmented reality;biomedical imaging;computerised tomography;helmet mounted displays;phantoms;surgery;computed tomography;head-mounted display;image guided surgery;navigation information;phantom;visualization;Computed tomography;Computer displays;Data visualization;Drilling;Ergonomics;Imaging phantoms;Monitoring;Navigation;Surgery;User interfaces;in situ visualization;Augmented reality;image-guided surgery (IGS)},
}

@Article{4578605,
  author   = {D. Stoyanov and G. P. Mylonas and M. Lerotic and A. J. Chung and G. Z. Yang},
  title    = {Intra-Operative Visualizations: Perceptual Fidelity and Human Factors},
  journal  = {Journal of Display Technology},
  year     = {2008},
  volume   = {4},
  number   = {4},
  pages    = {491-501},
  month    = {Dec},
  issn     = {1551-319X},
  abstract = {With increasing capability and complexity of surgical interventions, intra-operative visualization is becoming an important part of a surgical environment. This paper reviews some of our recent progress in the intelligent use of pre- and intra-operative data for enhanced surgical navigation and motion compensated visualization. High fidelity augmented reality (AR) with enhanced 3D depth perception is proposed to provide effective surgical guidance. To cater for large scale tissue deformation, real-time depth recovery based on stereo disparity and eye gaze tracking is introduced. This allows the development of motion compensated visualization for improved visual perception and for facilitating motion adaptive AR displays. The discussion of the paper is focused on how to ensure perceptual fidelity of AR and the need for real-time tissue deformation recovery and modeling, as well as the importance of incorporating human perceptual factors in surgical displays.},
  doi      = {10.1109/JDT.2008.926497},
  keywords = {augmented reality;biological tissues;display instrumentation;medical image processing;medical robotics;motion compensation;surgery;virtual instrumentation;visual perception;augmented reality;enhanced 3D depth perception;eye gaze tracking;human perceptual factors;intra-operative visualization;motion adaptive AR display;motion compensated visualization;perceptual fidelity;robotic surgery;stereo disparity;surgical displays;surgical guidance;surgical navigation;tissue deformation;visual perception;Augmented reality;Data visualization;Displays;Heart;Human factors;Large-scale systems;Medical robotics;Minimally invasive surgery;Navigation;Surges;Augmented reality (AR);motion compensation;robotic surgery},
}

@InProceedings{4649091,
  author    = {C. A. Linte and A. D. Wiles and J. Moore and C. Wedlake and T. M. Peters},
  title     = {Surgical accuracy under virtual reality-enhanced ultrasound guidance: An in vitro epicardial dynamic study},
  booktitle = {2008 30th Annual International Conference of the IEEE Engineering in Medicine and Biology Society},
  year      = {2008},
  pages     = {62-65},
  month     = {Aug},
  abstract  = {In the context of our ongoing objective to reduce morbidity associated with cardiac interventions, minimizing invasiveness has inevitably led to more limited visual access to the target tissues. To ameliorate these challenges, we provide the surgeons with a complex visualization environment that integrates interventional ultrasound imaging augmented with pre-operative anatomical models and virtual surgical instruments within a virtual reality environment. In this paper we present an in vitro study on a cardiac phantom aimed at assessing the feasibility and targeting accuracy of our surgical system in comparison to traditional ultrasound imaging for intra-operative surgical guidance. The “therapy delivery” was modeled in the context of a blinded procedure, mimicking a closed-chest intervention. Four users navigated a tracked pointer to a target, under guidance provide by either US imaging or virtual reality-enhanced ultrasound. A 2.8 mm RMS targeting error was achieved using our novel surgical system, which is adequate from both a clinical and engineering perspective, under the inherent procedure requirements and limitations of the system.},
  doi       = {10.1109/IEMBS.2008.4649091},
  issn      = {1094-687X},
  keywords  = {Context modeling;Imaging phantoms;In vitro;Navigation;Surgery;Surgical instruments;Target tracking;Ultrasonic imaging;Virtual reality;Visualization;beating heart phantom;image guidance;minimally-invasive cardiac interventions;off-pump cardiac surgery;virtual reality environment;Cardiovascular Surgical Procedures;Computer Simulation;Echocardiography;Humans;Models, Cardiovascular;Pericardium;Reproducibility of Results;Sensitivity and Specificity;Surgery, Computer-Assisted;Ultrasonography, Interventional;User-Computer Interface},
}

@InProceedings{4649902,
  author    = {Jiann-Der Lee and Ting-An Chien and Chung-Hsien Huang and Chien-Tsai Wu and Shin-Tseng Lee},
  title     = {A navigation system of cerebral endovascular surgery integrating multiple space-guiding trackers},
  booktitle = {2008 30th Annual International Conference of the IEEE Engineering in Medicine and Biology Society},
  year      = {2008},
  pages     = {3269-3272},
  month     = {Aug},
  abstract  = {In this paper, a navigation system combining multiple space-guiding trackers is proposed for cerebral endovascular surgery. This system not only provides 2-D and 3-D visualization of medical images but also integrates an ICP-based algorithm to increase the registration accuracy during cerebral endovascular navigation. This tracking system integrates a mechanical digitizer, an optical tracker and an electromagnetic sensor to increase the flexibility in clinical application. An ICP-based registration algorithm is proposed to dynamically correct the registration error according to the current observation of the cerebral endovascular. To evaluate the system performance, a plastic phantom with cerebral endovascular structures is used and the experimental results show that the localization error is significantly reduced from 5.21∼6.47mm to 2.29∼2.87mm using the proposed scheme.},
  doi       = {10.1109/IEMBS.2008.4649902},
  issn      = {1094-687X},
  keywords  = {Biomedical imaging;Biomedical optical imaging;Error correction;Integrated optics;Mechanical sensors;Navigation;Optical sensors;Sensor systems and applications;Surgery;Visualization;ICP Algorithm;Image Registration;Navigation Surgery;Tracking System;Algorithms;Brain;Computer Graphics;Humans;Image Processing, Computer-Assisted;Imaging, Three-Dimensional;Models, Statistical;Phantoms, Imaging;Software;Surgery, Computer-Assisted;Tomography, X-Ray Computed;User-Computer Interface},
}

@InProceedings{4637348,
  author    = {C. Bichlmeier and B. Ockert and S. M. Heining and A. Ahmadi and N. Navab},
  title     = {Stepping into the operating theater: ARAV #x2014; Augmented Reality Aided Vertebroplasty},
  booktitle = {2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality},
  year      = {2008},
  pages     = {165-166},
  month     = {Sept},
  abstract  = {Augmented reality (AR) for preoperative diagnostics and planning, intra operative navigation and postoperative follow-up examination has been a topic of intensive research over the last two decades. However, clinical studies showing AR technology integrated into the real clinical environment and workflow are still rare. The incorporation of an AR system as a standard tool into the real clinical workflow has not been presented so far. This paper reports on the strategies and intermediate results of the ARAV - augmented reality aided vertebroplasty project that has been initiated to make an AR system based on a stereo video see-through head mounted display that is permanently available in the operating room (OR).},
  doi       = {10.1109/ISMAR.2008.4637348},
  keywords  = {augmented reality;medical computing;ARAV;augmented reality aided vertebroplasty;intraoperative navigation;operating room;postoperative follow-up examination;preoperative diagnostics;preoperative planning;Augmented reality;Biomedical imaging;Circuit faults;Computed tomography;Current transformers;Fault currents;Surgery;H.5.1 [Information Interfaces and Presentation]: Artificial, augmented, and virtual realities;J.3 [Life and Medical Sciences]},
}

@InProceedings{4538836,
  author    = {C. Bichlmeier and S. M. Heining and M. Rustaee and N. Navab},
  title     = {Laparoscopic Virtual Mirror for Understanding Vessel Structure Evaluation Study by Twelve Surgeons},
  booktitle = {2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality},
  year      = {2007},
  pages     = {125-128},
  month     = {Nov},
  abstract  = {In this paper we present the evaluation of a virtual mirror used as a navigational tool within a medical augmented reality (AR) system for laparoscopy. 12 surgeons of our clinical partner participated in an experiment to evaluate whether laparoscope augmentation extended by a virtual mirror is useful for improved perception of complex structures. Such complex structures are encountered for instance in laparoscopic resection of tumor affected liver tissue. The blood vessels supplying the tumor have to be cut and closed before tumorous tissue can be removed. A laparoscopic camera and an optical tracking system allow for the visualization of visualized medical volumetric data registered with the real anatomy. Previously injected contrast agent provides an accentuation of blood vessels within the visualization. For evaluating the suitability of a virtual mirror to support the mentioned procedure, we designed a phantom consisting of wooden branches simulating the structure of blood vessel trees. Quantitative results of the experiment show the advantage of a mirror in certain cases, when blood vessels cannot be directly seen from the camera point of view due to self-occlusion of the structure. Results of a questionnaire filled out by the surgeons after the experiments confirm the acceptance of AR technology for particular medical procedures.},
  doi       = {10.1109/ISMAR.2007.4538836},
  keywords  = {augmented reality;blood vessels;liver;medical computing;tumours;blood vessels;laparoscopic virtual mirror;laparoscopy;liver tissue;medical augmented reality system;medical volumetric data;navigational tool;tumor;vessel structure evaluation;Augmented reality;Biomedical optical imaging;Blood vessels;Cameras;Data visualization;Laparoscopes;Liver neoplasms;Mirrors;Navigation;Surgery;Augmented reality;medical visualization;navigated surgery;user interaction},
}

@InProceedings{4479948,
  author    = {T. Edmunds and D. K. Pai},
  title     = {Perceptual Rendering for Learning Haptic Skills},
  booktitle = {2008 Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems},
  year      = {2008},
  pages     = {225-230},
  month     = {March},
  abstract  = {We approach the problem of creating haptic simulators that effectively impart skill without requiring high-fidelity devices by identifying perceptually salient events that signal transitions in the interaction. By augmenting these events, we seek to overcome deficiencies in the fidelity of the rendering hardware. We present an extension of event-based haptic rendering to non- collision events, and we describe a user-study of the training effectiveness of passive force-field haptic simulation vs. active event- augmented simulation in a tool-manipulation task. The results indicate that active augmentation improves skill transfer without requiring an increase in the quality of the rendering device.},
  doi       = {10.1109/HAPTICS.2008.4479948},
  issn      = {2324-7347},
  keywords  = {computer graphic equipment;haptic interfaces;learning (artificial intelligence);manipulators;rendering (computer graphics);virtual reality;event-augmented simulation;event-based haptic rendering;haptic skill learning;noncollision events;passive force-field haptic simulation;perceptual rendering hardware;signal transitions;tool-manipulation task;training;Bones;Discrete event simulation;Feedback;Haptic interfaces;Hardware;Humans;Shape;Signal processing;Surgery;Surges},
}

@Article{4456758,
  author   = {O. Tonet and F. Focacci and M. Piccigallo and L. Mattei and C. Quaglia and G. Megali and B. Mazzolai and P. Dario},
  title    = {Bioinspired Robotic Dual-Camera System for High-Resolution Vision},
  journal  = {IEEE Transactions on Robotics},
  year     = {2008},
  volume   = {24},
  number   = {1},
  pages    = {55-64},
  month    = {Feb},
  issn     = {1552-3098},
  abstract = {Due to the limited resolution of both cameras and displays, acuity of artificial vision systems is currently well below the human eye. Visual acuity, in cameras as well as in animal eyes, can be increased by making smaller receptors or bigger eyes. In some applications, the size of the camera is constrained, so alternative solutions must be sought. This paper presents a robotic dual-camera vision system whose design is inspired by the visual system of jumping spiders (Salticidae family). The system is composed of a telephoto camera whose field of view (FOV) can be moved within the larger FOV of a wide-angle camera and allows to form a high-resolution image, i.e., an image with the FOV of the wide-angle camera, yet having the same resolution as the telephoto camera. We describe the design of the robotic system, the direct and inverse kinematics, and the image processing algorithms that allow to build the high-resolution image. Images from experiments are presented, together with a discussion on sources of errors and possible solutions. The system is particularly useful for fixed-camera monitoring or teleoperation applications, such as remote surveillance and minimally invasive surgery. The system achieves seven times higher resolution than typical commercial endoscopes.},
  doi      = {10.1109/TRO.2008.915430},
  keywords = {image resolution;robot vision;artificial vision systems;bioinspired robotic dual-camera system;high-resolution vision;image processing algorithms;inverse kinematics;jumping spiders visual system;robotic dual-camera vision system;robotic system design;telephoto camera;Animals;Cameras;Displays;Eyes;Humans;Image resolution;Machine vision;Minimally invasive surgery;Robot vision systems;Visual system;Augmented reality;panoramic stitching;robot vision;scanning mirror},
}

@Article{4360134,
  author   = {P. Lamata and E. J. Gomez and F. Lamata and A. Oltra Pastor and F. M. Sanchez-Margallo and F. del Pozo Guerrero},
  title    = {Understanding Perceptual Boundaries in Laparoscopic Surgery},
  journal  = {IEEE Transactions on Biomedical Engineering},
  year     = {2008},
  volume   = {55},
  number   = {3},
  pages    = {866-873},
  month    = {March},
  issn     = {0018-9294},
  abstract = {Human perceptual capabilities related to the laparoscopic interaction paradigm are not well known. Its study is important for the design of virtual reality simulators, and for the specification of augmented reality applications that overcome current limitations and provide a supersensing to the surgeon. As part of this work, this article addresses the study of laparoscopic pulling forces. Two definitions are proposed to focalize the problem: the perceptual fidelity boundary, limit of human perceptual capabilities, and the Utile fidelity boundary, that encapsulates the perceived aspects actually used by surgeons to guide an operation. The study is then aimed to define the perceptual fidelity boundary of laparoscopic pulling forces. This is approached with an experimental design in which surgeons assess the resistance against pulling of four different tissues, which are characterized with both in vivo interaction forces and ex vivo tissue biomechanical properties. A logarithmic law of tissue consistency perception is found comparing subjective valorizations with objective parameters. A model of this perception is developed identifying what the main parameters are: the grade of fixation of the organ, the tissue stiffness, the amount of tissue bitten, and the organ mass being pulled. These results are a clear requirement analysis for the force feedback algorithm of a virtual reality laparoscopic simulator. Finally, some discussion is raised about the suitability of augmented reality applications around this surgical gesture.},
  doi      = {10.1109/TBME.2007.908068},
  keywords = {augmented reality;biological tissues;biomechanics;endoscopes;force feedback;medical computing;surgery;perceptual fidelity boundary;utile fidelity boundary;augmented reality;ex vivo tissue biomechanical property;force feedback algorithm;human perceptual capability;laparoscopic pulling force;laparoscopic surgery;surgeons;surgical gesture;tissue consistency perception;tissue stiffness;virtual reality simulator;Algorithm design and analysis;Augmented reality;Design for experiments;Humans;Immune system;In vivo;Laparoscopes;Minimally invasive surgery;Surges;Virtual reality;Force feedback (FF);force feedback;human factors;laparoscopy;virtual reality (VR) simulation requirements;virtual reality simulation requirements;Computer Simulation;Computer-Aided Design;Differential Threshold;Elasticity;Equipment Design;Equipment Failure Analysis;Hardness;Humans;Laparoscopes;Laparoscopy;Models, Biological;Stress, Mechanical;Surgery, Computer-Assisted;Task Performance and Analysis;Touch},
}

@InProceedings{4414661,
  author    = {M. Furukawa and M. Ohta and S. Miyajima and M. Sugimoto and S. Hasegawa and M. Inami},
  title     = {Support System for Micro Operation using a Haptic Display Device},
  booktitle = {17th International Conference on Artificial Reality and Telexistence (ICAT 2007)},
  year      = {2007},
  pages     = {310-311},
  month     = {Nov},
  abstract  = {We developed SmartTool as a part of a study of augmented reality. In this paper, we introduce micro spidar. The design goal was to enable micro spidar to support micro operations at a millimeter-scale. We report a trial implementation showing the modality transformation of brightness information of the surface condition into a sensation of force feedback.},
  doi       = {10.1109/ICAT.2007.38},
  keywords  = {augmented reality;display devices;end effectors;force feedback;haptic interfaces;micromanipulators;MicroSpidar;SmartTool;augmented reality;end effector;force feedback;haptic display device;microoperation support system;modality transformation;Augmented reality;Brightness;Displays;Force feedback;Force measurement;Haptic interfaces;Instruments;Optical fibers;Optical sensors;Surgery},
}

@InProceedings{4414000,
  author    = {F. Tansel Halic and S. Sinan Kockara and T. Coskun Bayrak and F. Kamran Iqbal and F. Richard Rowe},
  title     = {Two-way semi-automatic registration in augmented reality system},
  booktitle = {2007 IEEE International Conference on Systems, Man and Cybernetics},
  year      = {2007},
  pages     = {3390-3395},
  month     = {Oct},
  abstract  = {Augmented reality (AR) system is a virtual environment that overlays the computer generated graphics on real-world view. A typical AR system consists of optical motion tracking and capturing. The main problem in such a system is minimizing error of mapping the movements of the actor in real domain to the virtual model. To do that, the position of markers attached to the actor needs to be carefully selected and mapped to the virtual counterpart. This one-to-one mapping process is called registration. In this regard, virtual and real domains should be aligned correctly. Any minute error in registration of these two domains results in degrading of the realism e.g. wrongly registered objects floating through each other. Current registration method for optical motion capture systems requires time-consuming and tedious manual processing. To overcome this problem, we present two-way (virtual object to real domain and real domain to virtual object) semi-automatic registration method. With our method instead of priory determining positions of markers on the physical object, we determine markers' positions (extreme points found by our application) on the virtual object and then locate markers at the approximate positions on the real object. By tracking the markers in the following step, we get markers' positions in real domain and change virtual domain's markers' positions accordingly to reduce error. Results show that our system solves the registration problem and prevents unrealistic jittering and flickering effects due to misalignment.},
  doi       = {10.1109/ICSMC.2007.4414000},
  issn      = {1062-922X},
  keywords  = {augmented reality;image motion analysis;image registration;augmented reality system;computer generated graphics;flickering effects;mapping process;optical motion capture systems;optical motion tracking;two-way semi-automatic registration;unrealistic jittering;virtual environment;Argon;Augmented reality;Biomedical imaging;Biomedical optical imaging;Computer displays;Head;Microscopy;Modeling;Orthopedic surgery;Tracking},
}

@InProceedings{4381711,
  author    = {J. Chen and S. Luo},
  title     = {An Improved Method for Computation of Fundamental Matrix},
  booktitle = {2007 IEEE/ICME International Conference on Complex Medical Engineering},
  year      = {2007},
  pages     = {151-156},
  month     = {May},
  abstract  = {In augmented reality, the real surgical scene information is required for three-dimensional reconstruction in order to merge real and virtual information. The estimation of the fundamental matrix is one of the key steps. Both the linear method and iterative least squares didn't impose the constraint that the rank of the fundamental matrix is two and the estimation results weren't accurate enough and nonlinear method needs an appropriate initial value. The MAPSAC algorithm was robust, but the Sampson errors of the results were still large. In this paper we proposed a MAPSACNL algorithm which integrates the MAPSAC algorithm and nonlinear method by using the results of MAPSAC as the initial value of the fundamental matrix and then optimizing it by Levenberg-Marquardt method. We compared the results of our methods and the results of linear method and the iterative least squares, which showed MAPSACNL method estimating the fundamental matrix was more robust and had less Sampson errors for the variance of the iterative numbers, the corner points numbers and the zeta value.},
  doi       = {10.1109/ICCME.2007.4381711},
  keywords  = {augmented reality;biomedical imaging;image reconstruction;least squares approximations;matrix algebra;medical computing;surgery;3D reconstruction;Levenberg-Marquardt method;MAPSAC algorithm;MAPSACNL algorithm;Sampson errors;augmented reality;fundamental matrix computation;iterative least squares;nonlinear method;real surgical scene information;Augmented reality;Biomedical imaging;Image reconstruction;Iterative algorithms;Iterative methods;Layout;Magnetic resonance imaging;Surgery;Surges;Visualization},
}

@InProceedings{4379441,
  author    = {J. d. Lee and T. y. Lan and L. c. Liu and S. T. Lee and C. T. Wu and B. Yang},
  title     = {A Remote Virtual-Surgery Training and Teaching System},
  booktitle = {2007 3DTV Conference},
  year      = {2007},
  pages     = {1-4},
  month     = {May},
  abstract  = {This paper describes a remote virtual-surgery training and teaching system using image processing and virtual reality technologies. The intern operator, the instructor, and auditors can stay in different rooms and exchange their information via Internet or Intranet. CT data is pre-download and registered to the space under an LCD monitor at the operator's site such that the operator can use a navigation tool as a virtual operation knife to practice a simulated operation in a virtual reality environment with the instructor's advices. The instructor monitors the view windows from the operator's site and provides suggestions via voice, text and paint functions to the operator's and auditors' sites. The auditors can use voice and text functions to ask the instructor questions. The head-pose technology is applied in the detection of the operator's gaze to adjust the display angle of CT data accordingly so that the 3D image display will be closed to a visual angle in a realistic operational viewpoint. This system has the potential for a further telemedicine application.},
  doi       = {10.1109/3DTV.2007.4379441},
  issn      = {2161-2021},
  keywords  = {biomedical education;computer based training;medical image processing;telemedicine;virtual reality;Internet;Intranet;image processing;remote virtual-surgery teaching system;remote virtual-surgery training system;telemedicine;virtual reality;Computed tomography;Education;Image processing;Internet;Navigation;Paints;Remote monitoring;Space technology;Three dimensional displays;Virtual reality;3D medical applications;NDI Polaris;Virtual-surgery;augmented reality;gaze},
}

@InProceedings{4352326,
  author    = {C. A. Linte and M. Wierzbicki and J. Moore and G. Guiraudon and D. L. Jones and T. M. Peters},
  title     = {On Enhancing Planning and Navigation of Beating-Heart Mitral Valve Surgery Using Pre-operative Cardiac Models},
  booktitle = {2007 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society},
  year      = {2007},
  pages     = {475-478},
  month     = {Aug},
  abstract  = {In an effort to reduce morbidity during minimally- invasive cardiac procedures, we have recently developed an interventional technique targeted towards off-pump cardiac interventions. To compensate for the absence of direct visualization, our system employs a virtual reality environment for image guidance, that integrates pre-operative information with real-time intra-operative imaging and surgical tool tracking. This work focuses on enhancing intracardiac visualization and navigation by overlaying pre-operative cardiac models onto the intra-operative virtual space, to display surgical targets within their specific anatomical context. Our method for integrating pre-operative data into the intra-operative environment is accurate within ~5.0 mm. Thus, we feel that our virtually-augmented surgical space is accurate enough to improve spatial orientation and intracardiac navigation.},
  doi       = {10.1109/IEMBS.2007.4352326},
  issn      = {1094-687X},
  keywords  = {biomedical imaging;cardiology;data visualisation;medical computing;surgery;virtual reality;beating heart mitral valve surgery;image guidance;interventional technique;intracardiac navigation;intracardiac visualization;preoperative cardiac models;real time intraoperative imaging;surgical tool tracking;virtual reality environment;Anatomy;Heart;Magnetic resonance imaging;Navigation;Real time systems;Surgery;Target tracking;Valves;Virtual reality;Visualization;image-guidance;minimally-invasive interventions;off-pump cardiac surgery;virtual reality environment;Cardiac Surgical Procedures;Humans;Imaging, Three-Dimensional;Magnetic Resonance Imaging;Mitral Valve;Models, Biological;Monitoring, Intraoperative;Preoperative Care},
}

@Article{4300845,
  author   = {A. P. Santhanam and F. G. Hamza-Lup and J. P. Rolland},
  title    = {Simulating 3-D Lung Dynamics Using a Programmable Graphics Processing Unit},
  journal  = {IEEE Transactions on Information Technology in Biomedicine},
  year     = {2007},
  volume   = {11},
  number   = {5},
  pages    = {497-506},
  month    = {Sept},
  issn     = {1089-7771},
  abstract = {Medical simulations of lung dynamics promise to be effective tools for teaching and training clinical and surgical procedures related to lungs. Their effectiveness may be greatly enhanced when visualized in an augmented reality (AR) environment. However, the computational requirements of AR environments limit the availability of the central processing unit (CPU) for the lung dynamics simulation for different breathing conditions. In this paper, we present a method for computing lung deformations in real time by taking advantage of the programmable graphics processing unit (GPU). This will save the CPU time for other AR-associated tasks such as tracking, communication, and interaction management. An approach for the simulations of the three-dimensional (3-D) lung dynamics using Green's formulation in the case of upright position is taken into consideration. We extend this approach to other orientations as well as the subsequent changes in breathing. Specifically, the proposed extension presents a computational optimization and its implementation in a GPU. Results show that the computational requirements for simulating the deformation of a 3-D lung model are significantly reduced for point-based rendering.},
  doi      = {10.1109/TITB.2006.889679},
  keywords = {Green's function methods;augmented reality;lung;medical computing;optimisation;pneumodynamics;rendering (computer graphics);3-D lung dynamics simulation;CPU;Green's formulation;augmented reality;central processing unit;lung deformation;medical simulation;point-based rendering;programmable graphics processing unit;Augmented reality;Central Processing Unit;Computational modeling;Deformable models;Education;Graphics;Lungs;Medical simulation;Surgery;Visualization;Augmented reality;Green's function;lung physiology;spherical harmonics;Computer Graphics;Computer Simulation;Computer Systems;Equipment Design;Equipment Failure Analysis;Humans;Imaging, Three-Dimensional;Lung;Models, Biological;Respiratory Mechanics;Signal Processing, Computer-Assisted},
}

@InProceedings{4193458,
  author    = {T. M. Peters and C. A. Linte and A. D. Wiles and N. Hill and J. Moore and C. Wedlake and D. Jones and D. Bainbridge and G. Guiraudon},
  title     = {DEVELOPMENT OF AN AUGMENTED REALITY APPROACH FOR CLOSED INTRACARDIAC INTERVENTIONS},
  booktitle = {2007 4th IEEE International Symposium on Biomedical Imaging: From Nano to Macro},
  year      = {2007},
  pages     = {1004-1007},
  month     = {April},
  abstract  = {Many intracardiac interventions are currently performed after the patient has been placed on cardiopulmonary bypass, and the heart has been arrested. We have developed a new method for operating on multiple targets inside the beating heart. In this procedure, the targets are accessed using a virtual reality (VR)-assisted image-guidance platform that combines real-time ultrasound (US) with a virtual model of the surgical instruments, in the context of the 3D cardiac anatomy. This paper presents preliminary results with respect to the surgeon's ability to accurately position and secure an artificial valve to a "valve orifice" within a cardiac phantom, when guidance is performed via US alone, and with US augmented by the VR environment. In addition, we discuss both the advantages and expected challenges this procedure might entail during its translation from the laboratory to the operating room},
  doi       = {10.1109/ISBI.2007.357024},
  issn      = {1945-7928},
  keywords  = {biomedical equipment;biomedical ultrasonics;cardiology;medical image processing;phantoms;prosthetics;surgery;virtual reality;artificial valve;augmented reality approach;beating heart;cardiac arrest;cardiac phantom;cardiac surgery;cardiopulmonary bypass;closed intracardiac interventions;heart;image-guidance platform;multiple target operation;real-time ultrasound;surgical instruments;three-dimensional cardiac anatomy;ultrasound augmentation;valve orifice;virtual model;virtual reality;Anatomy;Augmented reality;Cardiology;Context modeling;Heart;Surges;Surgical instruments;Ultrasonic imaging;Valves;Virtual reality},
}

@InProceedings{4161020,
  author    = {P. B. Persson and M. D. Cooper and L. A. E. Tibell and S. Ainsworth and A. Ynnerman and B. H. Jonsson},
  title     = {Designing and Evaluating a Haptic System for Biomolecular Education},
  booktitle = {2007 IEEE Virtual Reality Conference},
  year      = {2007},
  pages     = {171-178},
  month     = {March},
  abstract  = {In this paper we present an in situ evaluation of a haptic system, with a representative test population, we aim to determine what, if any, benefit haptics can have in a biomolecular education context. We have developed a haptic application for conveying concepts of molecular interactions, specifically in protein-ligand docking. Utilizing a semi-immersive environment with stereo graphics, users are able to manipulate the ligand and feel its interactions in the docking process. The evaluation used cognitive knowledge tests and interviews focused on learning gains. Compared with using time efficiency as the single quality measure this gives a better indication of a system's applicability in an educational environment. Surveys were used to gather opinions and suggestions for improvements. Students do gain from using the application in the learning process but the learning appears to be independent of the addition of haptic feedback. However the addition of force feedback did decrease time requirements and improved the students understanding of the docking process in terms of the forces involved, as is apparent from the students' descriptions of the experience. The students also indicated a number of features which could be improved in future development},
  doi       = {10.1109/VR.2007.352478},
  issn      = {1087-8270},
  keywords  = {biology;computer aided instruction;computer graphics;force feedback;haptic interfaces;molecular biophysics;stereo image processing;biomolecular education;educational environment;force feedback;haptic feedback;haptic system;semiimmersive environment;stereo graphics;Biology;Chemical technology;Computer science education;Educational technology;Force feedback;Haptic interfaces;Physics education;Protein engineering;Testing;Visualization;H.5.1 [Multimedia Information Systems]: Artificial augmented and virtual realities;Haptic Interaction;Haptics;J.3 [LIFE AND MEDICAL SCIENCES]: Biology and genetics;K3.1 [Computer Uses in Education]: Computer-assisted instruction;Life Science Education;Protein Interactions;Visualization;[H.3.4]: Systems and Software¿¿Performance evaluation (efficiency and effectiveness);[H5.2]: User Interfaces¿¿Haptic I/O;[J.2]: PHYSICAL SCIENCES AND ENGINEERING¿¿Chemistry},
}

@InProceedings{4110319,
  author    = {N. Xi and G. Li and D. H. Wang},
  title     = {In Situ Sensing and Manipulation in Nano Bio Systems},
  booktitle = {2006 IEEE International Symposium on MicroNanoMechanical and Human Science},
  year      = {2006},
  pages     = {1-6},
  month     = {Nov},
  abstract  = {The goal of this article is to review the state-of-art techniques for characterizing specific single receptor using the functionalized AFM (atomic force microscopy) tip and introduce the progress of in situ probing biomolecules as well as the possibility of single cell surgery by AFM. An example of studying the angiotensin II type 1 (ATI) receptors expressed in sensory neuronal cells by AFM with a functionalized tip is given. Perspectives for identifying and characterizing specific individual membrane proteins using AFM in living cells are provided},
  doi       = {10.1109/MHS.2006.320304},
  keywords  = {atomic force microscopy;molecular biophysics;nanobiotechnology;proteins;surgery;angiotensin II type 1 receptors;atomic force microscopy;in situ probing biomolecules;in situ sensing;living cells;membrane proteins;nano bio systems;sensory neuronal cells;single cell surgery;Atomic force microscopy;Augmented reality;Biomembranes;Force sensors;High-resolution imaging;Molecular biophysics;Nanobioscience;Proteins;Surface topography;Virtual reality},
}

@InProceedings{4053572,
  author    = {M. Cai and W. Tianmiao and C. Wusheng and Z. Yuru},
  title     = {A Neurosurgical Robotic System under Image-Guidance},
  booktitle = {2006 4th IEEE International Conference on Industrial Informatics},
  year      = {2006},
  pages     = {1245-1250},
  month     = {Aug},
  abstract  = {In this paper, a newly designed extrinsic-image-guided robotic system for frameless stereotactic neurosurgery in China is introduced. At first the procedure of stereotactic neurosurgery is summarized. Secondly the main structure of the system is addressed. Medical augmented reality is utilized for surgeon to choose best solution and to simulate the operation. Then the issues about telesurgery and safety are discussed. In the end, the system accuracy test and clinic experiment are presented.},
  doi       = {10.1109/INDIN.2006.275818},
  issn      = {1935-4576},
  keywords  = {augmented reality;medical robotics;surgery;extrinsic-image-guided robotic system;image-guidance;medical augmented reality;neurosurgical robotic system;stereotactic neurosurgery;Biomedical imaging;Focusing;Head;Medical robotics;Neurosurgery;Orbital robotics;Robot kinematics;Safety;Service robots;Surges},
}

@Article{1717785,
  author   = {H. Delingette and X. Pennec and L. Soler and J. Marescaux and N. Ayache},
  title    = {Computational Models for Image-Guided Robot-Assisted and Simulated Medical Interventions},
  journal  = {Proceedings of the IEEE},
  year     = {2006},
  volume   = {94},
  number   = {9},
  pages    = {1678-1688},
  month    = {Sept},
  issn     = {0018-9219},
  abstract = {Medical image analysis plays a crucial role in the diagnosis, planning, control, and follow-up of therapy. To be combined efficiently with medical robotics, medical image analysis can be supported by the development of specific computational models of the human body operating at various levels. We describe a hierarchy of these computational models, including the geometrical, physical, and physiological levels, and illustrate their potential use in a number of advanced medical applications including image-guided robot-assisted and simulated medical interventions. We conclude with scientific perspectives},
  doi      = {10.1109/JPROC.2006.880718},
  keywords = {augmented reality;medical computing;medical image processing;medical robotics;physiological models;surgery;anatomical models;augmented reality;biomechanics;brain shift;human body computational models;image-guided medical interventions;medical image analysis;medical robotics;physiological models;robot -assisted medical interventions;simulated medical interventions;surgery simulation;Biological system modeling;Biomedical imaging;Computational modeling;Humans;Image analysis;Medical diagnostic imaging;Medical robotics;Medical simulation;Medical treatment;Solid modeling;Anatomical models;augmented reality;biomechanics;brain shift;computational models;medical image analysis;medical robotics;physiological models;surgery simulation},
}

@InProceedings{1684526,
  author    = {D. Balazs and E. Attila},
  title     = {Volumetric Medical Intervention Aiding Augmented Reality Device},
  booktitle = {2006 2nd International Conference on Information Communication Technologies},
  year      = {2006},
  volume    = {1},
  pages     = {1091-1096},
  abstract  = {Nowadays medical imaging can be used to generate highly accurate 3D images of the interior of patients' bodies. 3D medical data must be presented in a compact way in order not to overwhelm or distract doctors who often work under strict time constraints. Augmented reality (AR) lets doctors to optimize actual procedures. Surgical decisions usually depend on the extent of the disease. The accurate volume measurement is complicated in most cases. For instance, surgeons can use AR technology to analyze the disease and determine its extent in three dimensional space so diagnosis can be faster and more precise},
  doi       = {10.1109/ICTTA.2006.1684526},
  keywords  = {augmented reality;medical image processing;patient diagnosis;surgery;3D medical data;accurate volume measurement;augmented reality device;computer aided medical diagnosis;computer vision-based tracking;gesture recognition;medical imaging;perceptual user interface;surgical decisions;volumetric medical intervention;Augmented reality;Biomedical imaging;Computed tomography;Data visualization;Diseases;Hardware;Information technology;Medical diagnostic imaging;Space technology;Surgery;augmented reality;computer aided medical diagnosis;computer vision-based tracking;direct manipulation;gesture recognition;perceptual user intenface},
}

@Article{1667978,
  author   = {O. Wongwirat and S. Ohara},
  title    = {Haptic media synchronization for remote surgery through simulation},
  journal  = {IEEE MultiMedia},
  year     = {2006},
  volume   = {13},
  number   = {3},
  pages    = {62-69},
  month    = {July},
  issn     = {1070-986X},
  abstract = {Network delay affects the operational instability of haptic displays for telehaptic remote surgery applications using simulation. A synchronization control mechanism for haptic media reduces network delay's effects and enhances telehaptic capability in these applications},
  doi      = {10.1109/MMUL.2006.54},
  keywords = {control engineering computing;haptic interfaces;medical control systems;surgery;synchronisation;telecontrol;telemedicine;haptic media synchronization;synchronization control;telehaptic remote surgery;Displays;Graphics;Haptic interfaces;Layout;Medical simulation;Multicast protocols;Servomechanisms;Surgery;Transport protocols;Virtual environment;and virtual and augmented reality;haptics;synchronization;telehaptics},
}

@Article{1580461,
  author   = {S. T. Clanton and D. C. Wang and V. S. Chib and Y. Matsuoka and G. D. Stetten},
  title    = {Optical merger of direct vision with virtual images for scaled teleoperation},
  journal  = {IEEE Transactions on Visualization and Computer Graphics},
  year     = {2006},
  volume   = {12},
  number   = {2},
  pages    = {277-285},
  month    = {March},
  issn     = {1077-2626},
  abstract = {Scaled teleoperation is increasingly prevalent in medicine, as well as in other applications of robotics. Visual feedback in such systems is essential and should make maximal use of natural hand-eye coordination. This paper describes a new method of visual feedback for scaled teleoperation in which the operator manipulates the handle of a remote tool in the presence of a registered virtual image of the target in real time. The method adapts a concept already used successfully in a new medical device called the sonic flashlight, which permits direct in situ visualization of ultrasound during invasive procedures. The sonic flashlight uses a flat-panel monitor and a half-silvered mirror to merge the visual outer surface of a patient with a simultaneous ultrasound scan of the patient's interior. Adapting the concept to scaled teleoperation involves removing the imaging device and the target to a remote location and adding a master-slave control device. This permits the operator to see his hands, along with what appears to be the tool, and the target, merged in a workspace that preserves natural hand-eye coordination. Three functioning prototypes are described, one based on ultrasound and two on light microscopy. The limitations and potential of the new approach are discussed.},
  doi      = {10.1109/TVCG.2006.35},
  keywords = {biomedical ultrasonics;data visualisation;medical computing;medical robotics;surgery;telerobotics;virtual reality;Sonic Flashlight;flat panel monitor;half-silvered mirror;in situ visualization;invasive procedures;light microscopy;master-slave control device;natural hand-eye coordination;robotics;scaled teleoperation;ultrasound imaging;virtual image;visual feedback;Biomedical imaging;Biomedical monitoring;Biomedical optical imaging;Corporate acquisitions;Medical robotics;Optical feedback;Patient monitoring;Robot kinematics;Ultrasonic imaging;Visualization;Artificial;and virtual realities;augmented;image display;medical information systems;real time.;Algorithms;Computer Graphics;Equipment Design;Equipment Failure Analysis;Humans;Image Enhancement;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Numerical Analysis, Computer-Assisted;Robotics;Signal Processing, Computer-Assisted;Subtraction Technique;Telemedicine;User-Computer Interface;Visual Perception},
}

@InProceedings{1544659,
  author    = {A. State and K. P. Keller and H. Fuchs},
  title     = {Simulation-based design and rapid prototyping of a parallax-free, orthoscopic video see-through head-mounted display},
  booktitle = {Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)},
  year      = {2005},
  pages     = {28-31},
  month     = {Oct},
  abstract  = {We built a video see-through head-mounted display with zero eye offset from commercial components and a mount fabricated via rapid prototyping. The orthoscopic HMD's layout was created and optimized with a software simulator. We describe simulator and HMD design, we show the HMD in use and demonstrate zero parallax.},
  doi       = {10.1109/ISMAR.2005.52},
  keywords  = {augmented reality;digital simulation;helmet mounted displays;medical image processing;software prototyping;surgery;video cameras;augmented reality;medical AR;minimally invasive surgery;orthoscopic HMD layout;parallax-free orthoscopic video see-through head-mounted display;rapid prototyping;simulation-based design;software simulator;zero eye offset;zero parallax;Avatars;Biomedical optical imaging;Cameras;Computational modeling;Displays;Eyes;Mirrors;Optical devices;Optical distortion;Virtual prototyping},
}

@Article{1525184,
  author   = {M. Figl and C. Ede and J. Hummel and F. Wanschitz and R. Ewers and H. Bergmann and W. Birkfellner},
  title    = {A fully automated calibration method for an optical see-through head-mounted operating microscope with variable zoom and focus},
  journal  = {IEEE Transactions on Medical Imaging},
  year     = {2005},
  volume   = {24},
  number   = {11},
  pages    = {1492-1499},
  month    = {Nov},
  issn     = {0278-0062},
  abstract = {Ever since the development of the first applications in image-guided therapy (IGT), the use of head-mounted displays (HMDs) was considered an important extension of existing IGT technologies. Several approaches to utilizing HMDs and modified medical devices for augmented reality (AR) visualization were implemented. These approaches include video-see through systems, semitransparent mirrors, modified endoscopes, and modified operating microscopes. Common to all these devices is the fact that a precise calibration between the display and three-dimensional coordinates in the patient's frame of reference is compulsory. In optical see-through devices based on complex optical systems such as operating microscopes or operating binoculars-as in the case of the system presented in this paper-this procedure can become increasingly difficult since precise camera calibration for every focus and zoom position is required. We present a method for fully automatic calibration of the operating binocular Varioscope™ M5 AR for the full range of zoom and focus settings available. Our method uses a special calibration pattern, a linear guide driven by a stepping motor, and special calibration software. The overlay error in the calibration plane was found to be 0.14-0.91 mm, which is less than 1% of the field of view. Using the motorized calibration rig as presented in the paper, we were also able to assess the dynamic latency when viewing augmentation graphics on a mobile target; spatial displacement due to latency was found to be in the range of 1.1-2.8 mm maximum, the disparity between the true object and its computed overlay represented latency of 0.1 s. We conclude that the automatic calibration method presented in this paper is sufficient in terms of accuracy and time requirements for standard uses of optical see-through systems in a clinical environment.},
  doi      = {10.1109/TMI.2005.856746},
  keywords = {augmented reality;biomedical optical imaging;calibration;patient treatment;augmentation graphics;augmented reality visualization;binocular Varioscope MS AR;fully automated calibration;image-guided therapy;medical devices;modified endoscopes;optical see-through head-mounted operating microscope;semitransparent mirrors;video-see through systems;Augmented reality;Biomedical imaging;Biomedical optical imaging;Calibration;Delay;Displays;Focusing;Medical treatment;Optical devices;Optical microscopy;Augmented reality;computer-aided surgery;head-mounted display;Algorithms;Calibration;Equipment Design;Equipment Failure Analysis;Head Protective Devices;Humans;Image Enhancement;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Microscopy;Microsurgery;Optics;Reproducibility of Results;Sensitivity and Specificity;Surgery, Computer-Assisted;User-Computer Interface},
}

@Article{1525186,
  author   = {S. De Buck and F. Maes and J. Ector and J. Bogaert and S. Dymarkowski and H. Heidbuchel and P. Suetens},
  title    = {An augmented reality system for patient-specific guidance of cardiac catheter ablation procedures},
  journal  = {IEEE Transactions on Medical Imaging},
  year     = {2005},
  volume   = {24},
  number   = {11},
  pages    = {1512-1524},
  month    = {Nov},
  issn     = {0278-0062},
  abstract = {We present a system to assist in the treatment of cardiac arrhythmias by catheter ablation. A patient-specific three-dimensional (3-D) anatomical model, constructed from magnetic resonance images, is merged with fluoroscopic images in an augmented reality environment that enables the transfer of electrocardiography (ECG) measurements and cardiac activation times onto the model. Accurate mapping is realized through the combination of: a new calibration technique, adapted to catheter guided treatments; a visual matching registration technique, allowing the electrophysiologist to align the model with contrast-enhanced images; and the use of virtual catheters, which enable the annotation of multiple ECG measurements on the model. These annotations can be visualized by color coding on the patient model. We provide an accuracy analysis of each of these components independently. Based on simulation and experiments, we determined a segmentation error of 0.6 mm, a calibration error in the order of 1 mm and a target registration error of 1.04 ± 0.45 mm. The system provides a 3-D visualization of the cardiac activation pattern which may facilitate and improve diagnosis and treatment of the arrhythmia. Because of its low cost and similar advantages we believe our approach can compete with existing commercial solutions, which rely on dedicated hardware and costly catheters. We provide qualitative results of the first clinical use of the system in 11 ablation procedures.},
  doi      = {10.1109/TMI.2005.857661},
  keywords = {augmented reality;biomedical MRI;calibration;catheters;diagnostic radiography;electrocardiography;image matching;image registration;image segmentation;medical image processing;patient treatment;arrhythmia;augmented reality system;calibration;cardiac activation pattern;cardiac arrhythmias;cardiac catheter ablation;catheter guided treatments;color coding;electrocardiography;electrophysiology;fluoroscopic images;image segmentation;magnetic resonance images;patient-specific guidance;visual matching registration;Augmented reality;Calibration;Catheters;Costs;Electrocardiography;Electrophysiology;Image segmentation;Independent component analysis;Magnetic resonance;Visualization;Arrhythmia;augmented reality;cardiac ablation;Algorithms;Arrhythmias, Cardiac;Artificial Intelligence;Body Surface Potential Mapping;Catheter Ablation;Humans;Image Enhancement;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Magnetic Resonance Imaging;Models, Cardiovascular;Reproducibility of Results;Sensitivity and Specificity;Subtraction Technique;Surgery, Computer-Assisted;User-Computer Interface},
}

@Article{1463330,
  author   = {G. Fichtinger and A. Deguet and K. Masamune and E. Balogh and G. S. Fischer and H. Mathieu and R. H. Taylor and S. J. Zinreich and L. M. Fayad},
  title    = {Image overlay guidance for needle insertion in CT scanner},
  journal  = {IEEE Transactions on Biomedical Engineering},
  year     = {2005},
  volume   = {52},
  number   = {8},
  pages    = {1415-1424},
  month    = {Aug},
  issn     = {0018-9294},
  abstract = {We present an image overlay system to aid needle insertion procedures in computed tomography (CT) scanners. The device consists of a display and a semitransparent mirror that is mounted on the gantry. Looking at the patient through the mirror, the CT image appears to be floating inside the patient with correct size and position, thereby providing the physician with two-dimensional (2-D) "X-ray vision" to guide needle insertions. The physician inserts the needle following the optimal path identified in the CT image rendered on the display and, thus, reflected in the mirror. The system promises to reduce X-ray dose, patient discomfort, and procedure time by significantly reducing faulty insertion attempts. It may also increase needle placement accuracy. We report the design and implementation of the image overlay system followed by the results of phantom and cadaver experiments in several clinical applications.},
  doi      = {10.1109/TBME.2005.851493},
  keywords = {computerised tomography;phantoms;cadaver experiments;computed tomography scanner;image overlay guidance;needle insertion;phantom;two-dimensional X-ray vision;Cadaver;Computed tomography;Computer displays;Imaging phantoms;Layout;Mirrors;Needles;Rendering (computer graphics);Two dimensional displays;X-ray imaging;Augmented reality, computed tomography, image guidance, image overlay, needle insertion;Biopsy;Cadaver;Data Display;Equipment Design;Equipment Failure Analysis;Humans;Needles;Phantoms, Imaging;Radiographic Image Interpretation, Computer-Assisted;Surgery, Computer-Assisted;Tomography, X-Ray Computed;User-Computer Interface},
}

@InProceedings{1438891,
  author    = {M. I. Kassim and Wu Ruoyun and Shao Fan and Ng Wan Sing and Wee Siew Bock},
  title     = {Tracked arm manipulator for lumpectomy},
  booktitle = {IEEE Conference on Robotics, Automation and Mechatronics, 2004.},
  year      = {2004},
  volume    = {1},
  pages     = {55-59 vol.1},
  month     = {Dec},
  abstract  = {In this paper, we present a tracked and semimotorized arm manipulator system aiming to provide accurate positioning information for breast biopsy tool to be used as a treatment device. The system is integrated with augmented reality (AR) and visual reality (VR) visualization. Two tracking systems were used on the arm, camera tracking and encoder tracking to simultaneously track the ultrasound probe and surgical tool. Stereo-camera is used to track an optical marker, which is temporary mounted. The marker is removed once the initial location of the arms is computed and encoder-based tracking takes over. On each passive joint of the arm system, a rotary encoder is installed which tells the angle of each joint. Therefore the positions of both ultrasound probe and surgical tool are known. The surgical tool is fitted onto a motorized 2 degree of freedom (DOF) with motorized cutting sequence, which enables cutting planning, probe positioning and controlled tissue cutting procedure. The prototype of the system proved the concept and future work is expected to improve the accuracy.},
  doi       = {10.1109/RAMECH.2004.1438891},
  keywords  = {augmented reality;cancer;manipulators;medical robotics;position control;surgery;accurate positioning information;augmented reality visualization;breast biopsy tool;lumpectomy;semimotorized arm manipulator system;surgical tool;tracked arm manipulator;visual reality visualization;Arm;Augmented reality;Biomedical optical imaging;Breast biopsy;Cameras;Probes;Surgery;Ultrasonic imaging;Virtual reality;Visualization},
}

@InProceedings{1406965,
  author    = {P. Rizun and G. Sutherland},
  title     = {Tactile feedback laser system with applications to robotic surgery},
  booktitle = {First Joint Eurohaptics Conference and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems. World Haptics Conference},
  year      = {2005},
  pages     = {426-431},
  month     = {March},
  abstract  = {Despite the potential advantages of lasers, their adoption by surgeons has been limited. Unlike hand tools, lasers do not provide haptic feedback. With advances in augmented reality, haptics, and surgical robotics, it may now be possible to add the sense of touch to non-contact surgical lasers. This manuscript presents our initial work towards developing such a tactile feedback laser system. Two prototypes were constructed that allow an operator to feel surfaces using only light but are not yet equipped with lasers powerful enough to cut. Based on optical distance measurements, the prototypes synthesize haptic feedback through a robotic arm held by the operator when the focal point of the laser is coincident with a real surface, giving the operator the impression of touching something solid. To the best of our knowledge, no other device has been built that provides the sense of touch with a laser.},
  doi       = {10.1109/WHC.2005.123},
  keywords  = {augmented reality;force feedback;haptic interfaces;laser applications in medicine;medical computing;medical robotics;surgery;touch (physiological);augmented reality;haptic feedback;noncontact surgical lasers;robotic surgery;tactile feedback laser system;Augmented reality;Haptic interfaces;Laser applications;Laser beam cutting;Laser feedback;Laser surgery;Optical feedback;Prototypes;Robot sensing systems;Surface emitting lasers},
}

@InProceedings{1366185,
  author    = {W. Qi},
  title     = {A prototype of video see-through mixed reality interactive system},
  booktitle = {2004 International Conference on Cyberworlds},
  year      = {2004},
  pages     = {274-277},
  month     = {Nov},
  abstract  = {Mixed reality (MR), sometimes called enhanced reality, is a variety of virtual environment (VE) which explores various natural environments with immersive display technologies. VE technologies immerse a user completely inside a synthetic environment. By contrast, MR systems add electronic data from cyberspace onto physical space, allowing users to see the real world with virtual objects superimposed upon it. Moreover, MR can assist the users' interaction with the virtual object through the real environment. The objective of our research is to investigate the potential of MR techniques for improving human and computer interaction with scientific data through developing a video-see through MR system. Further applications will be built upon this generic platform.},
  doi       = {10.1109/CW.2004.11},
  keywords  = {augmented reality;human computer interaction;cyberspace;display technology;enhanced reality;human-computer interaction;scientific data;user interaction;video see-through mixed reality interactive system;Cameras;Displays;Firewire;Force feedback;Interactive systems;Minimally invasive surgery;Prototypes;Sensor systems;Space technology;Virtual reality},
}

@InProceedings{1364719,
  author    = {T. Starner and D. Ashbrook},
  title     = {Augmenting a pH medical study with wearable video for treatment of GERD},
  booktitle = {Eighth International Symposium on Wearable Computers},
  year      = {2004},
  volume    = {1},
  pages     = {194-195},
  month     = {Oct},
  abstract  = {In this paper we present an augmentation to the wearable computers typically used to determine if a patient is a candidate for surgery to correct problems associated with Gastroesophageal Reflux disease (GERD). A wearable camera was used by the first author while participating in a 24-hour stomach acid pH study. After the study's conclusion, an examination of the captured video and pH record revealed some results that allowed the first author to avoid many of the activities that result in symptoms related to GERD.},
  doi       = {10.1109/ISWC.2004.9},
  issn      = {1530-0811},
  keywords  = {augmented reality;diseases;health care;surgery;wearable computers;Gastroesophageal Reflux disease;pH medical study;wearable camera;wearable computers;wearable video;Biomedical monitoring;Cameras;Esophagus;Medical treatment;Nose;Oncological surgery;Probes;Stomach;Wearable computers;Wearable sensors},
}

@InProceedings{1317350,
  author    = {W. L. Nowinski},
  title     = {Virtual reality in brain intervention},
  booktitle = {Proceedings. Fourth IEEE Symposium on Bioinformatics and Bioengineering},
  year      = {2004},
  pages     = {245-248},
  month     = {May},
  abstract  = {Efficient tools to visualize, browse and quantify patient-specific data as well as more user-friendly and powerful ways of interacting with these by virtual reality (VR) are elaborated. VR applications for brain intervention have been developed and are discussed in the paper.},
  doi       = {10.1109/BIBE.2004.1317350},
  file      = {:ref_downloads/1317350 - Virtual reality in brain intervention.pdf:PDF},
  keywords  = {brain;data visualisation;human computer interaction;medical computing;medical information systems;neurophysiology;surgery;user interfaces;virtual reality;brain intervention;data browsing;data visualization;patient-specific data;user-friendly interaction;virtual reality;Augmented reality;Biological system modeling;Biomedical imaging;Computational modeling;Humans;Medical diagnostic imaging;Medical simulation;Pathology;Surgery;Virtual reality},
}

@InProceedings{1310095,
  author    = {A. P. Santhanam and C. Fidopiastis and B. Hoffman-Ruddy and J. P. Rolland},
  title     = {PRASAD: an augmented reality based non-invasive pre-operative visualization framework for lungs},
  booktitle = {IEEE Virtual Reality 2004},
  year      = {2004},
  pages     = {253-254},
  month     = {March},
  abstract  = {This paper presents a preoperative anatomical visualization framework, PRASAD (physically realistic adaptive and scalable anatomical deformation system), which combines a bio-mathematical representation of deformable lungs with real-time deformation and stereoscopic visualization technology. This framework provides a visualization of a dynamic patient-specific deformation of synthetic 3D anatomical models, that physicians can view from different viewpoints in a stereoscopic augmented reality environment for efficient diagnosis.},
  doi       = {10.1109/VR.2004.1310095},
  issn      = {1087-8270},
  keywords  = {augmented reality;data visualisation;deformation;lung;medical diagnostic computing;physiological models;3D anatomical models;PRASAD;anatomical deformation;anatomical visualization;augmented reality;bio-mathematical representation;lungs;noninvasive visualization;preoperative visualization;stereoscopic visualization;Augmented reality;Battery charge measurement;Biomedical imaging;Capacitive sensors;Deformable models;Lungs;Medical diagnostic imaging;Physiology;Surgery;Visualization},
}

@InProceedings{1198168,
  author    = {C. Yasuba and S. Kobashi and K. Kondo and Y. Hata and S. Imawaki and M. Ishikawa},
  title     = {Fuzzy inference based augmented reality in MR cholangiopancreatography images},
  booktitle = {Neural Information Processing, 2002. ICONIP '02. Proceedings of the 9th International Conference on},
  year      = {2002},
  volume    = {2},
  pages     = {796-800 vol.2},
  month     = {Nov},
  abstract  = {Augmented reality (AR) is a combination of a real scene viewed and a virtual scene generated by a computer. This paper introduces fuzzy inference techniques into AR. The fuzzy inference based AR enhances a region of interest (ROI) in medical images using expert knowledge expressed with fuzzy if-then rules. Therefore, the AR enables users who are not familiar with such medical images to observe the ROIs as well as expert's observation. Especially this paper discusses the fuzzy inference based AR in MR Cholangiopancreatography images. The AR enhances tube-formed tissues such as the pancreatic duct. In this case, a step for finding the tissue using fuzzy inference is included. We apply the proposed the fuzzy inference based AR to three subjects. Through experimental results, we showed the pancreatic duct was successfully found from any subjects and could clearly augment the 3D shape of the pancreatic duct.},
  doi       = {10.1109/ICONIP.2002.1198168},
  keywords  = {augmented reality;biomedical MRI;fuzzy logic;image recognition;inference mechanisms;medical image processing;MR Cholangiopancreatography images;MRCP images;MRI;augmented reality;fuzzy inference;fuzzy rules;image recognition;region of interest;Augmented reality;Biomedical imaging;Ducts;Hospitals;Image recognition;Layout;Pancreas;Shape;Surgery;Ultrasonic imaging},
}

@InProceedings{1019760,
  author    = {J. M. Rosen and M. K. Simpson},
  title     = {Cybercare: combining healthcare and cyberspace in the 21st century},
  booktitle = {2001 Conference Proceedings of the 23rd Annual International Conference of the IEEE Engineering in Medicine and Biology Society},
  year      = {2001},
  volume    = {4},
  pages     = {4108-4112 vol.4},
  abstract  = {We discuss the origins of the modern hospital-based tertiary care system. We define how this system was a product of a large-scale medical disaster that prompted the invention of the modern hospital. We will then similarly describe the disaster that prompted the beginning of telemedicine. Following, we review the history of telemedicine and the development of telesurgery and present the state-of-the-art in telesurgery and other advanced telemedicine technologies. We will then define cyber through a discussion of cybernetics and cyberspace. We present the concept of the digital physician and a cybercare vision of a new healthcare system. We will then predict what type of large-scale medical disaster would prompt the creation of a cybercare healthcare system. Finally, we discuss the challenges to be faced in the 21st century.},
  doi       = {10.1109/IEMBS.2001.1019760},
  issn      = {1094-687X},
  keywords  = {cybernetics;health care;history;information technology;medical information systems;medical robotics;surgery;technological forecasting;telemedicine;telerobotics;virtual reality;21st century;augmented reality;casualty data cubes;challenges;clinical information systems;computer patient records;cybercare;cybernetics;cyberspace;datafusion;digital physician;distance learning systems;healthcare;healthcare system;history;information technology;large-scale medical disaster;modern hospital-based tertiary care system;review;robots;software intelligent agents;state-of-the-art;telemedicine;telementoring;telesurgery;virtual reality;Airports;Biomedical engineering;Cybercare;History;Hospitals;Large-scale systems;Medical services;Modems;Space technology;Telemedicine},
}

@InProceedings{1029283,
  author    = {G. Stetten and D. Shelton and W. Chang and V. Chib and R. Tamburo and D. Hildebrand and L. Lobes and J. Sumkin},
  title     = {Towards a clinically useful sonic flashlight},
  booktitle = {Proceedings IEEE International Symposium on Biomedical Imaging},
  year      = {2002},
  pages     = {417-420},
  abstract  = {We have previously shown a new method of merging a direct view of the patient with an ultrasound image displayed in situ within the patient, using a half-silvered mirror. We call this method Real Time Tomographic Reflection (RTTR). This paper reviews our progress to date in developing an embodiment of RTTR that we call the sonic flashlight™. The clinical utility of the sonic flashlight for guiding invasive procedures will depend on a number of factors, and we have explored these factors through a series of prototypes. Responding to feedback from our clinical collaborators, we have upgraded various elements of our original apparatus and implemented a new generation of the system that is smaller, lighter, and more easily manipulated. We have improved performance as we gain a better understanding of the optical parameters of the system. Our results demonstrate in situ visualization of vasculature of the neck in a human volunteer and the anatomy of the eye in a cadaver.},
  doi       = {10.1109/ISBI.2002.1029283},
  keywords  = {augmented reality;biomedical transducers;biomedical ultrasonics;computerised tomography;eye;real-time systems;surgery;ultrasonic transducers;cadaver;clinical collaborators;clinically useful sonic flashlight;direct view;eye anatomy;feedback;half-silvered mirror;human volunteer;in situ visualization;invasive procedure guidance;neck;optical parameters;pencil grip prototype;percutaneous ultrasound-guided intervention;performance;pistol grip prototype;real time tomographic reflection;surgery;ultrasound image;ultrasound transducer;vasculature;virtual image;Acoustic reflection;Collaboration;Merging;Mirrors;Optical feedback;Optical reflection;Performance gain;Prototypes;Tomography;Ultrasonic imaging},
}

@InProceedings{996506,
  author    = {T. Nojima and D. Sekiguchi and M. Inami and S. Tachi},
  title     = {The SmartTool: a system for augmented reality of haptics},
  booktitle = {Proceedings IEEE Virtual Reality 2002},
  year      = {2002},
  pages     = {67-72},
  abstract  = {Previous research on augmented reality has been mainly focused on augmentation of visual or acoustic information. However, humans can receive information not only through vision and acoustics, but also through haptics. Haptic sensation is very intuitive, and some researchers are focusing on making use of haptics in augmented reality systems. While most previous research on haptics is based on static data, such as that generated from CAD, CT, and so on, these systems have difficulty responding to a changing real environment in real time. In this paper, we propose a new concept for the augmented reality of haptics, the SmartTool. The SmartTool responds to the real environment by using real time sensor(s) and a haptic display. The sensor(s) on the SmartTool measure the real environment then send us that information through haptic sensation. Furthermore, we will describe the prototype system we have developed},
  doi       = {10.1109/VR.2002.996506},
  issn      = {1087-8270},
  keywords  = {augmented reality;haptic interfaces;SmartTool;augmented reality;haptic display;haptic sensation;haptics;prototype system;real environment;time sensor;Acoustics;Augmented reality;Cameras;Displays;Haptic interfaces;Humans;Machine vision;Navigation;Real time systems;Surgery},
}

@InProceedings{930305,
  author    = {Lisheng Wang and Tien-Tsin Wong and Pheng Ann Heng and J. C. Y. Cheng},
  title     = {Template-matching approach to edge detection of volume data},
  booktitle = {Proceedings International Workshop on Medical Imaging and Augmented Reality},
  year      = {2001},
  pages     = {286-291},
  abstract  = {This paper proposes a template-matching approach to the edge detection of volume data. Twenty-six templates of an ideal step-like edge in the 3×3×3 neighborhood of volume data are given, and the step-like edge of volume data is detected by matching such patterns in various orientations. The approach is a simple and straightforward one for edge detection of volume data. It generalizes the well-known Kirsch operator for 2D images. It can detect change of intensity in every direction, and has the property of rotation invariance in 18-neighborhood. Implementation of proposed approach is given for biological and medical volume data, including MRI and CT volume data},
  doi       = {10.1109/MIAR.2001.930305},
  keywords  = {edge detection;image matching;medical image processing;2D images;CT;Kirsch operator;MRI;biological data;edge detection;medical image processing;medical volume data;pattern matching;rotation invariance;template matching;volume data;Biomedical imaging;Computed tomography;Computer science;Data engineering;Detectors;Image edge detection;Lifting equipment;Magnetic resonance imaging;Orthopedic surgery;Sea surface},
}

@InProceedings{930307,
  author    = {Jianyun Chai and Jian Sun and Zesheng Tang},
  title     = {Hybrid FEM for deformation of soft tissues in surgery simulation},
  booktitle = {Proceedings International Workshop on Medical Imaging and Augmented Reality},
  year      = {2001},
  pages     = {298-303},
  abstract  = {Surgery simulation has been found very useful in surgery planning and training. In order to obtain a physically realistic surgery simulation, it is needed to treat the deformation of soft tissues with nonlinear elastic properties and changeable topologies in real time. Most of the real-time approaches to soft tissue deformation used so far, however, have less functions. In this paper, an improved algorithm, called Hybrid FEM, is proposed to include the two vital factors into a unified framework, which generalizes the use of FEM in surgery simulation. Since the effects of nonlinear properties and topology changes on the deformation of soft tissues often appear locally around the operating positions, only the model of FEM for that limited region should be updated as the operation is performed; while the model for other regions can be kept constant. The Hybrid FEM algorithm has been implemented and examples are given to show the validity of the algorithm},
  doi       = {10.1109/MIAR.2001.930307},
  keywords  = {digital simulation;finite element analysis;medical computing;real-time systems;surgery;Hybrid FEM algorithm;finite element method;nonlinear elastic properties;real time system;soft tissue deformation;surgery planning;surgery simulation;topology;training;Acceleration;Biological tissues;Computational modeling;Computer science;Computer simulation;Deformable models;Medical simulation;Solid modeling;Surgery;Topology},
}

@InProceedings{930304,
  author    = {T. Hayasaka and Hao Liu and R. Himeno and T. Yamaguchi},
  title     = {A MRI based semi-automatic modeling system for computational biomechanics simulation},
  booktitle = {Proceedings International Workshop on Medical Imaging and Augmented Reality},
  year      = {2001},
  pages     = {282-285},
  abstract  = {Computational biomechanics has been recognized as a powerful tool for cardiovascular research, vascular surgery planning and medical device design. An accurate and efficient construction of anatomic models is a critical element in the application of these computational methods. We developed an interactive modeling system. The novel features of this system were real time volume rendering and multiresolution mesh editing. Using these capabilities, a medical expert can become able to efficiently make accurate models using one's anatomical knowledge and heuristic ability, without requiring a background in intensive computation},
  doi       = {10.1109/MIAR.2001.930304},
  keywords  = {biomechanics;biomedical MRI;medical image processing;real-time systems;rendering (computer graphics);solid modelling;MRI;anatomic models;cardiovascular research;computational biomechanics simulation;interactive modeling system;magnetic resonance imaging;medical device design;medical image processing;multiresolution mesh editing;real time volume rendering;semi-automatic modeling system;vascular surgery planning;Biomechanics;Biomedical imaging;Cardiology;Chemical technology;Computational modeling;Image databases;Magnetic resonance imaging;Solid modeling;Spatial databases;Surgery},
}

@InProceedings{930274,
  author    = {M. A. F. Rodrigues and D. F. Gillies and P. Charters},
  title     = {Realistic deformable models for simulating the tongue during laryngoscopy},
  booktitle = {Proceedings International Workshop on Medical Imaging and Augmented Reality},
  year      = {2001},
  pages     = {125-130},
  abstract  = {During the procedure of laryngoscopy, an anaesthetist uses a rigid blade to displace and compress the tongue of the patient, and then inserts a tube into the larynx to allow controlled ventilation of the lungs during an operation. This procedure can sometimes be difficult and even life threatening, and there is therefore a need for regular training. Currently, plastic models are used for this purpose, and these have many disadvantages. Computer simulation is an attractive alternative, however, for proper realism it is necessary to build a model of the upper airway. In particular, we need a deformable model that can realistically simulate the behaviour of the tongue as it is compressed by the blade. We start from medical images, extract the details that characterise the subject and then incorporate these in a finite element model to investigate how the tongue tissue behaves in response to the insertion of the blade, when it is subjected to a variety of loading conditions. The results show that, within a specific set of tongue material parameters, the simulated outcome can be successfully related to the experimental laryngoscopic studies. Further research is underway to apply these results in a virtual reality simulation for laryngoscopic training. One main problem to be solved is computing the deformations in real time},
  doi       = {10.1109/MIAR.2001.930274},
  keywords  = {computer based training;digital simulation;finite element analysis;medical image processing;realistic images;surgery;computer based training;computer simulation;deformable model;finite element model;laryngoscopy;lung ventilation;medical images;real time;realistic deformable models;tongue;upper airway;virtual reality;Blades;Computational modeling;Computer simulation;Deformable models;Displacement control;Larynx;Lungs;Plastics;Tongue;Ventilation},
}

@InProceedings{930257,
  author    = {L. K. Cheung and M. C. M. Wong and L. L. S. Wong},
  title     = {The applications of stereolithography in facial reconstructive surgery},
  booktitle = {Proceedings International Workshop on Medical Imaging and Augmented Reality},
  year      = {2001},
  pages     = {10-15},
  abstract  = {The development of rapid prototyping has evolved from the crude milled models to the laser polymerised stereolithographic models of excellent accuracy. The technology was advanced further with the recent introduction of fused deposition modelling and the 3-dimensional ink-jet printing technique in stereo-model fabrication. The concept of using a 3-dimensional model in planning the operation has amazed the maxillofacial surgeons since its first application in grafting a skull defect in 1995. It was followed by many bright ideas of applications in the field of facial reconstructive surgery. The stereo-model may assist in the diagnosis of facial fractures, joint ankylosis and even impacted teeth. The surgery can be simulated prior to the operation of complex craniofacial syndromes, facial asymmetry and distraction osteogenesis. The stereo-model can be used for preparation of reconstructive plate or joint prosthesis. It has an enormous value as an educational teaching and patient information tool for obtaining the consent for surgery. The aims of the paper are to present the modern manufacturing methods of the stereo-model and to illustrate the clinical applications of the stereomodel in facial reconstruction},
  doi       = {10.1109/MIAR.2001.930257},
  keywords  = {digital simulation;medical computing;rapid prototyping (industrial);solid modelling;surgery;3D model;facial reconstructive surgery;fused deposition modelling;manufacturing methods;maxillofacial surgeons;medical simulation;operation planning;patient information tool;rapid prototyping;stereo-model fabrication;stereolithography;teaching;three dimensional ink-jet printing;Ink jet printing;Laser modes;Laser surgery;Optical device fabrication;Polymers;Prosthetics;Prototypes;Skull;Stereolithography;Teeth},
}

@InProceedings{958489,
  author    = {J. P. Helferty and A. J. Sherbondy and A. P. Kiraly and J. Z. Turlington and E. A. Hoffman and G. McLennan and W. E. Higgins},
  title     = {Image-guided endoscopy for lung-cancer assessment},
  booktitle = {Proceedings 2001 International Conference on Image Processing (Cat. No.01CH37205)},
  year      = {2001},
  volume    = {2},
  pages     = {307-310 vol.2},
  month     = {Oct},
  abstract  = {A system for virtual-endoscopic 3D medical image assessment and follow-on live endoscopy is described. The development and results focus on 3D CT images of the chest and lung-cancer assessment. Two stages are involved in the assessment. In Stage-1, the physician uses a series of tools for visualization and 3D central-axes analysis to build a guidance plan (a "case study"). Next, during Stage-2 endoscopy, the physician links the computer system and case study to the endoscope. The endoscopic video is registered to 3D virtual image data to give the physician "augmented reality" information to better perform the endoscopy. Results illustrate the system's utility for animal and human cases},
  doi       = {10.1109/ICIP.2001.958489},
  keywords  = {augmented reality;cancer;computerised tomography;data visualisation;image registration;lung;medical image processing;video signal processing;3D medical image;3D virtual image data;CT images;augmented reality;central axes analysis;chest;endoscopic video registration;follow-on live endoscopy;guidance plan;image-guided endoscopy;lung-cancer assessment;virtual-endoscopic assessment;visualization;Biopsy;Cancer;Computed tomography;Endoscopes;Hardware;Image processing;Lungs;Lymph nodes;Minimally invasive surgery;Needles},
}

@InProceedings{930269,
  author    = {W. L. Nowinski and Chee-Kong Chui},
  title     = {Simulation of interventional neuroradiology procedures},
  booktitle = {Proceedings International Workshop on Medical Imaging and Augmented Reality},
  year      = {2001},
  pages     = {87-94},
  abstract  = {We describe the design and development of a computer environment for planning interventional neuroradiology procedures. The Neuroradiology Catheterization Simulator called NeuroCath is intended for interventional procedures involving vascular malformations, such as aneurysms, stenosis, and AVMs. NeuroCath include extraction and construction of a vascular model from different imaging modalities that represents the anatomy of patient in a computationally efficient manner, and a FEM-based physical model that simulates the behavior between the devices and cerebral vasculature. This model comprises topology, geometry (normal and pathological), and physical properties of the patient-specific vasculature. It also provides a reliable measurement of distance and volume allowing calculation of the size of vessels and aneurysms. A realistic visual interface with multiple, synchronized windows is developed. The visual interface comprises of fluoroscopic display that duplicates the views to be seen in actual intentional procedures, and other displays that enhance interpretation of the anatomy of the patient. The hybrid volume and surface renderer provides insight into inferior and exterior of patient's vasculature. NeuroCath is also provided with the haptic apparatus that gives the interventional neuroradiologist the sense of touch during intervention planning and training},
  doi       = {10.1109/MIAR.2001.930269},
  keywords  = {blood vessels;cardiovascular system;digital simulation;haptic interfaces;medical image processing;rendering (computer graphics);surgery;FEM;NeuroCath;Neuroradiology Catheterization Simulator;geometry;haptic interface;interventional neuroradiology procedure simulation;surface rendering;surgery planning;synchronized windows;topology;touch;vascular malformations;vascular model;visual interface;volume rendering;Anatomy;Aneurysm;Catheterization;Computational modeling;Displays;Geometry;Pathology;Physics computing;Solid modeling;Topology},
}

@InProceedings{930270,
  author    = {Shi Jiao-Ying and Yan Li-Xia},
  title     = {Deformation and cutting in virtual surgery},
  booktitle = {Proceedings International Workshop on Medical Imaging and Augmented Reality},
  year      = {2001},
  pages     = {95-102},
  abstract  = {This paper discusses key techniques of deformation and cutting in virtual surgery. It introduces a new geometry reconstruction algorithm, which can get a layered tetrahedron based volume model. Based on the real physical property of soft-tissue a simplified viscoelastic physical model is introduced, and the finite element method is used for the deformation computation. Different from the collision between rigid bodies, in a virtual surgery environment the collision happens usually between a rigid body and a soft body. This paper brings forward an FDH based bounding volume hierarchy algorithm to handle the collision detection successfully. In order to simulate the cutting operation, a global cutting algorithm is introduced. The 3D texture mapping is used to enhance the realistic effect},
  doi       = {10.1109/MIAR.2001.930270},
  keywords  = {finite element analysis;image texture;medical computing;realistic images;surgery;virtual reality;3D texture mapping;bounding volume hierarchy algorithm;collision detection;cutting;deformation;finite element method;geometry reconstruction algorithm;layered tetrahedron based volume model;realistic image;soft-tissue;virtual reality;virtual surgery;viscoelastic physical model;Computational modeling;Deformable models;Elasticity;Finite element methods;Geometry;Physics computing;Reconstruction algorithms;Solid modeling;Surgery;Viscosity},
}

@InProceedings{660372,
  author    = {K. N. Kutulakos},
  title     = {Altering reality through interactive image and video manipulation},
  booktitle = {Proceedings 1998 IEEE and ATR Workshop on Computer Vision for Virtual Reality Based Human Communications},
  year      = {1998},
  pages     = {72-77},
  month     = {Jan},
  abstract  = {Camera calibration and the acquisition of Euclidean 3D measurements have so far been considered necessary requirements for overlaying three-dimensional graphical objects with live video. We describe an image-based approach to augmented reality that avoids both requirements: it does not use any metric information about the calibration parameters of the camera or the 3D locations and dimensions of the environment's objects. The only requirement is the ability to track across frames at least four feature points that are specified by the user during system initialization and whose world coordinates are unknown. The resulting system allows a user to “augment” a real 3D environment with computer-generated objects by interactively manipulating one or more images of that environment},
  doi       = {10.1109/CVVRHC.1998.660372},
  keywords  = {image processing;virtual reality;altering reality;augmented reality;image-based approach;interactive image;system initialization;video manipulation;world coordinates;Application software;Augmented reality;Calibration;Cameras;Computer science;Electrical capacitance tomography;Graphics;Power system planning;Surgery;User interfaces},
}

@InProceedings{816462,
  author    = {C. Wada and Liyisong and S. Ino and T. Hukube},
  title     = {A proposal to correct depth perception of virtual objects by using tactile feedback},
  booktitle = {Systems, Man, and Cybernetics, 1999. IEEE SMC '99 Conference Proceedings. 1999 IEEE International Conference on},
  year      = {1999},
  volume    = {6},
  pages     = {92-97 vol.6},
  abstract  = {Mixed reality (MR) is a technique which can combine a virtual environment and a real environment without making users feel that anything is unnatural. We propose a method using a see-through head mounted display (STHMD) to improve the precision of depth perception using tactile feedback. The visual accuracy of depth perception was measured when virtual objects were displayed through a STHMD. Subjects perceived the real objects to be closer to them than the virtual objects. The reason for this may be due to the optical characteristics of the HMD. We then manipulated our fingers while watching virtual objects through the STHMD. After decreasing the visual depth gap, we measured how accurately subjects moved their fingers to the exact location where the virtual objects appeared. From the results, it was found that there were depth gaps between our fingers and the virtual objects. Finally, vibratory tactile feedback was presented to the fingertip when the finger was near the virtual object. We measured how accurately subjects moved their fingers to the virtual objects. There was a range where subjects did not perceive the gap between their fingers and the virtual objects when tactile feedback was presented. The reason for this improvement was the integration between visual and tactile information. Accordingly, by controlling the degree of perceptual error through various tactile feedback, we assume that the precision of depth perception would improve. From the above results, we have proposed that using visual and tactile integration is one method that could solve the problem of inaccurate virtual objects' depth perception},
  doi       = {10.1109/ICSMC.1999.816462},
  issn      = {1062-922X},
  keywords  = {augmented reality;computer displays;force feedback;haptic interfaces;head-up displays;human factors;visual perception;depth perception correction;finger movement;mixed reality;optical characteristics;perceptual error;precision;see-through head mounted display;tactile feedback;vibrations;virtual objects;visual accuracy;visual depth gap;Displays;Feedback;Fingers;Head;Humans;Minimally invasive surgery;Proposals;Surges;Virtual environment;Virtual reality},
}

@InProceedings{893780,
  author    = {D. Tanase and J. F. L. Goosen and P. J. Trimp and J. A. Reekers and P. J. French},
  title     = {Catheter navigation system for intravascular use},
  booktitle = {1st Annual International IEEE-EMBS Special Topic Conference on Microtechnologies in Medicine and Biology. Proceedings (Cat. No.00EX451)},
  year      = {2000},
  pages     = {239-242},
  abstract  = {Minimally invasive intravascular interventions are performed with the aid of guide wires and catheters. When inserted into the cardiovascular system, the location of these tools is monitored using X-ray imaging techniques. However, a high radiation dose during the medical procedures, creates an important health risk for clinicians and assistant. In order to reduce the radiation dose, a relevant X-ray image is taken during the medical intervention and is augmented, at successive moments during the procedure, with a virtual indicator for the position and orientation of the guide wire tip. The location of the tip is determined by means of an electromagnetic system, composed of coils, outside the patient, and a three-dimensional sensor on the tip of the guide wire. This way, the need to continuously use radiation is eliminated. Furthermore, the standard intravascular procedure is not changed and the cost of the system is kept low},
  doi       = {10.1109/MMB.2000.893780},
  keywords  = {biomedical equipment;blood vessels;diagnostic radiography;dosimetry;navigation;patient monitoring;virtual reality;X-ray imaging techniques;catheter navigation system;clinicians;guide wire tip orientation;guide wires;high radiation dose;important health risk;intravascular use;low cost system;minimally invasive intravascular interventions;standard intravascular procedure;three-dimensional sensor;tip location;tools location monitoring;virtual indicator;Biomedical imaging;Biomedical monitoring;Cardiovascular system;Catheters;Coils;Electromagnetic radiation;Minimally invasive surgery;Navigation;Wire;X-ray imaging},
}

@Article{736022,
  author   = {I. Bricault and G. Ferretti and P. Cinquin},
  title    = {Registration of real and CT-derived virtual bronchoscopic images to assist transbronchial biopsy},
  journal  = {IEEE Transactions on Medical Imaging},
  year     = {1998},
  volume   = {17},
  number   = {5},
  pages    = {703-714},
  month    = {Oct},
  issn     = {0278-0062},
  abstract = {This paper describes research work motivated by an innovative medical application: computer-assisted transbronchial biopsy. This project involves the registration, with no external localization device, of a preoperative three-dimensional (3-D) computed tomography (CT) scan of the thoracic cavity (showing a tumor that requires a needle biopsy), and an intraoperative endoscopic two-dimensional (2-D) image sequence, in order to provide assistance in transbronchial puncture of the tumor. Because of the specific difficulties resulting from the data being processed, a multilevel strategy was introduced. For each analysis level, the relevant information to process and the corresponding algorithms were defined. This multilevel strategy, thus, provides the best possible accuracy. Original image processing methods were elaborated, dealing with segmentation, registration and 3-D reconstruction of the bronchoscopic images. In particular, these methods involve adapted mathematical morphology tools, a "daemon-based" registration algorithm, and a model-based shape-from-shading algorithm. This pilot study presents the application of these algorithms to recorded bronchoscopic video sequences for five patients. The preliminary results presented here demonstrate that it is possible to precisely localize the endoscopic camera within the CT data coordinate system. The computer can thus synthesize in near real-time the CT-derived virtual view that corresponds to the actual endoscopic view.},
  doi      = {10.1109/42.736022},
  keywords = {augmented reality;computerised tomography;image registration;image sequences;lung;medical image processing;optical images;surgery;tumours;CT-derived virtual bronchoscopic images;augmented reality intervention;fibre optic endoscopy;intraoperative endoscopic two-dimensional image sequence;medical diagnostic imaging;multilevel strategy;thoracic cavity;transbronchial biopsy;tumor;Biomedical equipment;Biopsy;Computed tomography;Computer applications;Image sequences;Information analysis;Medical services;Needles;Neoplasms;Two dimensional displays;Biopsy, Needle;Bronchoscopy;Humans;Image Processing, Computer-Assisted;Lung;Pilot Projects;Radiography, Interventional;Tomography, X-Ray Computed},
}

@Article{486716,
  author   = {A. C. M. Dumay},
  title    = {Beyond medicine},
  journal  = {IEEE Engineering in Medicine and Biology Magazine},
  year     = {1996},
  volume   = {15},
  number   = {2},
  pages    = {34-40},
  month    = {Mar},
  issn     = {0739-5175},
  abstract = {Virtual Environments (VE) are simulation systems in which the subject (user) is to a large extent immersed in the apparent environment of a simulated, multidimensional reality. VE is the term used by researchers to describe the form of computer-human interaction where the human is immersed in a world created by the machine, which is usually a computer system. The term is derived from the term virtual reality (VR). We use the term VE to emphasise the immersion of the subject in the virtual environment synthesised by the machine. Other terms in use for indicating this type of interface are cyberspace, telepresence, mirror world, artificial reality, augmented reality, wraparound compuvision, and synthetic environments. The article looks at the potentials and limitations of VEs, with a focus on applications in medicine},
  doi      = {10.1109/51.486716},
  keywords = {biomedical education;computer based training;digital simulation;medical computing;medicine;simulation;training;virtual reality;apparent environment;artificial reality;augmented reality;computer system;computer-human interaction;cyberspace;medicine;mirror world;simulated multidimensional reality;simulation systems;synthetic environments;telepresence;virtual environments;virtual reality;wraparound compuvision;Actuators;Application software;Auditory system;Biological system modeling;Computational modeling;Data gloves;Humans;Medical simulation;Multidimensional systems;Surgery},
}

@InProceedings{490522,
  author    = {B. J. Brickman and L. J. Hettinger and M. M. Roe and Liem Lu and D. W. Repperger and M. W. Haas},
  title     = {Haptic specification of environmental events: implications for the design of adaptive, virtual interfaces},
  booktitle = {Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium},
  year      = {1996},
  pages     = {147-153},
  month     = {Mar},
  abstract  = {Future airborne crewstations are currently being designed that will incorporate multisensory virtual displays to convey operationally relevant information to crew members. In addition, these displays and associated controls will be designed to adapt to the changing psychological and physiological state of the user, and the tactical/environmental state of the external world. In support of this design goal, research is being conducted to explore the information extraction capabilities of the sensory modalities. Toward this end, an experiment was conducted to assess the degree to which force-reflective haptic stimulation can be used to provide individuals with information about their location and movement through space. Specifically, a force-reflecting, haptically-augmented aircraft control stick was designed and utilized with the goal of providing pilots with real-time information concerning lateral deviation (or “line-up”) with respect to the runway in a simulated instrument landing task. Pilots executed simulated landing approaches with either the force-reflecting stick or a standard aircraft displacement stick under either calm or turbulent conditions. The results indicated a consistent advantage in performance and perceived workload for the force-reflecting stick, particularly under conditions of simulated turbulence. The results are discussed in terms of their relevance for the design of advanced airborne crewstations that utilize multisensory, adaptive, virtual interfaces},
  doi       = {10.1109/VRAIS.1996.490522},
  keywords  = {aerospace computing;aircraft displays;human factors;psychology;real-time systems;systems analysis;user interfaces;virtual reality;adaptive virtual interfaces;airborne crewstations;aircraft control stick;aircraft displacement stick;environmental events;force-reflective haptic stimulation;haptic specification;multisensory virtual displays;performance;physiological state;psychological state;real-time information;simulated instrument landing task;user interface design;virtual reality;Aerospace control;Aircraft;Data mining;Displays;Haptic interfaces;Information resources;Instruments;Surgery;Vehicle dynamics;Virtual environment},
}

@InProceedings{802727,
  author    = {L. A. Khan and B. W. Fei and W. S. Ng and C. K. Kwoh},
  title     = {X-ray localization technique for total hip replacement operation in augmented reality for therapy (ART)},
  booktitle = {Proceedings of the First Joint BMES/EMBS Conference. 1999 IEEE Engineering in Medicine and Biology 21st Annual Conference and the 1999 Annual Fall Meeting of the Biomedical Engineering Society (Cat. N},
  year      = {1999},
  volume    = {1},
  pages     = {649 vol.1-},
  abstract  = {A novel approach for the localization of acetabular prosthesis cup placement, using X-ray images, in total hip replacement operation is described. A computational mathematical model is given and simulation results are presented. This information helps the orthopaedic surgeons to know the position of the hip in world coordinate system and direct him to the right position of implant insertion tool, viewing through a capture and display unit. X-ray localization can be used both preoperatively and intraoperatively. Ability to use X-rays intraoperatively is the major advantage of the authors' proposed technique},
  doi       = {10.1109/IEMBS.1999.802727},
  issn      = {1094-687X},
  keywords  = {augmented reality;diagnostic radiography;medical computing;orthopaedics;prosthetics;surgery;acetabular prosthesis cup placement localization approach;augmented reality for therapy;capture/display unit;computational mathematical model;implant insertion tool;intraoperative X-rays;total hip replacement operation;world coordinate system;Augmented reality;Computational modeling;Costs;Hip;Implants;Mathematical model;Medical treatment;Orthopedic surgery;Subspace constraints;X-ray imaging},
}

@Article{735781,
  author   = {T. Berlage},
  title    = {Augmented-reality communication for diagnostic tasks in cardiology},
  journal  = {IEEE Transactions on Information Technology in Biomedicine},
  year     = {1998},
  volume   = {2},
  number   = {3},
  pages    = {169-173},
  month    = {Sept},
  issn     = {1089-7771},
  abstract = {In the CardiAssist project, a teleconsultation module has been implemented that uses both ultrasound images and three-dimensional (3-D) animated heart models as common artifacts to support communication between medical professionals. Real images and conceptual models are linked as "augmented reality" so that the models provide orientation for diagnostic tasks in cardiology. Hypotheses or results can be documented on the image or the abstract level, as the heart model can be modified to include pathological details. In teleconsultation, pointing references accelerate communication between physicians of different backgrounds.},
  doi      = {10.1109/4233.735781},
  keywords = {augmented reality;echocardiography;medical diagnostic computing;3D animated heart models;CardiAssist project;abstract level;augmented-reality communication;cardiology;conceptual models;diagnostic tasks;medical professionals;pathological details;teleconsultation;teleconsultation module;ultrasound images;Augmented reality;Biomedical imaging;Cardiology;Echocardiography;Heart;Layout;Medical diagnostic imaging;Pathology;Surgery;Ultrasonic imaging;Cardiovascular Diseases;Humans;Models, Anatomic;Remote Consultation},
}

@InProceedings{974105,
  author    = {Zou Qingsong and Kwoh Chee Keong and Ng Wan Sing and Chen Yintao},
  title     = {MRI head segmentation for object based volume visualization},
  booktitle = {The Seventh Australian and New Zealand Intelligent Information Systems Conference, 2001},
  year      = {2001},
  pages     = {361-366},
  month     = {Nov},
  abstract  = {In this paper, we present a new image segmentation approach for MRI of the head, which is a semi-automatic process. Unlike automatic segmentation or manual segmentation, the semi-automatic segmentation approach is a robust and interactive segmentation process. This approach carries out 3D volume data segmentation based on 2D image slices. By utilising the user-provided image mask, including areas of interest or structural information, the semi-automatic segmentation process can generate a new segmented volume dataset and structural information. The object based volume visualization method can use this segmented dataset and structural information to perform structure based manipulation and visualization, which cannot be achieved using a normal volume rendering method.},
  doi       = {10.1109/ANZIIS.2001.974105},
  keywords  = {biomedical MRI;data visualisation;image segmentation;medical image processing;stereo image processing;2D image slices;3D volume data segmentation;MRI head segmentation;image mask;interactive segmentation;object based volume visualization;semi-automatic segmentation;structure-based manipulation;structure-based visualization;volume dataset;Augmented reality;Biomedical imaging;Computer displays;Data visualization;Head;Image segmentation;Magnetic resonance imaging;Quantum computing;Rendering (computer graphics);Surgery},
}

@InProceedings{6948399,
  author    = {T. Peters},
  title     = {Keynote address: The role of augmented reality displays for guiding intra-cardiac interventions},
  booktitle = {2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
  year      = {2014},
  pages     = {xxi-xxi},
  month     = {Sept},
  abstract  = {Summary form only given. Many inter-cardiac interventions are performed either via open-heart surgery, or using minimally invasive approaches, where instrumentation is introduced into the cardiac chambers via the vascular system or heart wall. While many of the latter procedures are often employed under x-ray guidance, for some of these xray imaging is not appropriate, and ultrasound is the preferred intra-operative imaging modality. Two such procedures involves the repair of a mitral-valve leafet, and the replacement of aortic valves. Both employ instruments introduced into the heart via the apex. For the mitral procedure, the standard of care for this procedure employs a 3D Trans-esophageal echo (TEE) probe as guidance, but using primarily its bi-plane mode, with full 3D only being used sporadically. In spite of the clinical success of this procedure, many problems are encountered during the navigation of the instrument to the site of the therapy. To overcome these difficulties, we have developed a guidance platform that tracks the US probe and instrument, and augments the US mages with virtual elements representing the instrument and target, to optimise the navigation process. Results of using this approach on animal studies have demonstrated increased performance in multiple metrics, including total tool distance from ideal pathway, total navigation time, and total tool path lengths, by factors of 3,4, and 5 respectively, as well as a 40 fold reduction in the number of times an instrument intruded into potentially unsafe zones in the heart.},
  doi       = {10.1109/ISMAR.2014.6948399},
  keywords  = {augmented reality;biomedical ultrasonics;cardiology;medical image processing;surgery;3D trans-esophageal echo;TEE probe;X-ray guidance;augmented reality display;bi-plane mode;imaging modality;intra-cardiac intervention guidance;minimally invasive approach;mitral procedure;mitral-valve leafet;open-heart surgery;ultrasound},
}

@InProceedings{6935429,
  author    = {T. Peters},
  title     = {The role of augmented reality displays for guiding intra-cardiac interventions},
  booktitle = {2014 IEEE International Symposium on Mixed and Augmented Reality - Media, Art, Social Science, Humanities and Design (ISMAR-MASH'D)},
  year      = {2014},
  pages     = {1-1},
  month     = {Sept},
  abstract  = {Many inter-cardiac interventions are performed either via open-heart surgery, or using minimally invasive approaches, where instrumentation is introduced into the cardiac chambers via the vascular system or heart wall. While many of the latter procedures are often employed under x-ray guidance, for some of these xray imaging is not appropriate, and ultrasound is the preferred intra-operative imaging modality. Two such procedures involves the repair of a mitral-valve leaflet, and the replacement of aortic valves. Both employ instruments introduced into the heart via the apex. For the mitral procedure, the standard of care for this procedure employs a 3D Trans-esophageal echo (TEE) probe as guidance, but using primarily its bi-plane mode, with full 3D only being used sporadically. In spite of the clinical success of this procedure, many problems are encountered during the navigation of the instrument to the site of the therapy. To overcome these difficulties, we have developed a guidance platform that tracks the US probe and instrument, and augments the US mages with virtual elements representing the instrument and target, to optimise the navigation process. Results of using this approach on animal studies have demonstrated increased performance in multiple metrics, including total tool distance from ideal pathway, total navigation time, and total tool path lengths, by factors of 3,4,and 5 respectively, as well as a 40 fold reduction in the number of times an instrument intruded into potentially unsafe zones in the heart. The aortic valve procedure primarily uses X-ray fluoroscopy guidance, but this suffers from the problem of high radiations dose, poor target visibility and potential kidney damage as a result of x-ray contrast administration. To overcome these limitations, we have adopted similar technology to that used for the mitral valve problem, to develop an ultrasound-only solution, again augmented with virtual models of instruments and key targets to guide ao- tic valve replacement procedures. Preliminary results of this approach on cardiac phantoms indicate that the US-only approach may be as accurate as the standard fluoroscopy-guided technique.},
  doi       = {10.1109/ISMAR-AMH.2014.6935429},
  issn      = {2381-8360},
  keywords  = {X-ray imaging;augmented reality;cardiology;medical computing;radiography;surgery;3D Trans-esophageal echo probe;TEE probe;X-ray contrast administration;X-ray fluoroscopy guidance;X-ray guidance;X-ray imaging;aortic valve replacement procedures;aortic valves;augmented reality displays;bi-plane mode;cardiac chambers;heart wall;intra-cardiac intervention guidance;intra-operative imaging modality;minimally invasive approaches;mitral valve problem;mitral-valve leaflet;open-heart surgery;standard fluoroscopy-guided technique;ultrasound-only solution;vascular system},
}

@InProceedings{7847779,
  author    = {S. Martins and M. Vairinhos and S. Eliseu and J. Borgerson},
  title     = {Input system interface for image-guided surgery based on augmented reality},
  booktitle = {2016 1st International Conference on Technology and Innovation in Sports, Health and Wellbeing (TISHW)},
  year      = {2016},
  pages     = {1-6},
  month     = {Dec},
  abstract  = {Augmented reality allows users to enhance their perception of the real world by using technology to overlap digital information onto their field of view. This raises challenges of a technical and conceptual nature, the latter particularly from an interaction design standpoint. The implementation of such technology has advantages in many areas of human activity, including its use in clinical interventions. The main objective of this research is to develop a navigation system for Image Guided Surgery (IGS), capable of combining the real world elements of the surgeon's visual field with virtual representations of internal organs, tissues, meta-information and clinical instruments allowing clinicians to approach surgical procedures much more fluently. This proposed method would be supported by electromagnetic tracking, allowing surgeons to view captures from the ultrasonic probe directly in their field of view. This would make it possible to support needle insertion in a more naturalistic environment. As the first of a series of evaluations, the input system interface will be presented and discussed in this paper.},
  date      = {2016-12},
  doi       = {10.1109/TISHW.2016.7847779},
  keywords  = {augmented reality;human computer interaction;image representation;medical image processing;surgery;user interfaces;augmented reality;clinical interventions;electromagnetic tracking;human activity;image-guided surgery;input system interface;ultrasonic probe;virtual representations;Acoustics;Augmented reality;Needles;Optical imaging;Probes;Surgery;augmented reality;human-computer interaction;image-guided surgery},
}

@Article{rukzio_formate_2001,
  author  = {Rukzio, Enrico},
  title   = {Formate, Technologien und Architekturkonzepte f{\"u}r {3D-Web-Applikationen}},
  journal = {Belegarbeit der Technischen Universit{\"a}t Dresden},
  year    = {2001},
  url     = {http://www.medien.ifi.lmu.de/fileadmin/mimuc/rukzio/BelegarbeitEnricoRukzio.pdf},
}

@Article{rukzio_realisierung_2002,
  author = {Rukzio, Enrico},
  title  = {Realisierung von Interaktionen und Verhalten in dokumentbestimmten, komponentenbasierten {3D-Applikationen}},
  year   = {2002},
  url    = {http://www.comp.lancs.ac.uk/\verb=~=rukzio/publications/Diplom2002_rukzio.pdf},
}

@Book{rukzio_generic_2003,
  title     = {A generic extension mechanism for x3d to define, implement and integrate new first-class nodes, components, and profiles},
  publisher = {{PhD} Thesis, Dresden University of Technology},
  year      = {2003},
  author    = {Rukzio, Enrico},
  url       = {http://www.comp.lancs.ac.uk/\verb=~=rukzio/publications/X3D2003_rukzio.pdf},
}

@InProceedings{dachselt_behavior3d:_2003,
  author    = {Dachselt, Raimund and Rukzio, Enrico},
  title     = {Behavior3D: an XML-based framework for 3D graphics behavior},
  booktitle = {Proceedings of the eighth international conference on {3D} Web technology},
  year      = {2003},
  doi       = {10.1145/636593.636609},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2003/DachseltWeb3D2003.pdf},
}

@Article{rukzio_privacy-enhanced_2004,
  author  = {Rukzio, Enrico and Schmidt, Albrecht and Hussmann, Heinrich},
  title   = {Privacy-enhanced intelligent automatic form filling for context-aware services on mobile devices},
  journal = {Artificial Intelligence in Mobile Systems 2004 (AIMS 2004)},
  year    = {2004},
  pages   = {84},
  url     = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2004/RukzioAIMS2004.pdf},
}

@InProceedings{rukzio_context_2004,
  author    = {Rukzio, Enrico and Prezerakos, George N. and Cortese, Giovanni and Koutsoloukas, Eleftherios and Kapellaki, Sofia},
  title     = {Context for Simplicity: A Basis for Context-aware Systems Based on the {3GPP} Generic User Profile},
  booktitle = {International Conference on Computational Intelligence ({ICCI} 2004)},
  year      = {2004},
  pages     = {466\&ndash;469},
  url       = {http://www.comp.lancs.ac.uk/\verb=~=rukzio/publications/icci2004_Rukzio.pdf},
}

@InProceedings{rukzio_analysis_2004,
  author    = {Rukzio, Enrico and Schmidt, Albrecht and Hussmann, Heinrich},
  title     = {An analysis of the usage of mobile phones for personalized interactions with ubiquitous public displays},
  booktitle = {Workshop Ubiquitous Display Environments in conjunction with {UbiComp}},
  year      = {2004},
  url       = {http://www.medien.ifi.lmu.de/pubdb/publications/pub/rukzio2004ubidisplay/rukzio2004ubidisplay.pdf},
}

@InProceedings{rukzio_physical_2004,
  author    = {Rukzio, Enrico and Schmidt, Albrecht and Hussmann, Heinrich},
  title     = {Physical posters as gateways to context-aware services for mobile devices},
  booktitle = {Mobile Computing Systems and Applications, 2004. WMCSA 2004. Sixth IEEEWorkshop on},
  year      = {2004},
  doi       = {10.1109/MCSA.2004.20},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2004/RukzioWMCSA2004.pdf},
}

@Article{rukzio_framework_2005,
  author  = {Rukzio, Enrico and Wetzstein, Sergej and Schmidt, Albrecht},
  title   = {A Framework for Mobile Interactions with the Physical World},
  journal = {Proceedings of Wireless Personal Multimedia Communication ({WPMC'05)}},
  year    = {2005},
  url     = {https://www.medien.ifi.lmu.de/fileadmin/mimuc/rukzio/rukzio_wpmc2005.pdf},
}

@InProceedings{brugnoli_user_2005,
  author    = {Brugnoli, Maria Cristina and Hamard, John and Rukzio, Enrico},
  title     = {User expectations for simple mobile ubiquitous computing environments},
  booktitle = {Mobile Commerce and Services, 2005. {WMCS'05.} The Second {IEEE} International Workshop on},
  year      = {2005},
  doi       = {10.1109/WMCS.2005.27},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2005/BrugnoliWMCS2005.pdf},
}

@InProceedings{rukzio_physical_2005,
  author    = {Rukzio, Enrico and Pleuss, Andreas and Terrenghi, Lucia},
  title     = {The physical user interface profile (PUIP): Modelling mobile interactions with the real world},
  booktitle = {Proceedings of the 4th international workshop on Task models and diagrams},
  year      = {2005},
  doi       = {10.1145/1122935.1122954},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2005/RukzioTAMODIA2005.pdf},
}

@InProceedings{rukzio_rotating_2005,
  author    = {Rukzio, Enrico and Schmidt, Albrecht and Kr{\"u}ger, Antonio},
  title     = {The rotating compass: a novel interaction technique for mobile navigation},
  booktitle = {{CHI'05} extended abstracts on Human factors in computing systems},
  year      = {2005},
  doi       = {10.1145/1056808.1057016},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2005/RukzioCHI2005.pdf},
}

@InProceedings{rukzio_development_2005,
  author    = {Rukzio, Enrico and Rohs, Michael and Wagner, Daniel and Hamard, John},
  title     = {Development of interactive applications for mobile devices},
  booktitle = {{ACM} International Conference Proceeding Series},
  year      = {2005},
  volume    = {111},
  pages     = {365\&ndash;366},
  url       = {http://www.medien.ifi.lmu.de/fileadmin/mimuc/rukzio/tutorial_mobilehci2005.pdf},
}

@InProceedings{leichtenstern_analysis_2005,
  author    = {Leichtenstern, Karin and Luca, A. D. and Rukzio, Enrico},
  title     = {Analysis of built-in mobile phone sensors for supporting interactions with the real world},
  booktitle = {Pervasive Mobile Interaction Devices ({PERMID} 2005)-Mobile Devices as Pervasive User Interfaces and Interaction Devices-Workshop at the Pervasive 2005},
  year      = {2005},
  url       = {http://www.medien.ifi.lmu.de/permid2005/pdf/KarinLeichtenstern_Permid2005.pdf},
}

@InProceedings{rukzio_policy_2005,
  author    = {Rukzio, Enrico and Siorpaes, Sven and Falke, Oliver and Hussmann, Heinrich},
  title     = {Policy based adaptive services for mobile commerce},
  booktitle = {Mobile Commerce and Services, 2005. {WMCS'05.} The Second {IEEE} International Workshop on},
  year      = {2005},
  doi       = {10.1109/WMCS.2005.18},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2005/RukzioWMCS2005.pdf},
}

@Article{leichtenstern_mobile_2006,
  author  = {Leichtenstern, Karin and Rukzio, Enrico and Chin, Jeannette and Callaghan, Vic and Schmidt, Albrecht},
  title   = {Mobile interaction in smart environments},
  journal = {Computing},
  year    = {2006},
  volume  = {7},
  url     = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2006/LeichtensternPervCom2006.pdf},
}

@InCollection{rukzio_experimental_2006,
  author    = {Rukzio, Enrico and Leichtenstern, Karin and Callaghan, Vic and Holleis, Paul and Schmidt, Albrecht and Chin, Jeannette},
  title     = {An experimental comparison of physical mobile interaction techniques: Touching, pointing and scanning},
  booktitle = {{UbiComp} 2006: Ubiquitous Computing},
  publisher = {Springer},
  year      = {2006},
  doi       = {10.1007/11853565_6},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2006/RukzioUbiComp2006.pdf},
}

@InProceedings{schmidt_utilizing_2006,
  author    = {Schmidt, Albrecht and Hakkila, Jonna and Atterer, Richard and Rukzio, Enrico and Holleis, Paul},
  title     = {Utilizing mobile phones as ambient information displays},
  booktitle = {CHI'06 Extended Abstracts on Human Factors in Computing Systems},
  year      = {2006},
  doi       = {10.1145/1125451.1125692},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2006/SchmidtCHI2006.pdf},
}

@InProceedings{rukzio_visualization_2006,
  author    = {Rukzio, Enrico and Hamard, John and Noda, Chie and De Luca, Alexander},
  title     = {Visualization of uncertainty in context aware mobile applications},
  booktitle = {Proceedings of the 8th conference on Human-computer interaction with mobile devices and services},
  year      = {2006},
  pages     = {247\&ndash;250},
  doi       = {10.1145/1152215.1152267},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2006/RukzioCHI2006.pdf},
}

@InProceedings{broll_supporting_2006,
  author    = {Broll, Gregor and Siorpaes, Sven and Rukzio, Enrico and Paolucci, Massimo and Hamard, John and Wagner, Matthias and Schmidt, Albrecht},
  title     = {Supporting Mobile Service Interaction through Semantic Service Description Annotation and Automatic Interface Generation.},
  booktitle = {{ISWC} 2006 workshop on Semantic Desktop and Social Semantic Collaboration (Semdesk 2006)},
  year      = {2006},
  url       = {http://eprints.lancs.ac.uk/42334/1/semdesk2006.pdf},
}

@InProceedings{broll_supporting_2006-1,
  author    = {Broll, Gregor and Siorpaes, Sven and Rukzio, Enrico and Paolucci, Massimo and Hamard, John and Wagner, Matthias and Schmidt, Albrecht},
  title     = {Supporting service interaction in the real world},
  booktitle = {Workshop Permid},
  year      = {2006},
  url       = {http://www.researchgate.net/publication/228364452_Supporting_Service_Interaction_in_the_Real_World/file/32bfe50ec4f2cb65ab.pdf},
}

@InProceedings{bartolomeo_simplicity_2006,
  author    = {Bartolomeo, Giovanni and Martire, Francesca and Rukzio, Enrico and Salsano, Stefano and Melazzi, Nicola Blefari and Noda, Chie and Hamard, John and De Luca, Alexander},
  title     = {The Simplicity Device: Your Personal Mobile Representative},
  booktitle = {Workshop on Pervasive Mobile Interaction Devices ({PERMID} 2006), Dublin, Ireland},
  year      = {2006},
  url       = {http://www.comp.lancs.ac.uk/\verb=~=rukzio/publications/permid2006_Bartolomeo.pdf},
}

@InProceedings{siorpaes_mobile_2006,
  author    = {Siorpaes, Sven and Broll, Gregor and Paolucci, Massimo and Rukzio, Enrico and Hamard, John and Wagner, Matthias and Schmidt, Albrecht and Eurolabs, Docomo},
  title     = {Mobile interaction with the internet of things},
  booktitle = {Poster at 4th International Conference on Pervasive Computing (Pervasive 2006), Dublin, Ireland},
  year      = {2006},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2006/SiorpaesPERC2006.pdf},
}

@InProceedings{rukzio_mobile_2006,
  author    = {Rukzio, Enrico and Paolucci, Massimo and Finin, Tim and Wisner, Paul and Payne, Terry},
  title     = {Mobile interaction with the real world},
  booktitle = {{ACM} International Conference Proceeding Series},
  year      = {2006},
  volume    = {159},
  pages     = {295\&ndash;296},
  url       = {http://www.comp.lancs.ac.uk/\verb=~=rukzio/mobilehci2009tutorials/Rukzio_MobileInteractionWithTheRealWorld.pdf},
}

@InProceedings{schmidt_mobile_2006,
  author    = {Schmidt, Albrecht and Holleis, Paul and Hakkila, Jonna and Rukzio, Enrico and Atterer, Richard},
  title     = {Mobile phones as tool to increase communication and location awareness of users},
  booktitle = {Proceedings of the 3rd international conference on Mobile technology, applications \& systems},
  year      = {2006},
  doi       = {10.1145/1292331.1292355},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2006/SchmidtMobility2006.pdf},
}

@InProceedings{rukzio_mobile_2006-1,
  author    = {Rukzio, Enrico and Paolucci, Massimo and Wagner, Matthias and Berndt, Hendrik H. and Hamard, John and Schmidt, Albrecht},
  title     = {Mobile service interaction with the web of things},
  booktitle = {13th International Conference on Telecommunications (ICT 2006), Funchal, Madeira island, Portugal, 2006c},
  year      = {2006},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2006/RukzioICT2006.pdf},
}

@PhdThesis{rukzio_physical_2006,
  author = {Rukzio, Enrico},
  title  = {Physical mobile interactions: Mobile devices as pervasive mediators for interactions with the real world},
  school = {University of Munich},
  year   = {2006},
  url    = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2006/RukzioDissertation.pdf},
}

@Article{broll_comparing_2007,
  author = {Broll, Gregor and Siorpaes, Sven and Rukzio, Enrico and Paolucci, Massimo and Hamard, John and Wagner, Matthias and Schmidt, Albrecht},
  title  = {Comparing Techniques for Mobile Interaction with Objects from the Real World.},
  year   = {2007},
  url    = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2007/BrollPermid2007.pdf},
}

@Article{broll_authoring_2007,
  author = {Broll, Gregor and Rukzio, Enrico and Wedi, Bj{\"o}rn},
  title  = {Authoring Support for Mobile Interaction with the Real World.},
  year   = {2007},
  url    = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2007/BrollPervasive2007.pdf},
}

@Article{falke_mobile_2007,
  author  = {Falke, Oliver and Rukzio, Enrico and Dietz, Ulrich and Holleis, Paul and Schmidt, Albrecht},
  title   = {Mobile services for near field communication},
  journal = {Ludwig-Maximilians-Universit{\"a}t ({LMU)}, Munich, Germany, Technical Report {LMUMI-2007-1}},
  year    = {2007},
  url     = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2007/FalkeNFC2007.pdf},
}

@InCollection{rukzio_mobile_2007,
  author    = {Rukzio, Enrico and Broll, Gregor and Leichtenstern, Karin and Schmidt, Albrecht},
  title     = {Mobile interaction with the real world: An evaluation and comparison of physical mobile interaction techniques},
  booktitle = {Ambient Intelligence},
  publisher = {Springer},
  year      = {2007},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2007/RukzioAmbIntelli2007.pdf},
}

@InProceedings{broll_using_2007,
  author    = {Broll, Gregor and Hussmann, Heinrich and Rukzio, Enrico and Wimmer, Raphael},
  title     = {Using Video Clips to Support Requirements Elicitation in Focus Groups-An Experience Report.},
  booktitle = {{SE} 2007 workshop on Multimedia Requirements Engineering},
  year      = {2007},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2007/BrollRequEng2007.pdf},
}

@InProceedings{broll_supporting_2007,
  author    = {Broll, Gregor and Siorpaes, Sven and Rukzio, Enrico and Paolucci, Massimo and Hamard, John and Wagner, Matthias and Schmidt, Albrecht},
  title     = {Supporting mobile service usage through physical mobile interaction},
  booktitle = {Pervasive Computing and Communications, 2007. PerCom'07. Fifth Annual IEEE International Conference on},
  year      = {2007},
  doi       = {10.1109/PERCOM.2007.35},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2007/BrollPervCom2007.pdf},
}

@InProceedings{henze_contextual_2007,
  author    = {Henze, Niels and Lim, Mingyu and Lorenz, Andreas and Mueller, Michael and Righetti, Xavier and Rukzio, Enrico and Magnenat-Thalmann, Nadia and Zimmermann, Andreas},
  title     = {Contextual bookmarks},
  booktitle = {MobileHCI 2007 workshop on Mobile Interaction with the Real World (MIRW 2007)},
  year      = {2007},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2007/HenzeMobileHCI2007.pdf},
}

@InProceedings{vetter_physical_2007,
  author    = {Vetter, Johannes and Hamard, John and Paolucci, Massimo and Rukzio, Enrico and Schmidt, Albrecht},
  title     = {Physical mobile interaction with dynamic physical object},
  booktitle = {Proceedings of the 9th international conference on Human computer interaction with mobile devices and services},
  year      = {2007},
  doi       = {10.1145/1377999.1378030},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2007/VetterMobileHCI2007.pdf},
}

@InProceedings{holleis_privacy_2007,
  author    = {Holleis, Paul and Rukzio, Enrico and Otto, Friderike and Schmidt, Albrecht},
  title     = {Privacy and Curiosity in Mobile Interactions with Public Displays.},
  booktitle = {{CHI} 2007 workshop on Mobile Spatial Interaction},
  year      = {2007},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2007/HolleisCHI2007.pdf},
}

@InProceedings{broll_mobile_2007,
  author    = {Broll, Gregor and Hamard, John and Paolucci, Massimo and Haarl{\"a}nder, Markus and Wagner, Matthias and Siorpaes, Sven and Rukzio, Enrico and Schmidt, Albrecht and Wiesner, Kevin},
  title     = {Mobile interaction with web services through associated real world objects},
  booktitle = {Proceedings of the 9th international conference on Human computer interaction with mobile devices and services},
  year      = {2007},
  pages     = {319\&ndash;321},
  doi       = {10.1145/1377999.1378025},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/mitarbeiterbereiche/wolf/awards/broll_mobile_2007.pdf},
}

@Article{henze_services_2008,
  author  = {Henze, Niels and Reiners, Ren{\'e} and Righetti, Xavier and Rukzio, Enrico and Boll, Susanne},
  title   = {Services surround you},
  journal = {The Visual Computer},
  year    = {2008},
  doi     = {10.1007/s00371-008-0266-4},
  url     = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2008/HenzeVisualComp2008.pdf},
}

@Article{rukzio_physical_2008,
  author  = {Rukzio, Enrico and Broll, Gregor and Wetzstein, Sergej},
  title   = {The physical mobile interaction framework (pmif)},
  journal = {Technical Report {LMU-MI-2008-2}},
  year    = {2008},
  url     = {http://eprints.lancs.ac.uk/42386/1/lmureport2008.pdf},
}

@Article{rukzio_automatic_2008,
  author  = {Rukzio, Enrico and Noda, Chie and De Luca, Alexander and Hamard, John and Coskun, Fatih},
  title   = {Automatic form filling on mobile devices},
  journal = {Pervasive and Mobile Computing},
  year    = {2008},
  doi     = {10.1016/j.pmcj.2007.09.001},
  url     = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2008/RukzioPervMobileComp2008.pdf},
}

@Article{paolucci_bringing_2008,
  author  = {Paolucci, Massimo and Broll, Gregor and Hamard, John and Rukzio, Enrico and Wagner, Matthew and Schmidt, Albrect},
  title   = {Bringing semantic services to real-world objects},
  journal = {International Journal on Semantic Web and Information Systems (IJSWIS)},
  year    = {2008},
  volume  = {4},
  number  = {1},
  doi     = {10.4018/jswis.2008010103},
  url     = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2008/PaolucciJSemWebIncSys2008.pdf},
}

@Article{hardy_direct_2008,
  author = {Hardy, Robert and Rukzio, Enrico},
  title  = {Direct Touch-based Mobile Interaction with Dynamic Displays.},
  year   = {2008},
  url    = {http://comp.eprints.lancs.ac.uk/1644/1/chi2008ws_hardy.pdf},
}

@InCollection{broll_collect&drop:_2008,
  author    = {Broll, Gregor and Haarl{\"a}nder, Markus and Paolucci, Massimo and Wagner, Matthias and Rukzio, Enrico and Schmidt, Albrecht},
  title     = {Collect\&Drop:A technique for multi-tag interaction with real world objects and information},
  booktitle = {Ambient Intelligence},
  publisher = {Springer},
  year      = {2008},
  doi       = {10.1007/978-3-540-89617-3_12},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2008/BrollAmbIntelligence2008.pdf},
}

@InProceedings{gostner_usage_2008,
  author    = {Gostner, Roswitha and Rukzio, Enrico and Gellersen, Hans},
  title     = {Usage of spatial information for selection of co-located devices},
  booktitle = {Proceedings of the 10th international conference on Human computer interaction with mobile devices and services},
  year      = {2008},
  doi       = {10.1145/1409240.1409305},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2008/GostnerMobileHCI2008.pdf},
}

@InProceedings{hardy_touch_2008-1,
  author    = {Hardy, Robert and Rukzio, Enrico},
  title     = {Touch \& Interact: touch-based interaction with a tourist application},
  booktitle = {Proceedings of the 10th international conference on Human computer interaction with mobile devices and services},
  year      = {2008},
  doi       = {10.1145/1409240.1409337},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2008/HardyMobileHCI2008_2.pdf},
}

@InProceedings{hardy_touch_2008,
  author    = {Hardy, Robert and Rukzio, Enrico},
  title     = {Touch \& interact: touch-based interaction of mobile phones with displays},
  booktitle = {Proceedings of the 10th international conference on Human computer interaction with mobile devices and services},
  year      = {2008},
  doi       = {10.1145/1409240.1409267},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2008/HardyMobileHCI2008.pdf},
}

@InProceedings{hang_projector_2008,
  author    = {Hang, Alina and Rukzio, Enrico and Greaves, Andrew},
  title     = {Projector phone: a study of using mobile phones with integrated projector for interaction with maps},
  booktitle = {Proceedings of the 10th international conference on Human computer interaction with mobile devices and services},
  year      = {2008},
  doi       = {10.1145/1409240.1409263},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2008/HangMobileHCI2008.pdf},
}

@InProceedings{greaves_evaluation_2008,
  author    = {Greaves, Andrew and Rukzio, Enrico},
  title     = {Evaluation of picture browsing using a projector phone},
  booktitle = {Proceedings of the 10th international conference on Human computer interaction with mobile devices and services},
  year      = {2008},
  doi       = {10.1145/1409240.1409286},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2008/GreavesMobileHCI2008.pdf},
}

@InProceedings{henze_physical-virtual_2008,
  author    = {Henze, Niels and Rukzio, Enrico and Lorenz, Andreas and Righetti, Xavier and Boll, Susanne},
  title     = {Physical-virtual linkage with contextual bookmarks},
  booktitle = {Proceedings of the 10th international conference on Human computer interaction with mobile devices and services},
  year      = {2008},
  doi       = {10.1145/1409240.1409335},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2008/HenzeMobileHCI2008.pdf},
}

@InProceedings{greaves_picture_2008,
  author    = {Greaves, Andrew and Hang, Alina and Rukzio, Enrico},
  title     = {Picture browsing and map interaction using a projector phone},
  booktitle = {Proceedings of the 10th international conference on Human computer interaction with mobile devices and services},
  year      = {2008},
  doi       = {10.1145/1409240.1409336},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2008/GreavesMobileHCI2008_2.pdf},
}

@InProceedings{henze_mobile_2008,
  author    = {Henze, Niels and Broll, Gregor and Rukzio, Enrico and Rohs, Michael and Zimmermann, Andreas},
  title     = {Mobile interaction with the real world},
  booktitle = {Proceedings of the 10th international conference on Human computer interaction with mobile devices and services},
  year      = {2008},
  doi       = {10.1145/1409240.1409351},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2008/HenzeMobileHCI2008_2.pdf},
}

@Article{gellersen_supporting_2009,
  author  = {Gellersen, Hans and Fischer, Carl and Guinard, Dominique and Gostner, Roswitha and Kortuem, Gerd and Kray, Christian and Rukzio, Enrico and Streng, Sara},
  title   = {Supporting device discovery and spontaneous interaction with spatial references},
  journal = {Personal and Ubiquitous Computing},
  year    = {2009},
  doi     = {10.1007/s00779-008-0206-3},
  url     = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2009/GellersenPersUbiComp2009.pdf},
}

@Article{greaves_exploring_2009,
  author = {Greaves, Andrew and Akerman, Panu and Rukzio, Enrico and Cheverst, Keith and Hakkila, Jonna},
  title  = {Exploring user reaction to personal projection when used in shared public places: A formative study},
  year   = {2009},
  url    = {http://comp.eprints.lancs.ac.uk/2264/1/cam3sn2009_greaves.pdf},
}

@Article{broll_perci:_2009,
  author  = {Broll, Gregor and Rukzio, Enrico and Paolucci, Massimo and Wagner, Matthias and Schmidt, Albrecht and Hussmann, Heinrich},
  title   = {Perci: Pervasive service interaction with the internet of things},
  journal = {Internet Computing, {IEEE}},
  year    = {2009},
  volume  = {13},
  number  = {6},
  pages   = {74\&ndash;81},
  doi     = {10.1109/MIC.2009.120},
  url     = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2009/BrollInternetComp2009.pdf},
}

@InCollection{seewoonauth_nfc-based_2009,
  author    = {Seewoonauth, Khoovirajsingh and Rukzio, Enrico and Hardy, Robert and Holleis, Paul},
  title     = {NFC-based Mobile Interactions with Direct-View Displays},
  booktitle = {Human-Computer Interaction\&ndash;INTERACT 2009},
  publisher = {Springer},
  year      = {2009},
  doi       = {10.1007/978-3-642-03655-2_91},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2009/SeewonauthINTERACT2009.pdf},
}

@InProceedings{lorenz_using_2009,
  author    = {Lorenz, Andreas and De Castro, Clara Fernandez and Rukzio, Enrico},
  title     = {Using handheld devices for mobile interaction with displays in home environments},
  booktitle = {Proceedings of the 11th International Conference on Human-Computer Interaction with Mobile Devices and Services},
  year      = {2009},
  doi       = {10.1145/1613858.1613882},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2009/LorenzMobileHCI2009.pdf},
}

@InProceedings{greaves_view_2009,
  author    = {Greaves, Andrew and Rukzio, Enrico},
  title     = {View \& share: supporting co-present viewing and sharing of media using personal projection},
  booktitle = {Proceedings of the 11th International Conference on Human-Computer Interaction with Mobile Devices and Services},
  year      = {2009},
  doi       = {10.1145/1613858.1613914},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2009/GreavesMobileHCI2009.pdf},
}

@InProceedings{seewoonauth_touch_2009,
  author    = {Seewoonauth, Khoovirajsingh and Rukzio, Enrico and Hardy, Robert and Holleis, Paul},
  title     = {Touch \& connect and touch \& select: interacting with a computer by touching it with a mobile phone},
  booktitle = {Proceedings of the 11th International Conference on Human-Computer Interaction with Mobile Devices and Services},
  year      = {2009},
  doi       = {10.1145/1613858.1613905},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2009/SeewoonauthMobileHCI2009.pdf},
}

@InProceedings{rukzio_design_2009,
  author    = {Rukzio, Enrico and M{\"u}ller, Michael and Hardy, Robert},
  title     = {Design, implementation and evaluation of a novel public display for pedestrian navigation: the rotating compass},
  booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
  year      = {2009},
  doi       = {10.1145/1518701.1518722},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2009/RukzioCHI2009.pdf},
}

@InProceedings{hardy_exploring_2009,
  author    = {Hardy, Robert and Rukzio, Enrico and Wagner, Matthias and Paolucci, Massimo},
  title     = {Exploring expressive nfc-based mobile phone interaction with large dynamic displays},
  booktitle = {Near Field Communication, 2009. NFC'09. First International Workshop on},
  year      = {2009},
  doi       = {10.1109/NFC.2009.10},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2009/HardyNFC2009.pdf},
}

@InProceedings{zimmermann_mobile_2009,
  author    = {Zimmermann, Andreas and Henze, Niels and Righetti, Xavier and Rukzio, Enrico},
  title     = {Mobile interaction with the Real World},
  booktitle = {Proceedings of the 11th international Conference on Human-Computer interaction with Mobile Devices and Services},
  year      = {2009},
  doi       = {10.1145/1613858.1613980},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2009/ZimmermannMobileHCI2009.pdf},
}

@InProceedings{rukzio_projector_2010,
  author    = {Rukzio, Enrico and Holleis, Paul},
  title     = {Projector phone interactions: design space and survey},
  booktitle = {Workshop on coupled display visual interfaces at {AVI}},
  year      = {2010},
  url       = {http://www.researchgate.net/publication/228370421_Projector_Phone_Interactions_Design_Space_and_Survey/file/32bfe5121ef910a965.pdf},
}

@InProceedings{chehimi_throw_2010,
  author    = {Chehimi, Fadi and Rukzio, Enrico},
  title     = {Throw your photos: an intuitive approach for sharing between mobile phones and interactive tables},
  booktitle = {Proceedings of the 12th {ACM} international conference adjunct papers on Ubiquitous computing-Adjunct},
  year      = {2010},
  doi       = {10.1145/1864431.1864479},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2010/ChehimiUbicomp2010.pdf},
}

@InProceedings{kawsar_explorative_2010,
  author    = {Kawsar, Fahim and Rukzio, Enrico and Kortuem, Gerd},
  title     = {An explorative comparison of magic lens and personal projection for interacting with smart objects},
  booktitle = {Proceedings of the 12th international conference on Human computer interaction with mobile devices and services},
  year      = {2010},
  doi       = {10.1145/1851600.1851627},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2010/KawsarMobileHCI2010.pdf},
}

@InProceedings{schmidt_phonetouch:_2010,
  author    = {Schmidt, Dominik and Chehimi, Fadi and Rukzio, Enrico and Gellersen, Hans},
  title     = {PhoneTouch: a technique for direct phone interaction on surfaces},
  booktitle = {Proceedings of the 23nd annual {ACM} symposium on User interface software and technology},
  year      = {2010},
  pages     = {13\&ndash;16},
  doi       = {10.1145/1866029.1866034},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2010/PhoneTouch.pdf},
}

@InProceedings{hardy_mystate:_2010,
  author    = {Hardy, Robert and Rukzio, Enrico and Holleis, Paul and Broll, Gregor and Wagner, Matthias},
  title     = {MyState: using NFC to share social and contextual information in a quick and personalized way},
  booktitle = {Proceedings of the 12th {ACM} international conference adjunct papers on Ubiquitous computing-Adjunct},
  year      = {2010},
  doi       = {10.1145/1864431.1864481},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2010/HardyUbicomp2010.pdf},
}

@InProceedings{schildbach_investigating_2010,
  author    = {Schildbach, Bastian and Rukzio, Enrico},
  title     = {Investigating selection and reading performance on a mobile phone while walking},
  booktitle = {Proceedings of the 12th international conference on Human computer interaction with mobile devices and services},
  year      = {2010},
  doi       = {10.1145/1851600.1851619},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2010/SchildbachMobileHCI2010.pdf},
}

@InProceedings{hardy_mobile_2010,
  author    = {Hardy, Robert and Rukzio, Enrico and Holleis, Paul and Wagner, Matthias},
  title     = {Mobile interaction with static and dynamic {NFC-based displays}},
  booktitle = {Proceedings of the 12th international conference on Human computer interaction with mobile devices and services},
  year      = {2010},
  doi       = {10.1145/1851600.1851623},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2010/HardyMobileHCI2010.pdf},
}

@InProceedings{Hardy:2011:MUI:2107596.2107598,
  author    = {Hardy, Robert and Rukzio, Enrico and Wagner, Matthias and Holleis, Paul},
  title     = {MultiKit: a user interface toolkit for multi-tag applications.},
  booktitle = {In Proc. of MUM 2011 (International Conference on Mobile and Ubiquitous Multimedia), ACM, 10 pages.},
  year      = {2011},
  doi       = {10.1145/2107596.2107598},
  url       = {/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2011/2011_rhardy_mum2011.pdf},
}

@InProceedings{Dachselt:2011:MPP:1979482.1979588,
  author    = {Dachselt, Raimund and Jones, Matt and Hakkila, Jonna and L{\"o}chtefeld, Markus and Rohs, Michael and Rukzio, Enrico},
  title     = {Mobile and personal projection (MP2)},
  booktitle = {In Extended Abstracts of CHI 2011 (SIGCHI Conference on Human Factors in Computing Systems), ACM, 3 pages.},
  year      = {2011},
  doi       = {10.1145/1979742.1979588},
  url       = {/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2011/2011_dachselt_chi2011ea.pdf},
}

@InProceedings{winkler_interactive_20110,
  author    = {Winkler, Christian and Reinartz, Christian and Nowacka, Diana and Rukzio, Enrico},
  title     = {Interactive phone call: synchronous remote collaboration and projected interactive surfaces.},
  booktitle = {In Proc. of ITS 2011 (ACM International Conference on Interactive Tabletops and Surfaces), ACM, 10 pages.},
  year      = {2011},
  doi       = {10.1145/2076354.2076367},
  web_url2  = {http://youtu.be/Qv6Wv8Nv6sI},
}

@InProceedings{henze_100000000_2011,
  author    = {Henze, Niels and Rukzio, Enrico and Boll, Susanne},
  title     = {100,000,000 Taps: Analysis and Improvement of Touch Performance in the Large.},
  booktitle = {Proc. of Mobile HCI 2011 (International Conference on Human Computer Interaction with Mobile Devices and Services), Winner of Best Paper Award, ACM, 10 pages.},
  year      = {2011},
  doi       = {10.1145/2037373.2037395},
  url       = {/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2011/2011_henze_mobilehci2011.pdf},
}

@InProceedings{rumelin_naviradar:_2011,
  author    = {R{\"u}melin, Sonja and Rukzio, Enrico and Hardy, Robert},
  title     = {NaviRadar: A Novel Tactile Information Display for Pedestrian Navigation.},
  booktitle = {Proc. of UIST 2011 (Annual ACM Symposium on User Interface Software and Technology), ACM, 10 pages.},
  year      = {2011},
  doi       = {10.1145/2047196.2047234},
  url       = {/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2011/2011_ruemelin_uist2011.pdf},
  web_url2  = {http://youtu.be/-wyRovwgzH0},
}

@InProceedings{hardy_mystate:_2011,
  author    = {Hardy, Robert and Rukzio, Enrico and Holleis, Paul and Wagner, Matthias},
  title     = {Mystate: sharing social and contextual information through touch interactions with tagged objects.},
  booktitle = {Proc. of Mobile HCI 2011 (International Conference on Human Computer Interaction with Mobile Devices and Services), ACM, 10 pages.},
  year      = {2011},
  doi       = {10.1145/2037373.2037444},
  url       = {/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2011/2011_hardy_mobilehci.pdf},
  web_url2  = {http://youtu.be/RJuGnhpIKBY},
}

@InProceedings{seifert_mobidev:_2011,
  author    = {Seifert, Julian and Pfleging, Bastian and del Carmen Valderrama Baham{\'o}ndez, Elba and Hermes, Martin and Rukzio, Enrico and Schmidt, Albrecht},
  title     = {Mobidev: a tool for creating apps on mobile phones.},
  booktitle = {Proc. of Mobile HCI 2011 (International Conference on Human Computer Interaction with Mobile Devices and Services), ACM, 4 pages.},
  year      = {2011},
  doi       = {10.1145/2037373.2037392},
  url       = {/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2012-Seifert-MobiDev.pdf},
}

@Article{Dachselt:2012:PPF:2090150.2090158,
  author    = {Dachselt, Raimund and Hakkila, Jonna and Jones, Matt and L{\"o}chtefeld, Markus and Rohs, Michael and Rukzio, Enrico},
  title     = {Pico projectors: firefly or bright future?},
  journal   = {ACM Interactions},
  year      = {2012},
  volume    = {19},
  number    = {2},
  pages     = {24--29},
  address   = {New York, NY, USA},
  doi       = {10.1145/2090150.2090158},
  publisher = {ACM},
  url       = {/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2012/2012_dachselt_interactions.pdf},
}

@Article{rukzio_personal_2012,
  author  = {Rukzio, Enrico and Holleis, Paul and Gellersen, Hans},
  title   = {Personal projectors for pervasive computing},
  journal = {Pervasive Computing, IEEE},
  year    = {2012},
  volume  = {11},
  number  = {2},
  pages   = {30\&ndash;37},
  doi     = {10.1109/MPRV.2011.17},
  url     = {/fileadmin/website_uni_ulm/iui.inst.100/institut/mitarbeiterbereiche/rukzio/publications/PersonalProjectorsForPervasiveComputing.pdf},
}

@Conference{winkler_investigating_2012,
  author    = {Winkler, Christian and Pfeuffer, Ken and Rukzio, Enrico},
  title     = {Investigating mid-air pointing interaction for projector phones.},
  booktitle = {Proc. of ITS 2012 (ACM International Conference on Interactive Tabletops and Surfaces), ACM, 10 pages.},
  year      = {2012},
  doi       = {10.1145/2396636.2396650},
  web_url2  = {http://youtu.be/r51z70PRb0M},
}

@InCollection{Struse2012,
  author    = {Struse, Eric and Seifert, Julian and Uellenbeck, Sebastian and Rukzio, Enrico and Wolf, Christopher},
  title     = {PermissionWatcher: Creating User Awareness of Application Permissions in Mobile Systems.},
  booktitle = {In Proc. of AMI 2012 (International Joint Conference on Ambient Intelligence), Springer, 16 pages.},
  year      = {2012},
  doi       = {10.1007/978-3-642-34898-3_5},
  url       = {/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2012-Struse-PermissionWatcher.pdf},
}

@InCollection{Bial2012,
  author    = {Bial, Dominik and Appelmann, Thorsten and Rukzio, Enrico and Schmidt, Albrecht},
  title     = {Improving Cyclists Training with Tactile Feedback on Feet},
  booktitle = {In Proc. of Haid 2012 (Haptic and Audio Interaction Design), Springer, 10 pages.},
  year      = {2012},
  doi       = {10.1007/978-3-642-32796-4_5},
  url       = {/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2012/2012_bial_haid2012.pdf},
}

@InProceedings{Schneider:2012,
  author    = {Schneider, Dennis and Seifert, Julian and Rukzio, Enrico},
  title     = {MobIES: extending mobile interfaces using external screens.},
  booktitle = {In Proc. of MUM 2112 (International Conference on Mobile and Ubiquitous Multimedia), ACM, 2 Pages.},
  year      = {2012},
  doi       = {10.1145/2406367.2406438},
  url       = {/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2012/2012_schneider_mum2012.pdf},
  web_url2  = {http://youtu.be/dZaCNV64ltk},
}

@InProceedings{Seifert:2012a,
  author    = {Seifert, Julian and De Luca, Alexander and Rukzio, Enrico},
  title     = {Don't queue up!: user attitudes towards mobile interactions with public terminals.},
  booktitle = {In Proc. of MUM 2012 (International Conference on Mobile and Ubiquitous Multimedia), ACM, 4 pages.},
  year      = {2012},
  doi       = {10.1145/2406367.2406422},
  url       = {/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/MobileATM-Paper-CameraReady.pdf},
  web_url2  = {http://youtu.be/zt-RVzQMsI4},
}

@InProceedings{Seifert:2012,
  author    = {Seifert, Julian and Simeone, Adalberto and Schmidt, Dominik and Holleis, Paul and Reinartz, Christian and Wagner, Matthias and Gellersen, Hans and Rukzio, Enrico},
  title     = {MobiSurf: improving co-located collaboration through integrating mobile devices and interactive surfaces.},
  booktitle = {In Proc. of ITS 2012 (ACM International Conference on Interactive tabletops and Surfaces), ACM, 10 pages.},
  year      = {2012},
  doi       = {10.1145/2396636.2396644},
  url       = {/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/MobiSurf.pdf},
  web_url2  = {http://youtu.be/u-TAwIZXXwo},
}

@InProceedings{Schmidt:2012,
  author    = {Schmidt, Dominik and Seifert, Julian and Rukzio, Enrico and Gellersen, Hans},
  title     = {A cross-device interaction style for mobiles and surfaces},
  booktitle = {In Proc. of DIS '12 (Designing Interactive Systems Conference), ACM, 10 pages.},
  year      = {2012},
  doi       = {10.1145/2317956.2318005},
  url       = {/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2012-Schmidt-D-A-Cross-Device-Interaction-Style-for-Mobiles-and-Surfaces.pdf},
  web_url2  = {http://youtu.be/Z01Xh23X2mc},
}

@InProceedings{winkler_wall_2012,
  author    = {Winkler, Christian and Hutflesz, Patrick and Holzmann, Clemens and Rukzio, Enrico},
  title     = {Wall Play: a novel wall/floor interaction concept for mobile projected gaming.},
  booktitle = {In Proc. of Mobile HCI 2012 (14th International Conference on Human-Computer Interaction with Mobile Devices and Services), ACM, 4 pages.},
  year      = {2012},
  doi       = {10.1145/2371664.2371687},
  url       = {http://mobivis.labs-exit.de/AcceptedPapers/mobivis2012_Hutflesz_et_al.pdf},
}

@InProceedings{henze_observational_2012,
  author    = {Henze, Niels and Rukzio, Enrico and Boll, Susanne},
  title     = {Observational and experimental investigation of typing behaviour using virtual keyboards for mobile devices.},
  booktitle = {Proc. of CHI 2012 (ACM Annual Conference on Human Factors in Computing Systems), ACM, 10 pages.},
  year      = {2012},
  doi       = {10.1145/2207676.2208658},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/mitarbeiter/wolf/awards/henze_observational_2012.pdf},
  web_url2  = {http://youtu.be/Til1bC23Pic},
}

@Article{Seifert:2013a,
  author   = {Seifert, Julian and Dobbelstein, David and Schmidt, Dominik and Holleis, Paul and Rukzio, Enrico},
  title    = {From the Private Into the Public: Privacy-Respecting Mobile Interaction Techniques for Sharing Data on Surfaces.},
  journal  = {Personal and Ubiquitous Computing, Springer, 14 pages},
  year     = {2013},
  doi      = {10.1007/s00779-013-0667-x},
  language = {English},
  url      = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2013/2013-Seifert-et-al_FromThePrivateIntoThePublic.pdf},
  web_url  = {https://www.uni-ulm.de/index.php?id=47570},
  web_url2 = {http://youtu.be/nKI-3cAhgVs},
}

@Article{RaderHRS2013,
  author    = {Rader, Markus and Holzmann, Clemens and Rukzio, Enrico and Seifert, Julian},
  title     = {MobiZone: Personalized Interaction with Multiple Items on Interactive Surfaces},
  journal   = {International Conference on Mobile and Ubiquitous Multimedia - MUM '13},
  year      = {2013},
  address   = {New York, NY, USA},
  publisher = {ACM},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2013/Rader-at-al-MUM13.pdf},
}

@Article{SimeoneSSHRG2013,
  author    = {Simeone, Adalberto L. and Seifert, Julian and Schmidt, Dominik and Holleis, Paul and Rukzio, Enrico and Gellersen, Hans},
  title     = {A Cross-Device Drag-and-Drop Technique},
  journal   = {International Conference on Mobile and Ubiquitous Multimedia - MUM '13},
  year      = {2013},
  address   = {New York, NY, USA},
  publisher = {ACM},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2013/Simeone_et_al_MUM2013.pdf},
  web_url   = {http://youtu.be/mWsp3XksNYs},
}

@Article{WinklerSRKR2013,
  author   = {Winkler, Christian and Seifert, Julian and Reinartz, Christian and Krahmer, Pascal and Rukzio, Enrico},
  title    = {Penbook: Bringing Pen+Paper Interaction to a Tablet Device to Facilitate Paper-Based Workflows in the Hospital Domain},
  journal  = {Proc. of ITS 2013 (ACM International Conference on Interactive Tabletops and Surfaces), ACM, 4 pages [Winner of Best Note Award]},
  year     = {2013},
  doi      = {10.1145/2512349.2512797},
  url      = {http://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2013/its128na-winkler.pdf},
  web_url  = {https://www.uni-ulm.de/index.php?id=85272},
  web_url2 = {http://www.youtube.com/watch?v=4D207Er3H8Q},
}

@InProceedings{Seifert2013e,
  author    = {Seifert, Julian and Packeiser, Markus and Rukzio, Enrico},
  title     = {Adding Vibrotactile Feedback to Large Interactive Surfaces},
  booktitle = {In Proc. of Interact 2013 (IFIP TC13 Conference on Human-Computer Interaction), Springer.},
  year      = {2013},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2013/2013-Seifert-et-al_VibrotactileFeedback.pdf},
  web_url2  = {http://youtu.be/0DzTtBTeglQ},
}

@InProceedings{Seifert2013d,
  author    = {Seifert, Julian and Schneider, Dennis and Rukzio, Enrico},
  title     = {MoCoShoP: Supporting Mobile and Collaborative Shopping and Planning of Interiors},
  booktitle = {In Proc. of Interact 2013 (IFIP TC13 Conference on Human-Computer Interaction), Springer, 8 pages.},
  year      = {2013},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2013/2013-Seifert-et-al_MoCoShoP.pdf},
}

@InProceedings{Seifert2013c,
  author    = {Seifert, Julian and Schneider, Dennis and Rukzio, Enrico},
  title     = {Extending Mobile Interfaces with External Screens},
  booktitle = {In Proc. of Interact 2013 (IFIP TC13 Conference on Human-Computer Interaction), Springer, 8 pages.},
  year      = {2013},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2013/2013-Seifert-et-al_ExtendingMobileInterfaces.pdf},
  web_url2  = {http://youtu.be/dZaCNV64ltk},
}

@InProceedings{Seifert2013b,
  author    = {Seifert, Julian and Bayer, Andreas and Rukzio, Enrico},
  title     = {PointerPhone: Using Mobile Phones for Direct Pointing Interactions with Remote Displays.},
  booktitle = {In Proc. of Interact 2013 (IFIP TC13 Conference on Human-Computer Interaction), Springer, 18 pages.},
  year      = {2013},
  url       = {http://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2013/2013-Seifert-PointerPhone.pdf},
  web_url2  = {http://youtu.be/qp3pIklYLxo},
}

@InProceedings{Weing:2013:PEI:2494091.2494113,
  author    = {Weing, Matthias and R{\"o}hlig, Amrei and Rogers, Katja and Gugenheimer, Jan and Schaub, Florian and K{\"o}nings, Bastian and Rukzio, Enrico and Weber, Michael},
  title     = {P.I.A.N.O.: Enhancing Instrument Learning via Interactive Projected Augmentation},
  booktitle = {Proceedings of UbiComp '13 Adjunct (2013 ACM Conference on Pervasive and Ubiquitous Computing), ACM, 4 pages},
  year      = {2013},
  doi       = {10.1145/2494091.2494113},
  url       = {/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2013/PIANO_UbiComp13.pdf},
  web_url   = {http://youtu.be/4ohSTRXjBNI},
}

@InProceedings{Vaananen-Vainio-Mattila:2013:EIP:2468356.2479665,
  author    = {V{\"a}{\"a}n{\"a}nen-Vainio-Mattila, Kaisa and Hakkila, Jonna and Cassinelli, Alvaro and M{\"u}ller, J{\"o}rg and Rukzio, Enrico and Schmidt, Albrecht},
  title     = {Experiencing interactivity in public spaces (eips).},
  booktitle = {In Extended Abstracts of CHI '13 (ACM Annual Conference on Human Factors in Computing Systems), ACM, 4 pages.},
  year      = {2013},
  doi       = {10.1145/2468356.2479665},
  url       = {/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2013/2013_chi_kaisa.pdf.pdf},
}

@InProceedings{nowacka2013touchbugs,
  author    = {Nowacka, Diana and Ladha, Karim and Hammerla, Nils Y and Jackson, Daniel and Ladha, Cassim and Rukzio, Enrico and Olivier, Patrick},
  title     = {Touchbugs: actuated tangibles on multi-touch tables.},
  booktitle = {In Proc. of CHI 2013 (SIGCHI Conference on Human Factors in Computing Systems), ACM, 4 pages.},
  year      = {2013},
  doi       = {10.1145/2470654.2470761},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/mitarbeiterbereiche/wolf/awards/nowacka2013touchbugs.pdf},
  web_url2  = {http://youtu.be/k4oz_ErsvqM},
}

@Conference{OttoPR2014,
  author    = {Otto, Michael and Prieur, Michael and Rukzio, Enrico},
  title     = {Using Scalable, Interactive Floor Projection for Production Planning Scenario},
  booktitle = {In Adj. Proc. (Poster) of ITS 2014 (Ninth ACM International Conference on Interactive Tabletops and Surfaces)},
  year      = {2014},
  month     = {11},
  day       = {16},
  doi       = {10.1145/2669485.2669547},
  url       = {/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2014/ProductionPlanning_ITS14.pdf},
}

@Conference{SeifertBWSSHMM2014,
  author    = {Seifert, Julian and Boring, Sebastian and Winkler, Christian and Schaub, Florian and Schwab, Fabian and Herrdum, Steffen and Maier, Fabian and Mayer, Daniel and Rukzio, Enrico},
  title     = {Hover Pad: Interacting with Autonomous and Self-Actuated Displays in Space},
  booktitle = {Proceedings of ACM Symposium on User Interface Software and Technology (UIST)},
  year      = {2014},
  month     = {10},
  day       = {5},
  doi       = {10.1145/2642918.2647385},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2014/Seifert-et-al.-HoverPad.pdf},
  web_url   = {https://www.youtube.com/watch?v=qAS6EC7cvU8},
  web_url2  = {https://www.uni-ulm.de/index.php?id=56454},
}

@Article{winkler2014_Springer_TischbasierteProjektion,
  author  = {Winkler, Christian and Rukzio, Enrico},
  title   = {Projizierte tischbasierte Benutzungsschnittstellen},
  journal = {Informatik-Spektrum},
  year    = {2014},
  pages   = {1-5},
  month   = {5},
  day     = {28},
  doi     = {10.1007/s00287-014-0803-7},
  url     = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2014/Winkler_Rukzio_2014_Projizierte_tischbasierte_Benutzungsschnittstellen.pdf},
}

@InProceedings{Winkler:2014CHIa,
  author    = {Winkler, Christian and Seifert, Julian and Dobbelstein, David and Rukzio, Enrico},
  title     = {Pervasive Information through Constant Personal Projection: The Ambient Mobile Pervasive Display (AMP-D)},
  booktitle = {Proc. of CHI 2014 (SIGCHI Conference on Human Factors in Computing Systems), ACM, 10 pages, Honorable Mention Award},
  year      = {2014},
  month     = {4},
  doi       = {10.1145/2556288.2557365},
  url       = {/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2014/winkler2168-AMP-D.pdf},
  web_url   = {https://www.uni-ulm.de/index.php?id=85269},
  web_url2  = {http://www.youtube.com/watch?v=ahG06CERqAI},
}

@InProceedings{Winkler:2014CHIb,
  author     = {Winkler, Christian and Loechtefeld, Markus and Dobbelstein, David and Krueger, Antonio and Rukzio, Enrico},
  title      = {SurfacePhone: A Mobile Projection Device for Single- and Multiuser Everywhere Tabletop Interaction},
  booktitle  = {Proc. of CHI 2014 (SIGCHI Conference on Human Factors in Computing Systems), ACM, 10 pages},
  year       = {2014},
  month      = {4},
  doi        = {10.1145/2556288.2557075},
  event_name = {CHI 2014},
  url        = {/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2014/winkler672-surfacephone.pdf},
  web_url    = {https://www.uni-ulm.de/index.php?id=85270},
  web_url2   = {http://www.youtube.com/watch?v=DKofzCI7Yfw},
}

@InProceedings{Schaub:2014,
  author    = {Schaub, Florian and Seifert, Julian and Honold, Frank and Mueller, Michael and Rukzio, Enrico and Weber, Michael},
  title     = {Broken Display = Broken Interface? The Impact of Display Damage on Smartphone Interaction},
  booktitle = {In Proc. of CHI 2014 (SIGCHI Conference on Human Factors in Computing Systems)},
  year      = {2014},
  doi       = {10.1145/2556288.2557067},
  url       = {fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2014/Schaub-et-al.-BrokenInterfaces-CHI14.pdf},
  web_url   = {https://youtu.be/KmNuaavfyG8},
}

@InProceedings{Gugenheimer:2014:UIP:2669485.2669537,
  author    = {Gugenheimer, Jan and Knierim, Pascal and Seifert, Julian and Rukzio, Enrico},
  title     = {UbiBeam: An Interactive Projector-Camera System for Domestic Deployment},
  booktitle = {Proceedings of the Ninth ACM International Conference on Interactive Tabletops and Surfaces},
  year      = {2014},
  series    = {ITS '14},
  pages     = {305--310},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2669485.2669537},
  url       = {/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2014/UbiBeam_ITS14.pdf},
  web_url   = {https://www.youtube.com/watch?v=t-2ddmX5s2M},
  web_url2  = {http://doi.acm.org/10.1145/2669485.2669537},
}

@InProceedings{Rogers:2014:PFP:2669485.2669514,
  author    = {Rogers, Katja and R{\"o}hlig, Amrei and Weing, Matthias and Gugenheimer, Jan and K{\"o}nings, Bastian and Klepsch, Melina and Schaub, Florian and Rukzio, Enrico and Seufert, Tina and Weber, Michael},
  title     = {P.I.A.N.O.: Faster Piano Learning with Interactive Projection},
  booktitle = {Proceedings of the Ninth ACM International Conference on Interactive Tabletops and Surfaces},
  year      = {2014},
  series    = {ITS '14},
  pages     = {149--158},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2669485.2669514},
  url       = {/fileadmin/website_uni_ulm/iui.inst.100/institut/mitarbeiterbereiche/gugenheimer/piano.pdf},
  web_url   = {https://www.youtube.com/watch?v=4ohSTRXjBNI},
  web_url2  = {http://doi.acm.org/10.1145/2669485.2669514},
}

@Article{UbiBeamInteract,
  author   = {Gugenheimer, Jan and Knierim, Pascal and Winkler, Christian and Seifert, Julian and Rukzio, Enrico},
  title    = {UbiBeam: Exploring the Interaction Space for Home Deployed Projector-Camera Systems},
  journal  = {In Proc. of Interact 2015 (IFIP TC15 Conference on Human-Computer Interaction)},
  year     = {2015},
  month    = {10},
  day      = {14},
  doi      = {10.1007/978-3-319-22698-9_23},
  url      = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2015/UbiBeam.pdf},
  web_url  = {https://youtu.be/t-2ddmX5s2M},
  web_url2 = {https://www.uni-ulm.de/en/in/mi/mi-forschung/research-group-rukzio/projects/ubibeam-an-interactive-projector-camera-system-for-domestic-deployment/},
}

@Article{GeiselhartCMS2015,
  author     = {Geiselhart, Florian and Otto, Michael and Rukzio, Enrico},
  title      = {On the use of Multi-Depth-Camera based Motion Tracking Systems in Production Planning Environments},
  journal    = {In Proc. of CIRP CMS 2015 (48th CIRP Conference on Manufacturing Systems)},
  year       = {2015},
  month      = {6},
  booktitle  = {Proc. of 48th CIRP Conference on Manufacturing Systems - CIRP CMS 2015, 6 pages},
  doi        = {10.1016/j.procir.2015.12.088},
  event_name = {48th CIRP Conference on MANUFACTURING SYSTEMS - CIRP CMS 2015},
  publisher  = {Elsevier},
  url        = {/fileadmin/website_uni_ulm/iui.inst.100/institut/mitarbeiterbereiche/geiselhart/cirp_cms15_geiselhart.pdf},
}

@Conference{winkler_CHI2015_glassunlock,
  author    = {Winkler, Christian and Gugenheimer, Jan and De Luca, Alexander and Haas, Gabriel and Speidel, Philipp and Dobbelstein, David and Rukzio, Enrico},
  title     = {Glass Unlock: Enhancing Security of Smartphone Unlocking through Leveraging a Private Near-eye Display},
  booktitle = {Proc. of CHI 2015 (SIGCHI Conference on Human Factors in Computing Systems)},
  year      = {2015},
  month     = {4},
  doi       = {10.1145/2702123.2702316},
  url       = {http://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2015/Glass_unlock_winkler.pdf},
  web_url   = {https://www.uni-ulm.de/index.php?id=85251},
  web_url2  = {https://www.youtube.com/watch?v=LqfbVckVUNs},
}

@Conference{dobbelstein_CHI2015_belt,
  author    = {Dobbelstein, David and Hock, Philipp and Rukzio, Enrico},
  title     = {Belt: An Unobtrusive Touch Input Device for Head-worn Displays},
  booktitle = {Proc. of CHI 2015 (SIGCHI Conference on Human Factors in Computing Systems)},
  year      = {2015},
  month     = {4},
  doi       = {10.1145/2702123.2702450},
  url       = {http://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut//Papers/Prof_Rukzio/2015/Belt.pdf},
  web_url   = {https://www.uni-ulm.de/index.php?id=85243},
  web_url2  = {https://www.youtube.com/watch?v=o0a46fhmBS8},
}

@Article{OctiCamPoster,
  author   = {Wolf, Dennis and Gugenheimer, Jan and Rukzio, Enrico},
  title    = {OctiCam: An immersive and mobile video communication device for parents and children},
  journal  = {Proc. of ISCT 2015 (1st International Symposium on Companion-Technology)},
  year     = {2015},
  language = {English},
  url      = {http://vts.uni-ulm.de/docs/2015/9771/vts_9771_14853.pdf},
  web_url  = {http://vts.uni-ulm.de/doc.asp?id=9771},
}

@Article{ColorSnakes,
  author     = {Gugenheimer, Jan and De Luca, Alexander and Hess, Hayato and Karg, Stefan and Wolf, Dennis and Rukzio, Enrico},
  title      = {ColorSnakes: Using Colored Decoys to Secure Authentication in Sensitive Contexts},
  journal    = {In Proc. of MobileHCI 2015 (17th International Conference on Human-Computer Interaction with Mobile Devices and Services)},
  year       = {2015},
  doi        = {10.1145/2785830.2785834},
  event_date = {2015},
  url        = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/mitarbeiterbereiche/gugenheimer/ColorSnakes_CameraReady.pdf},
  web_url    = {https://youtu.be/hz1MYrhhj1Y},
}

@Conference{RukzioGAO2015,
  author      = {Otto, Michael and Agethen, Philipp and Geiselhart, Florian and Rukzio, Enrico},
  title       = {Towards ubiquitous tracking: Presenting a scalable, markerless tracking approach using multiple depth cameras},
  booktitle   = {In Proc. of EuroVR 2015 (European Association for Virtual Reality and Augmented Reality), Best Industrial Paper award},
  year        = {2015},
  event_name  = {EuroVR 2015},
  event_place = {Milano},
  journal     = {In Proc. of EuroVR 2015 (European Association for Virtual Reality and Augmented Reality), Best Industrial Paper award},
  url         = {/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2015/OttoEtAl_Towards_ubiquituous_tracking.pdf},
}

@Article{ZihslerHWDSSR2016,
  author  = {Zihsler, Jens and Hock, Philipp and Walch, Marcel and Dzuba, Kirill and Schwager, Denis and Szauer, Patrick and Rukzio, Enrico},
  title   = {Carvatar: Increasing Trust in Highly-Automated Driving Through Social Cues},
  journal = {Adjunct Proceedings of the 8th International Conference on Automotive User Interfaces and Interactive Vehicular Applications (AutomotiveUI '16 Adjunct)},
  year    = {2016},
  month   = {10},
  doi     = {10.1145/3004323.3004354\%20},
  url     = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/mitarbeiterbereiche/hock/Publications/carvatar_zihsler_109.pdf},
}

@Article{GyroVR,
  author     = {Gugenheimer, Jan and Wolf, Dennis and Eiriksson, Eythor and Maes, Pattie and Rukzio, Enrico},
  title      = {GyroVR: Simulating Inertia in Virtual Reality using Head Worn Flywheels},
  journal    = {In Proceedings UIST 2016 (ACM Symposium on User Interface Software and Technology)},
  year       = {2016},
  month      = {10},
  doi        = {10.1145/2984511.2984535},
  event_date = {October 2016},
  url        = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/mitarbeiterbereiche/gugenheimer/ForceVRCamRead_ey.pdf},
  web_url    = {https://youtu.be/RPWsaIYYI6g},
}

@Article{ftUIST,
  author   = {Gugenheimer, Jan and Dobbelstein, David and Winkler, Christian and Haas, Gabriel and Rukzio, Enrico},
  title    = {FaceTouch: Enabling Touch Interaction in Display Fixed UIs for Mobile Virtual Reality},
  journal  = {In Proceedings of UIST 2016 (ACM Symposium on User Interface Software and Technology)},
  year     = {2016},
  month    = {10},
  doi      = {10.1145/2984511.2984576},
  url      = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2016/Facetouch.pdf},
  web_url  = {https://www.uni-ulm.de/en/in/mi/mi-forschung/research-group-rukzio/projects/facetouch-touch-interaction-for-mobile-virtual-reality/},
  web_url2 = {https://youtu.be/MHbN9lseHYE},
}

@Article{AgethenCARV2016,
  author  = {Agethen, Philipp and Otto, Michael and Gaisbauer, Felix and Rukzio, Enrico},
  title   = {Presenting a Novel Motion Capture-based Approach for Walk Path Segmentation and Drift Analysis in Manual Assembly},
  journal = {In Proc. of 6th Conference on Changeable, Agile, Reconfigurable and Virtual Production (CARV)},
  year    = {2016},
  number  = {52},
  month   = {9},
  doi     = {10.1016/j.procir.2016.07.048},
  url     = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2016/Motion_Capture-based_Walk_Path_Segmentation_and_Drift_Analysis.pdf},
}

@Article{TremorDemo,
  author  = {Plaumann, Katrin and Babic, Milos and Drey, Tobias and Hepting, Witali and Stoo{\ss}, Daniel and Rukzio, Enrico},
  title   = {Towards Improving Touchscreen Input Speed and Accuracy on Smartphones for Tremor Affected Persons},
  journal = {In Adj. Proc. (Demo) of Ubicomp 2016 (ACM International Joint Conference on Pervasive and Ubiquitous Computing)},
  year    = {2016},
  month   = {9},
  doi     = {10.1145/2968219.2971396},
  url     = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2016/TremoUbicompDemo2016.pdf},
}

@Article{SwiVRDemo,
  author  = {Gugenheimer, Jan and Wolf, Dennis and Haas, Gabriel and Krebs, Sebastian and Rukzio, Enrico},
  title   = {A Demonstration of SwiVRChair: A Motorized Swivel Chair to Nudge Users\&rsquo; Orientation for 360 Degree Storytelling in Virtual Reality},
  journal = {In Adj. Proc. (Demo) of Ubicomp 2016 (ACM International Joint Conference on Pervasive and Ubiquitous Computing),},
  year    = {2016},
  month   = {9},
  doi     = {10.1145/2968219.2971363},
  url     = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2016/SwiVRChairUbicompDemo2016.pdf},
}

@Article{EyeVR2016,
  author  = {Geiselhart, Florian and Rietzler, Michael and Rukzio, Enrico},
  title   = {EyeVR - Low Cost VR Eye Interaction},
  journal = {In Adj. Proc. (Demo) of Ubicomp 2016 (ACM International Joint Conference on Pervasive and Ubiquitous Computing)},
  year    = {2016},
  month   = {9},
  doi     = {10.1145/2968219.2971384},
  url     = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2016/EyeVRUbicompDemo2016.pdf},
}

@Article{CircularSelection,
  author   = {Plaumann, Katrin and M{\"u}ller, Michael and Rukzio, Enrico},
  title    = {Circular Selection: Optimizing List Selection for Smartwatches},
  journal  = {In Proc. of ISWC 2016 (International Symposium on Wearable Computers)},
  year     = {2016},
  month    = {9},
  doi      = {10.1145/2971763.2971766},
  extern   = {1},
  reviewed = {1},
  url      = {www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/mitarbeiter/plaumann/p128-plaumann.pdf},
  web_url  = {http://www.uni-ulm.de/en/in/mi/mi-forschung/research-group-rukzio/projects/circularselection/},
  web_url2 = {https://youtu.be/z71Xebwu2oo},
}

@Article{AgethenOMR2016,
  author  = {Agethen, Philipp and Otto, Michael and Mengel, Stefan and Rukzio, Enrico},
  title   = {Using Marker-less Motion Capture Systems for Walk Path Analysis in Paced Assembly Flow Lines},
  journal = {In Proc. of 6th Conference on Learning Factories (CLF)},
  year    = {2016},
  month   = {7},
  doi     = {10.1016/j.procir.2016.04.125},
  url     = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2016/Marker-less_Motion_Capture_Systems_for_Walk_Path_Analysis.pdf},
}

@Article{RietzlerFusionKit2016,
  author   = {Rietzler, Michael and Geiselhart, Florian and Thomas, Janek and Rukzio, Enrico},
  title    = {FusionKit: A Generic Toolkit for Skeleton, Marker and Rigid-Body Tracking},
  journal  = {In Proc. of EICS 2016 (8th ACM SIGCHI Symposium on Engineering Interactive Computing Systems)},
  year     = {2016},
  month    = {6},
  doi      = {10.1145/2933242.2933263},
  extern   = {1},
  reviewed = {1},
  url      = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2016/FusionKit.pdf},
  web_url  = {https://youtu.be/ZQoDrKShPfk},
  web_url2 = {https://www.uni-ulm.de/index.php?id=85220},
}

@Article{MultiUserGestures,
  author  = {Plaumann, Katrin and Lehr, David and Rukzio, Enrico},
  title   = {Who Has the Force? Solving Conflicts for Multi User Mid-Air Gestures for TVs},
  journal = {In Proc. of TVX 2016 (ACM International Conference on Interactive Experiences for Television and Online Video)},
  year    = {2016},
  month   = {6},
  doi     = {10.1145/2932206.2932208},
  url     = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2016/WhoHasTheForceTVX2016.pdf},
  web_url = {https://www.uni-ulm.de/index.php?id=85223},
}

@Article{SwiVRchair,
  author     = {Gugenheimer, Jan and Wolf, Dennis and Haas, Gabriel and Krebs, Sebastian and Rukzio, Enrico},
  title      = {SwiVRChair: A Motorized Swivel Chair to Nudge Users' Orientation for 360 Degree Storytelling in Virtual Reality},
  journal    = {In Proc. of CHI 2016 (SIGCHI Conference on Human Factors in Computing Systems)},
  year       = {2016},
  month      = {5},
  doi        = {10.1145/2858036.2858040},
  event_date = {2016},
  url        = {http://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/mitarbeiterbereiche/wolf/p1996-gugenheimer.pdf},
  web_url    = {https://youtu.be/qRSqbCHiMS4},
}

@Article{FaceTouchDemo,
  author   = {Gugenheimer, Jan and Dobbelstein, David and Winkler, Christian and Haas, Gabriel and Rukzio, Enrico},
  title    = {FaceTouch: Touch Interaction for Mobile Virtual Reality},
  journal  = {In Adj. Proc. (Demo) of CHI 2016 (SIGCHI Conference on Human Factors in Computing Systems)},
  year     = {2016},
  month    = {5},
  doi      = {10.1145/2851581.2890242},
  url      = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2016/FaceTouchGugenheimer.pdf},
  web_url  = {https://www.uni-ulm.de/en/in/mi/mi-forschung/research-group-rukzio/projects/facetouch-touch-interaction-for-mobile-virtual-reality/},
  web_url2 = {https://youtu.be/tvAjOvXB56c},
}

@Article{WatchNavigation,
  author  = {Dobbelstein, David and Henzler, Philipp and Rukzio, Enrico},
  title   = {Unconstrained Pedestrian Navigation based on Vibro-tactile Feedback around the Wristband of a Smartwatch},
  journal = {In Adj. Proc. (Poster) of CHI 2016 (SIGCHI Conference on Human Factors in Computing Systems)},
  year    = {2016},
  month   = {5},
  doi     = {10.1145/2851581.2892292},
  url     = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2016/UnconstrainedDobbelstein.pdf},
}

@Article{CATS2016,
  author  = {Otto, Michael and Prieur, Michael and Agethen, Philipp and Rukzio, Enrico},
  title   = {Dual Reality for Production Verification Workshops: A Comprehensive Set of Virtual Methods},
  journal = {In Proc. of 6th CIRP Conference on Assembly Technologies and Systems (CATS)},
  year    = {2016},
  month   = {5},
  doi     = {doi:10.1016/j.procir.2016.02.140},
  url     = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2016/DualRealityOtto.pdf},
}

@Article{SmarTVision,
  author    = {Gugenheimer, Jan and Honold, Frank and Wolf, Dennis and Sch{\"u}ssel, Felix and Seifert, Julian and Weber, Michael and Rukzio, Enrico},
  title     = {How Companion-Technology can Enhance a Multi-Screen Television Experience: A Test Bed for Adaptive Multimodal Interaction in Domestic Environments},
  journal   = {KI - K{\"u}nstliche Intelligenz},
  year      = {2016},
  month     = {2},
  address   = {Berlin Heidelberg},
  doi       = {10.1007/s13218-015-0395-7},
  language  = {English},
  publisher = {Springer},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/mitarbeiterbereiche/rukzio/publications/GugenheimerCompanion2015.pdf},
  web_url   = {https://youtu.be/srODhHgU3LA},
}

@Conference{moteye,
  author     = {Dobbelstein, David and Walch, Marcel and K{\"o}ll, Andreas and Şahin, {\"O}mer and Hartmann, Tamino and Rukzio, Enrico},
  title      = {Reducing In-Vehicle Interaction Complexity: Gaze-Based Mapping of a Rotary Knob to Multiple Interfaces},
  booktitle  = {In Adj. Proc. (Poster) of MUM 2016 (International Conference on Mobile and Ubiquitous Multimedia)},
  year       = {2016},
  event_name = {International Conference on Mobile and Ubiquitous Multimedia},
  url        = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2016/Reducing_In-Vehicle_Interaction_Complexity_-_Gaze-Based_Mapping_of_a_Rotary_Knob_to_Multiple_Interfaces.pdf},
}

@Article{inscent,
  author      = {Dobbelstein, David and Herrdum, Steffen and Rukzio, Enrico},
  title       = {inScent: a Wearable Olfactory Display as an Amplification for Mobile Notifications},
  journal     = {Proc. of ISWC 2017 (The International Symposium on Wearable Computers), ACM, 8 pages},
  year        = {2017},
  month       = {9},
  booktitle   = {The International Symposium on Wearable Computers (ISWC 2017)},
  doi         = {10.1145/3123021.3123035},
  event_name  = {The International Symposium on Wearable Computers (ISWC 2017)},
  event_place = {Maui},
  state       = {accepted},
  url         = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/mitarbeiterbereiche/dobbelstein/inScent_a_Wearable_Olfactory_Display.pdf},
  web_url     = {https://www.youtube.com/watch?v=pzugi0AHaJs},
}

@Article{smartwatchstudy,
  author  = {Dobbelstein, David and Haas, Gabriel and Rukzio, Enrico},
  title   = {The Effects of Mobility, Encumbrance, and (Non-)Dominant Hand on Interaction with Smartwatches},
  journal = {Proc. of ISWC 2017 (The International Symposium on Wearable Computers), ACM, 4 pages},
  year    = {2017},
  month   = {9},
  doi     = {10.1145/3123021.3123033},
  state   = {accepted},
  url     = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/mitarbeiterbereiche/dobbelstein/The_Effects_of_Encumbrance_Mobility_and_Hand_On_Smartwatch_Interaction.pdf},
}

@Article{icme_ipa,
  author  = {Gaisbauer, Felix and Agethen, Philipp and Lunde, Ruediger and Rukzio, Enrico},
  title   = {Iterative Path Adaption (IPA): Predictive Trajectory-Estimation Using Static Pathfinding Algorithms},
  journal = {Proc. of 11th CIRP Conference on Intelligent Computation in Manufacturing Engineering (ICME)},
  year    = {2017},
  month   = {7},
  state   = {accepted},
}

@Article{icme_interact,
  author  = {Agethen, Philipp and Gaisbauer, Felix and Froehlich, Philipp and Manns, Martin and Rukzio, Enrico},
  title   = {Towards Realistic Walk Path Simulation in Automotive Assembly Lines: A Probabilistic Approach},
  journal = {Proc. of 11th CIRP Conference on Intelligent Computation in Manufacturing Engineering (ICME)},
  year    = {2017},
  month   = {7},
  state   = {accepted},
}

@Article{pocketthumb,
  author   = {Dobbelstein, David and Winkler, Christian and Haas, Gabriel and Rukzio, Enrico},
  title    = {PocketThumb: a Wearable Dual-Sided Touch Interface for Cursor-based Control of Smart-Eyewear},
  journal  = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT), 17 pages},
  year     = {2017},
  month    = {6},
  doi      = {10.1145/3090055},
  url      = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/mitarbeiterbereiche/dobbelstein/pocketThumb.pdf},
  web_url2 = {https://youtu.be/Ep0GUToErJg},
}

@Article{VaiR,
  author  = {Rietzler, Michael and Plaumann, Katrin and Kr{\"a}nzle, Taras and Erath, Marcel and Stahl, Alexander and Rukzio, Enrico},
  title   = {VaiR3: Simulating 3D Airflows in Virtual Reality},
  journal = {In Proc. of CHI 2017 (SIGCHI Conference on Human Factors in Computing Systems)},
  year    = {2017},
  month   = {5},
  url     = {http://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2017/VaiR3.pdf},
}

@Article{shareVR,
  author  = {Gugenheimer, Jan and Stemasov, Evgeny and Frommel, Julian and Rukzio, Enrico},
  title   = {ShareVR: Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users},
  journal = {In Proc. of CHI 2017 (SIGCHI Conference on Human Factors in Computing Systems)},
  year    = {2017},
  month   = {5},
  url     = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2017/ShareVRFinal.pdf},
  web_url = {https://www.youtube.com/watch?v=Uc5fkTFMHr4},
}

@Conference{2017_CHI_Frommel_CanTouchThis,
  author    = {Frommel, Julian and Gugenheimer, Jan and Klein, David and Rukzio, Enrico and Weber, Michael},
  title     = {CanTouchThis: Examining the Effect of Physical Contact in a Mobile Multiplayer Game},
  booktitle = {In Proceedings of CHI EA '17 (CHI '17 Extended Abstracts on Human Factors in Computing Systems)},
  year      = {2017},
  month     = {5},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Weber/2017-CHI-Frommel-CanTouchThis.pdf},
}

@Conference{HockBGR2017,
  author    = {Hock, Philipp and Benedikter, Sebastian and Gugenheimer, Jan and Rukzio, Enrico},
  title     = {CarVR: Enabling In-Car Virtual Reality Entertainment},
  booktitle = {In Proc. of CHI 2017 (SIGCHI Conference on Human Factors in Computing Systems),},
  year      = {2017},
  month     = {5},
  url       = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2017/paper2106.pdf},
}

@Article{BodySign,
  author  = {Gugenheimer, Jan and Plaumann, Katrin and Schaub, Florian and Di Campli San Vito, Patrizia and Duck, Saskia and Rabus, Melanie and Rukzio, Enrico},
  title   = {The Impact of Assistive Technology on Communication Quality Between Deaf and Hearing Individuals},
  journal = {In Proc. of CSCW 2017 (20th ACM Conference on Computer Supported Cooperative Work \& Social Computing)},
  year    = {2017},
  month   = {3},
  url     = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2016/paper181.pdf},
  web_url = {https://youtu.be/dKtg_YBgwdY},
}

@Article{OttoJVRB2017,
  author  = {Otto, Michael and Agethen, Philipp and Geiselhart, Florian and Rietzler, Michael and Gaisbauer, Felix and Rukzio, Enrico},
  title   = {Presenting a Holistic Framework for Scalable, Marker-less Motion Capturing: Skeletal Tracking Performance Analysis, Sensor Fusion Algorithms and Usage in Automotive Industry},
  journal = {Journal of Virtual Reality and Broadcasting},
  year    = {2017},
  volume  = {13},
  number  = {3},
  doi     = {10.20385/1860-2037/13.2016.3},
  url     = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/Papers/Prof_Rukzio/2016/1320163.pdf},
  web_url = {http://www.jvrb.org/past-issues/13.2016/4481},
}

@Article{demopocketthumb,
  author  = {Dobbelstein, David and Winkler, Christian and Haas, Gabriel and Rukzio, Enrico},
  title   = {Demonstration of PocketThumb: a Wearable Dual-Sided Touch Interface for Cursor-based Control of Smart-Eyewear},
  journal = {Proc. of UbiComp/ISWC 2017 Adjunct (Demo), ACM, 4 pages},
  year    = {2017},
  doi     = {10.1145/3123024.3123185},
  state   = {accepted},
  url     = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/mitarbeiterbereiche/dobbelstein/pocketthumb_demo.pdf},
  web_url = {https://youtu.be/Ep0GUToErJg},
}

@Article{inscentdemo,
  author  = {Dobbelstein, David and Herrdum, Steffen and Rukzio, Enrico},
  title   = {Demonstration of inScent: a Wearable Olfactory Display as an Amplification for Mobile Notifications},
  journal = {Proc. of UbiComp/ISWC 2017 Adjunct (Demo), ACM, 4 pages},
  year    = {2017},
  doi     = {10.1145/3123024.3123185},
  state   = {accepted},
  url     = {https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/mitarbeiterbereiche/dobbelstein/inscent_demo.pdf},
  web_url = {https://www.youtube.com/watch?v=pzugi0AHaJs},
}

@InProceedings{6840182,
  author    = {S. Escaida Navarro and D. Weiss and D. Stogl and D. Milev and B. Hein},
  title     = {Tracking and Grasping of Known and Unknown Objects from a Conveyor Belt},
  booktitle = {ISR/Robotik 2014; 41st International Symposium on Robotics},
  year      = {2014},
  pages     = {1-8},
  month     = {June},
  abstract  = {In this paper we present an approach for grasping of objects from a moving conveyor belt. A tracking scheme is used to estimate the velocity of the object, respectively of the conveyor. The scheme we present allows for tracking of unknown rotationally symmetric objects and is based on a robust technique for estimating the axis of these objects adapted from a previous work. Aditionally we use template matching to track previously known objects. For grasping, a trajectory is planned for the gripper that chases the object and is able to synchronize to the object's motion, allowing it to lift the object from the conveyor.},
}

@InProceedings{5876729,
  author    = {T. Tamiya and M. Kawanishi and S. Guo},
  title     = {Skull base surgery using Navigation Microscope Integration system},
  booktitle = {The 2011 IEEE/ICME International Conference on Complex Medical Engineering},
  year      = {2011},
  pages     = {185-187},
  month     = {May},
  abstract  = {Objective: The use of intraoperative navigation system in neurosurgery has increased rapidly. The Neuronavigation Microscope Integration (NMI) system consists of a microscope (Zeiss, Germany) with StealthStation (Medotronic, U.S.A.) including light emitting diodes, a dynamic reference frame with light emitting diodes, an optical digitizer with camera array and a computer workstation. We investigated the usefulness of the NMI system for skull base surgery. Patients and Methods: Between April 2003 and August 2010, we used the NMI system in 136 patients undergoing skull base surgery including transsphenoidal surgery at Kagawa University Hospital. Results: Since the navigational informations could be superimposed into the microscope view, we obtained the accurate locations of tumor and normal anatomical structures before skin incision. During the operations, the surgeons did not need to turn away from the surgical field or to use bulky pointer. For the skull base surgery, the navigational informations were not affected by the brain shift during the operations. The deviations of registration assessment were within 2 mm and the real anatomical deviations were within 3 mm. For the transsphenoidal surgery, pituitary tumors could be removed safety without X-ray imaging. Conclusion: Our findings suggest that the NMI system can provide valuable and reliable intraoperative navigational informations in skull base surgery.},
  doi       = {10.1109/ICCME.2011.5876729},
  keywords  = {biomedical optical imaging;brain;light emitting diodes;navigation;neurophysiology;optical microscopes;surgery;tumours;StealthStation;intraoperative navigation system;light emitting diodes;navigation microscope integration;neurosurgery;optical digitizer;skin incision;skull base surgery;transsphenoidal surgery;tumor;Computers;Indexes;Microscopy;Navigation;Skull;Frameless Neuronavigation;Skull base surgery;imaged-guided surgery},
}

@Article{6967823,
  author   = {R. Kafieh and H. Rabbani and I. Selesnick},
  title    = {Three Dimensional Data-Driven Multi Scale Atomic Representation of Optical Coherence Tomography},
  journal  = {IEEE Transactions on Medical Imaging},
  year     = {2015},
  volume   = {34},
  number   = {5},
  pages    = {1042-1062},
  month    = {May},
  issn     = {0278-0062},
  abstract = {In this paper, we discuss about applications of different methods for decomposing a signal over elementary waveforms chosen in a family called a dictionary (atomic representations) in optical coherence tomography (OCT). If the representation is learned from the data, a nonparametric dictionary is defined with three fundamental properties of being data-driven, applicability on 3D, and working in multi-scale, which make it appropriate for processing of OCT images. We discuss about application of such representations including complex wavelet based K-SVD, and diffusion wavelets on OCT data. We introduce complex wavelet based K-SVD to take advantage of adaptability in dictionary learning methods to improve the performance of simple dual tree complex wavelets in speckle reduction of OCT datasets in 2D and 3D. The algorithm is evaluated on 144 randomly selected slices from twelve 3D OCTs taken by Topcon 3D OCT-1000 and Cirrus Zeiss Meditec. Improvement of contrast to noise ratio (CNR) (from 0.9 to 11.91 and from 3.09 to 88.9, respectively) is achieved. Furthermore, two approaches are proposed for image segmentation using diffusion. The first method is designing a competition between extended basis functions at each level and the second approach is defining a new distance for each level and clustering based on such distances. A combined algorithm, based on these two methods is then proposed for segmentation of retinal OCTs, which is able to localize 12 boundaries with unsigned border positioning error of 9.22 ±3.05 μm, on a test set of 20 slices selected from 13 3D OCTs.},
  doi      = {10.1109/TMI.2014.2374354},
  keywords = {biodiffusion;biomedical optical imaging;eye;feature selection;image denoising;image segmentation;medical image processing;optical tomography;speckle;wavelet transforms;Cirrus Zeiss Meditec;OCT datasets;OCT image processing;Topcon 3D OCT-1000;complex wavelet based K-SVD;contrast-to-noise ratio;data-driven applicability;dictionary learning methods;diffusion wavelets;elementary waveforms;image segmentation;nonparametric dictionary;optical coherence tomography;randomly selected slices;retinal OCT;signal decomposition;simple dual tree complex wavelet performance;speckle reduction;three-dimensional data-driven multiscale atomic representation;unsigned border positioning error;Adaptive optics;Dictionaries;Image segmentation;Noise reduction;Three-dimensional displays;Wavelet transforms;Denoising;dictionary learning;diffusion wavelet;optical coherence tomography;segmentation;three-dimensional (3D) complex wavelet transform;Algorithms;Cluster Analysis;Fourier Analysis;Humans;Imaging, Three-Dimensional;Retina;Retinal Diseases;Tomography, Optical Coherence},
}

@InProceedings{7820989,
  author    = {W. J. Chen},
  title     = {Two dimensional translation detection by comprehensively calculating three cross-correlations},
  booktitle = {2016 Sixth International Conference on Image Processing Theory, Tools and Applications (IPTA)},
  year      = {2016},
  pages     = {1-6},
  month     = {Dec},
  abstract  = {This paper suggests a new method for detecting 2D translation between two images based on calculating three independent cross-correlations (CCs) on them. Such a method is conceptually different from other area based methods which generally perform only one CC or its variants for phase shift detection. The principle of traditional area based methods could be interpreted as a fast but simplified implementation of least squares (LS), by ignoring two summed squares of given images while keeping one CC component between them. It is argued by us that such an ignorance often inevitably results in the requirement of data pre-processing for robustness and accuracy. Keeping all the source information but calculating the whole LS by three CCs, the computation performance is kept as O(N log N). Without any data pre-processing, experiments on a dataset with rich application backgrounds and comparisons with widely recommended methods including both the area based and the feature based methods, show that our suggestion is very promising for general-purpose 2D translation detection.},
  doi       = {10.1109/IPTA.2016.7820989},
  keywords  = {image processing;least squares approximations;2D translation detection;CC;cross-correlation;image content;least squares implementation;phase shift detection;two-dimensional translation detection;Discrete Fourier transforms;Feature extraction;Image registration;Mathematical model;Robustness;Two dimensional displays;2D translation detection;Cross-correlation;Fast Fourier Transform (FFT);Least squares},
}

@InProceedings{7511973,
  author    = {W. J. Chen},
  title     = {Measuring distance by angular domain filtering},
  booktitle = {2015 International Conference on Photonics, Optics and Laser Technology (PHOTOPTICS)},
  year      = {2015},
  volume    = {1},
  pages     = {78-84},
  month     = {March},
  abstract  = {In this paper a paraxial imaging system with incoherent illumination is interpreted as a signal processing system in which a thin lens performs a forward angular transform on light rays traveling through it. Inverse angular transform exists and could be performed by another thin lens which coincides its focus plane with the first. The common focus plane acts as an angular domain, on which filtering is possible by placing aperture stops. A symmetrically angular filtering results in a direct correspondence between a recorded signal and its object distance. Such a “transform-filtering-inverse transform” system could be understood as a modified tele-centric system, with which a novel concept for measuring a distance without focusing on the target is suggested.},
  keywords  = {distance measurement;inverse transforms;optical filters;optical information processing;angular domain filtering;distance measurement;forward angular transform;incoherent illumination;inverse angular transform;light rays;modified tele-centric system;paraxial imaging system;signal processing system;symmetrically angular filtering;thin lens;transform-filtering-inverse transform system;Filtering;Focusing;LED lamps;Lenses;Transforms;Afocal System;Angular Domain filtering;Bi-lateral telecentric optics;Distance measurement;Transform},
}

@InProceedings{Roodaki2015,
  author    = {H. Roodaki and K. Filippatos and A. Eslami and N. Navab},
  title     = {Introducing Augmented Reality to Optical Coherence Tomography in Ophthalmic Microsurgery},
  booktitle = {2015 IEEE International Symposium on Mixed and Augmented Reality},
  year      = {2015},
  pages     = {1-6},
  month     = {Sept},
  abstract  = {Augmented Reality (AR) in microscopic surgery has been subject of several studies in the past two decades. Nevertheless, AR has not found its way into everyday microsurgical workflows. The introduction of new surgical microscopes equipped with Optical Coherence Tomography (OCT) enables the surgeons to perform multimodal (optical and OCT) imaging in the operating room. Taking full advantage of such elaborate source of information requires sophisticated intraoperative image fusion, information extraction, guidance and visualization methods. Medical AR is a unique approach to facilitate utilization of multimodal medical imaging devices. Here we propose a novel medical AR solution to the long-known problem of determining the distance between the surgical instrument tip and the underlying tissue in ophthalmic surgery to further pave the way of AR into the surgical theater. Our method brings augmented reality to OCT for the first time by augmenting the surgeon's view of the OCT images with an estimated instrument cross-section shape and distance to the retinal surface using only information from the shadow of the instrument in intraoperative OCT images. We demonstrate the applicability of our method in retinal surgery using a phantom eye and evaluate the accuracy of the augmented information using a micromanipulator.},
  doi       = {10.1109/ISMAR.2015.15},
  keywords  = {augmented reality;data visualisation;medical image processing;micromanipulators;optical tomography;surgery;augmented reality;image fusion;information extraction;intraoperative OCT image;medical AR solution;micromanipulator;microscopic surgery;multimodal medical imaging device;ophthalmic microsurgery;ophthalmic surgery;optical coherence tomography;retinal surgery;surgical instrument tip;surgical microscope;Biomedical optical imaging;Estimation;Microscopy;Optical microscopy;Retina;Surgery;Augmented reality;instrument cross-section;ophthalmic surgery;optical coherence tomography},
}

@InProceedings{6625302,
  author    = {W. J. Chen},
  title     = {Spectrum magnifier: Zooming into local details in the frequency domain},
  booktitle = {2013 IEEE China Summit and International Conference on Signal and Information Processing},
  year      = {2013},
  pages     = {82-85},
  month     = {July},
  abstract  = {Zooming into a spectrum obtained from the Fast Fourier Transform (FFT) could not be performed directly by linear/cubic interpolations in the frequency domain. A spectrum magnifier is presented in this paper as a two-step procedure: the carrier-wave demodulation and the fractional Fast Fourier Transform1 (frFFT). Particularly for a narrow-band signal within Nyquist limits, the carrier-wave demodulation shifts the band center to zero; afterward the frFFT efficiently investigates into precision details of the near-zero band without any resolution cost either in the time domain or in the frequency domain. Such a cost is frequently compromised from accuracy and efficiency demands in the methods based on traditional techniques of zero-padding, low- or band-pass filtering, sub-sampling, etc.},
  doi       = {10.1109/ChinaSIP.2013.6625302},
  keywords  = {demodulation;fast Fourier transforms;filtering theory;frequency-domain analysis;interpolation;Nyquist limits;band-pass filtering;carrier-wave demodulation;carrier-wave demodulation shifts;frFFT;fractional fast Fourier transform;frequency domain;linear-cubic interpolations;lowpass filtering;narrowband signal;spectrum magnifier;time domain;zero-padding techniques;Convolution;Fast Fourier transforms;Interpolation;Time-domain analysis;Time-frequency analysis;Chirp-z transform;Fast Fourier Transform(FFT);Fractional FFT;Spectrum analysis},
}

@InProceedings{5653010,
  author    = {W. J. Chen},
  title     = {Aligning tilt slices for 3D TEM tomography based on 2D to 1D Radon transform},
  booktitle = {2010 IEEE International Conference on Image Processing},
  year      = {2010},
  pages     = {609-612},
  month     = {Sept},
  abstract  = {Aligning a tilt series in 3D TEM tomography is not only a shift compensation problem among tilt slices, but rather a parameter registration problem, in which a tilt axis should be registered on each slice with its position/orientation. A new method for 3D tilt alignment is suggested in this paper based on analyzing 1D signals resulting from projecting individual 2D slices in various directions (2D Radon transform). The axis orientation and position are identified by techniques of 1D shift compensation, 2D frequency analysis, etc. Without any information loss during the calculating process, the accuracy and the robustness for axis registration are ensured. Meanwhile, essentially working on 1D data provides the concrete base of computing performance. Based on this method, no any extra work of adding fiducial markers into samples, or detecting corners/edges from slice images are required; therefore both experimental and computational efforts could be heavily reduced.},
  doi       = {10.1109/ICIP.2010.5653010},
  issn      = {1522-4880},
  keywords  = {Radon transforms;image registration;tomography;transmission electron microscopy;2D frequency analysis;3D TEM tomography;Radon transform;parameter registration;shift compensation problem;tilt slice alignment;Image reconstruction;Three dimensional displays;Tomography;Transforms;Transmission electron microscopy;Cross-correlation;Fractional Fast Fourier Transform;Radon transform;TEM tomography},
}

@Article{4358881,
  author   = {H. Narasimha-Iyer and V. Mahadevan and J. M. Beach and B. Roysam},
  title    = {Improved Detection of the Central Reflex in Retinal Vessels Using a Generalized Dual-Gaussian Model and Robust Hypothesis Testing},
  journal  = {IEEE Transactions on Information Technology in Biomedicine},
  year     = {2008},
  volume   = {12},
  number   = {3},
  pages    = {406-410},
  month    = {May},
  issn     = {1089-7771},
  abstract = {This updates an earlier publication by the authors describing a robust framework for detecting vasculature in noisy retinal fundus images. We improved the handling of the "central reflex'' phenomenon in which a vessel has a "hollow'' appearance. This is particularly pronounced in dual-wavelength images acquired at 570 and 600 nm for retinal oximetry. It is prominent in the 600 nm images that are sensitive to the blood oxygen content. Improved segmentation of these vessels is needed to improve oximetry. We show that the use of a generalized dual-Gaussian model for the vessel intensity profile instead of the Gaussian yields a significant improvement. Our method can account for variations in the strength of the central reflex, the relative contrast, width, orientation, scale, and imaging noise. It also enables the classification of regular and central reflex vessels. The proposed method yielded a sensitivity of 72% compared to 38% by the algorithm of Can , and 60% by the robust detection based on a single-Gaussian model. The specificity for the methods were 95%, 97%, and 98%, respectively.},
  doi      = {10.1109/TITB.2007.897782},
  keywords = {Gaussian distribution;blood vessels;eye;image segmentation;medical image processing;oximetry;blood oxygen content;central reflex phenomena;dual-wavelength images;generalized dual-Gaussian model;image segmentation;noisy retinal fundus images;retinal oximetry;retinal vessels;vasculature;Dual-wavelength retinal imaging;Vasculature detection and segmentation;dual wavelength retinal imaging;hollow blood vessels;hypothesis testing;mathematical models of vasculature;retinal fundus images;vasculature detection and segmentation},
}

@Article{4273613,
  author   = {H. Narasimha-Iyer and J. M. Beach and B. Khoobehi and B. Roysam},
  title    = {Automatic Identification of Retinal Arteries and Veins From Dual-Wavelength Images Using Structural and Functional Features},
  journal  = {IEEE Transactions on Biomedical Engineering},
  year     = {2007},
  volume   = {54},
  number   = {8},
  pages    = {1427-1435},
  month    = {Aug},
  issn     = {0018-9294},
  abstract = {This paper presents an automated method to identify arteries and veins in dual-wavelength retinal fundus images recorded at 570 and 600 nm. Dual-wavelength imaging provides both structural and functional features that can be exploited for identification. The processing begins with automated tracing of the vessels from the 570-nm image. The 600-nm image is registered to this image, and structural and functional features are computed for each vessel segment. We use the relative strength of the vessel central reflex as the structural feature. The central reflex phenomenon, caused by light reflection from vessel surfaces that are parallel to the incident light, is especially pronounced at longer wavelengths for arteries compared to veins. We use a dual-Gaussian to model the cross-sectional intensity profile of vessels. The model parameters are estimated using a robust -estimator, and the relative strength of the central reflex is computed from these parameters. The functional feature exploits the fact that arterial blood is more oxygenated relative to that in veins. This motivates use of the ratio of the vessel optical densities (ODs) from images at oxygen-sensitive and oxygen-insensitive wavelengths () as a functional indicator. Finally, the structural and functional features are combined in a classifier to identify the type of the vessel. We experimented with four different classifiers and the best result was given by a support vector machine (SVM) classifier. With the SVM classifier, the proposed algorithm achieved true positive rates of 97% for the arteries and 90% for the veins, when applied to a set of 251 vessel segments obtained from 25 dual wavelength images. The ability to identify the vessel type is useful in applications such as automated retinal vessel oximetry and automated analysis of vascular changes without manual intervention.},
  doi      = {10.1109/TBME.2007.900804},
  keywords = {biomedical optical imaging;blood vessels;eye;image segmentation;medical image processing;M-estimator;arterial blood;automated retinal vessel oximetry;dual-Gaussian model;dual-wavelength images;dual-wavelength retinal fundus images;light reflection;retinal arteries;support vector machine classifier;vascular changes;veins;vessel central reflex;wavelength 570 nm;wavelength 600 nm;Arteries;Image segmentation;Optical imaging;Optical reflection;Optical surface waves;Retina;Support vector machine classification;Support vector machines;Surface waves;Veins;Automated image analysis;automatic identification;retinal oximetry;structural and functional features;vessel profile modeling;vessel type;Algorithms;Artificial Intelligence;Humans;Image Enhancement;Image Interpretation, Computer-Assisted;Microscopy, Fluorescence, Multiphoton;Pattern Recognition, Automated;Reproducibility of Results;Retinal Vessels;Sensitivity and Specificity},
}

@Article{4273614,
  author   = {H. Narasimha-Iyer and A. Can and B. Roysam and H. L. Tanenbaum and A. Majerovics},
  title    = {Integrated Analysis of Vascular and Nonvascular Changes From Color Retinal Fundus Image Sequences},
  journal  = {IEEE Transactions on Biomedical Engineering},
  year     = {2007},
  volume   = {54},
  number   = {8},
  pages    = {1436-1445},
  month    = {Aug},
  issn     = {0018-9294},
  abstract = {Algorithms are presented for integrated analysis of both vascular and nonvascular changes observed in longitudinal time-series of color retinal fundus images, extending our prior work. A Bayesian model selection algorithm that combines color change information, and image understanding systems outputs in a novel manner is used to analyze vascular changes such as increase/decrease in width, and disappearance/appearance of vessels, as well as nonvascular changes such as appearance/disappearance of different kinds of lesions. The overall system is robust to false changes due to inter-image and intra-image nonuniform illumination, imaging artifacts such as dust particles in the optical path, alignment errors and outliers in the training-data. An expert observer validated the algorithms on 54 regions selected from 34 image pairs. The regions were selected such that they represented diverse types of vascular changes of interest, as well as no-change regions. The algorithm achieved a sensitivity of 82% and a 9% false positive rate for vascular changes. For the nonvascular changes, 97% sensitivity and a 10% false positive rate are achieved. The combined system is intended for diverse applications including computer-assisted retinal screening, image-reading centers, quantitative monitoring of disease onset and progression, assessment of treatment efficacy, and scoring clinical trials.},
  doi      = {10.1109/TBME.2007.900807},
  keywords = {Bayes methods;biomedical optical imaging;blood vessels;eye;image classification;image colour analysis;image sequences;medical image processing;time series;Bayesian classification;Bayesian model selection algorithm;alignment errors;color retinal fundus image sequences;computer-assisted retinal screening;diabetic retinopathy;disease monitoring;image-reading centers;imaging artifacts;integrated vascular-nonvascular change analysis;longitudinal time-series;nonuniform illumination;retinal image analysis;treatment efficacy assessment;vascular width;Algorithm design and analysis;Bayesian methods;Image analysis;Image color analysis;Image sequence analysis;Image sequences;Information analysis;Lesions;Retina;Time series analysis;Bayesian classification;change analysis;change detection;diabetic retinopathy;illumination correction;retinal image analysis;Algorithms;Artificial Intelligence;Colorimetry;Humans;Image Enhancement;Image Interpretation, Computer-Assisted;Pattern Recognition, Automated;Reproducibility of Results;Retinal Vessels;Retinoscopy;Sensitivity and Specificity;Subtraction Technique;Systems Integration},
}

@InProceedings{7493222,
  author    = {M. Alsheakhali and A. Eslami and N. Navab},
  title     = {Detection of articulated instruments in retinal microsurgery},
  booktitle = {2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)},
  year      = {2016},
  pages     = {107-110},
  month     = {April},
  abstract  = {Instrument detection in retinal microsurgery is still one of the most challenging operations due to illumination changes, fast motion, cluttered background and deformable shape of the instrument. In this work, a new technique is proposed to detect an articulated forceps instrument by modeling it using Conditional Random Field (CRF). The unary potentials of the CRF, which represent the instrument parts, are detected using the deep convolutional neural network, where two probability distribution maps for both the forceps center and its shaft are estimated. The pairwise potentials are modeled using a regression random forest to learn the relation between the instrument parts based on their joint structural features. Sampled combinations from both unary distributions are selected, and each is tested using the regression forest to compute its similarity to the medical instrument structure. The best combination candidate chosen by the CRF predicts the forceps center point (instrument joint point) and the orientation of its shaft. The approach shows high detection accuracy on public datasets and real videos for retinal microsurgery operations.},
  doi       = {10.1109/ISBI.2016.7493222},
  file      = {:ref_downloads/7493222 - Detection of articulated instruments in retinal microsurgery.pdf:PDF},
  keywords  = {Feature extraction;Instruments;Microsurgery;Retina;Shafts;Testing;Conditional Random Field;Deep Learning;Instrument Detection;Retinal Microsurgery},
}

@Article{DELARAM20166,
  author   = {Jalal Delaram and Omid Fatahi Valilai},
  title    = {Development of a Novel Solution to Enable Integration and Interoperability for Cloud Manufacturing},
  journal  = {Procedia CIRP},
  year     = {2016},
  volume   = {52},
  pages    = {6 - 11},
  issn     = {2212-8271},
  note     = {The Sixth International Conference on Changeable, Agile, Reconfigurable and Virtual Production (CARV2016)},
  abstract = {Nowadays, manufacturing enterprises have been faced with a globalized competitive environment. In this fierce condition, Cloud Manufacturing paradigm emerged as a promising concept for competition. It provides effective solutions and tools for manufacturing enterprises to collaborate in globalized market. The revolution that Cloud Manufacturing has created is based on the redefinition of the classic methods to those which are appropriate for todays’ modern and globalized manufacturing environments. In parallel with cloud-based revolution, the expansion of internet-based technologies has been started. These technologies previously have been applied in many fields and resulted in interoperability and integration among different technological solutions. This promotes this paper to fill the gap which of technologies in Cloud Manufacturing solutions as well and enabling integrated and interoperable communication among manufacturing clouds. This paper has focused on the idea of manufacturing cloud integration and interoperability. The paper has studied the dominant Cloud Manufacturing researches to find the opportunities for proposing a novel solution. This solution is capable to resolve the integration and interoperability consideration for different manufacturing clouds. The paper has applied the EDI X12 standards for insuring an integrated and standard data format for its contribution. An example in area of Supply Chain Management will be discussed to show the capabilities of the proposed solution.},
  doi      = {http://dx.doi.org/10.1016/j.procir.2016.07.056},
  keywords = {Cloud Manufacturing, Cloud Integration and Interoperability, Electronic Data Interchange (EDI), Vendor Managed Inventory (VMI).},
  url      = {http://www.sciencedirect.com/science/article/pii/S2212827116308290},
}

@InProceedings{Hardy:2011:RWR:2107596.2107600,
  author    = {Hardy, John and Rukzio, Enrico and Davies, Nigel},
  title     = {Real World Responses to Interactive Gesture Based Public Displays},
  booktitle = {Proceedings of the 10th International Conference on Mobile and Ubiquitous Multimedia},
  year      = {2011},
  series    = {MUM '11},
  pages     = {33--39},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {2107600},
  doi       = {10.1145/2107596.2107600},
  isbn      = {978-1-4503-1096-3},
  keywords  = {attention, deployments, digital signage, engagement, experimentation, gestures, hand control, interaction, public display},
  location  = {Beijing, China},
  numpages  = {7},
  url       = {http://doi.acm.org/10.1145/2107596.2107600},
}

@InBook{Huang2008,
  pages     = {228--243},
  title     = {Overcoming Assumptions and Uncovering Practices: When Does the Public Really Look at Public Displays?},
  publisher = {Springer Berlin Heidelberg},
  year      = {2008},
  author    = {Huang, Elaine M. and Koster, Anna and Borchers, Jan},
  editor    = {Indulska, Jadwiga and Patterson, Donald J. and Rodden, Tom and Ott, Max},
  address   = {Berlin, Heidelberg},
  isbn      = {978-3-540-79576-6},
  abstract  = {This work reports on the findings of a field study examining the current use practices of large ambient information displays in public settings. Such displays are often assumed to be inherently eye-catching and appealing to people nearby, but our research shows that glancing and attention at large displays is complex and dependent on many factors. By understanding how such displays are being used in current, public, non-research settings and the factors that impact usage, we offer concrete, ecologically valid knowledge and design implications about these technologies to researchers and designers who are employing large ambient displays in their work.},
  booktitle = {Pervasive Computing: 6th International Conference, Pervasive 2008 Sydney, Australia, May 19-22, 2008 Proceedings},
  doi       = {10.1007/978-3-540-79576-6_14},
  url       = {https://doi.org/10.1007/978-3-540-79576-6_14},
}

@InProceedings{7528373,
  author    = {J. S. Artal-Sevil and J. L. Montañés},
  title     = {Development of a robotic arm and implementation of a control strategy for gesture recognition through Leap Motion device},
  booktitle = {2016 Technologies Applied to Electronics Teaching (TAEE)},
  year      = {2016},
  pages     = {1-9},
  month     = {June},
  abstract  = {The objective of this paper has been the development of a prototype of articulated robotic arm and the implementation of a control strategy through gesture recognition (Leap Motion Sensor), by means the natural movement of the forearm and hand. The series of advances relative to the control techniques have caused that the robotics it has also introduced as an educational complement in obligatory basic teachings. Final Year Project TFG is an academic task that allows to evaluate the skills and competences acquired by students along their university period. So students, during their development, can implement numerous theoretical bases of an entertaining and fun form. To develop and to control robotic elements locally or remotely, it has always proven to be a clear example of additional motivation on the students. The prototype developed has exceeded the initial expectations and at low cost.},
  doi       = {10.1109/TAEE.2016.7528373},
  file      = {:ref_downloads/7528373 - Development of a robotic arm and implementation of a control strategy for gesture recognition through Leap Motion device.pdf:PDF},
  keywords  = {computer aided instruction;control engineering computing;control engineering education;educational institutions;gesture recognition;manipulators;sensors;teaching;Final Year Project TFG;Leap Motion Sensor;Leap Motion device;articulated robotic arm development;control strategy;educational complement;forearm natural movement;gesture recognition;hand natural movement;obligatory basic teachings;university period;Cameras;Gesture recognition;Manipulators;Robot sensing systems;Software;Three-dimensional displays;Articulated Robotic Arm;Asus Xtion Sensor;Autonomous Mobile Robot;Gesture recognition;Leap Motion Sensor;Microsoft Kinect},
}

@InProceedings{7939924,
  author    = {S. Ameur and A. B. Khalifa and M. S. Bouhlel},
  title     = {A comprehensive leap motion database for hand gesture recognition},
  booktitle = {2016 7th International Conference on Sciences of Electronics, Technologies of Information and Telecommunications (SETIT)},
  year      = {2016},
  pages     = {514-519},
  month     = {Dec},
  abstract  = {The touchless interaction has received considerable attention in recent years with benefit of removing the burden of physical contact. The recent introduction of novel acquisition devices, like the leap motion controller, allows obtaining a very informative description of the hand pose and motion that can be exploited for accurate gesture recognition. In this work, we present an interactive application with gestural hand control using leap motion for medical visualization, focusing on the satisfaction of the user as an important component in the composition of a new specific database. In this paper, we propose a 3D dynamic gesture recognition approach explicitly targeted to leap motion data. Spatial feature descriptors based on the positions of fingertips and palm center are extracted and fed into a support vector machine classifier in order to recognize the performed gestures. The experimental results show the effectiveness of the suggested approach in the recognition of the modeled gestures with a high accuracy rate of about 81%.},
  doi       = {10.1109/SETIT.2016.7939924},
  file      = {:ref_downloads/7939924 - A comprehensive leap motion database for hand gesture recognition.pdf:PDF},
  keywords  = {feature extraction;gesture recognition;image classification;image motion analysis;interactive systems;support vector machines;3D dynamic gesture recognition;hand gesture recognition;interactive application;leap motion database;medical visualization;spatial feature descriptors extraction;support vector machine classifier;Biomedical imaging;Databases;Feature extraction;Gesture recognition;Three-dimensional displays;Thumb;Hand gesture recognition;Leap motion;Support vector machine;Touchless interaction},
}

@InProceedings{7995825,
  author    = {V. John and M. Umetsu and A. Boyali and S. Mita and M. Imanishi and N. Sanma and S. Shibata},
  title     = {Real-time hand posture and gesture-based touchless automotive user interface using deep learning},
  booktitle = {2017 IEEE Intelligent Vehicles Symposium (IV)},
  year      = {2017},
  pages     = {869-874},
  month     = {June},
  abstract  = {In this study, a vision based in-car entertainment user interface is presented. The user interface is designed using a hand posture and gesture recognition algorithm in deep learning framework. The hand posture recognition algorithm is formulated using the convolutional neural network to perform the fundamental tasks in the user interface. The hand gesture recognition algorithm is formulated using the long-term recurrent convolutional neural network to intuitively interact with the touchless automotive user interface in a detailed manner. In the recurrent deep learning framework, typically, the gesture frames are taken from a uniformly sampled image sequence. In this work, the recurrent structure is enhanced using a reduced number of input frames captured from the image sequence. The reduced input frames or key frames represent the action present in the video sequence. Sparse dictionary learning provide reliable key frame extraction from video sequences. However, sparse dictionary learning is computationally expensive, and are individually optimized for every video sequence. In this paper, we propose to approximate sparse dictionary learning using a non-linear regression framework. The multilayer perceptron is utilized to model the non-linear regression framework. The optimal neural network architecture is identified after a detailed evaluation. We evaluate the proposed recognition methods on public datasets. The proposed methods yield a recognition accuracy of 92% and 90% for pose and gestures, respectively. The combined hand posture and gesture recognition takes 82ms which is a reasonable for real time implementation.},
  doi       = {10.1109/IVS.2017.7995825},
  keywords  = {feature extraction;gesture recognition;image sequences;learning (artificial intelligence);multilayer perceptrons;pose estimation;recurrent neural nets;regression analysis;traffic engineering computing;video signal processing;deep learning;frame extraction;gesture-based touchless automotive user interface;hand gesture recognition algorithm;hand posture recognition algorithm;image sequence;multilayer perceptron;nonlinear regression framework;optimal neural network architecture;real-time hand posture user interface;recurrent convolutional neural network;sparse dictionary learning;video sequence;vision based in-car entertainment user interface;Dictionaries;Feature extraction;Gesture recognition;Image sequences;Machine learning;Training;User interfaces},
}

@InProceedings{8014787,
  author    = {A. Gupta and S. Mohatta and J. Maurya and R. Perla and R. Hebbalaguppe and E. Hassan},
  title     = {Hand Gesture Based Region Marking for Tele-Support Using Wearables},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  year      = {2017},
  pages     = {386-392},
  month     = {July},
  abstract  = {Wearable Augmented Reality (AR) devices1 are being explored in many applications for visualizing real-time contextual information. More importantly, these devices can also be used in tele-assistance from remote sites when on-field operators require off-field expert's guidance for trouble-shooting. For an effective communication, touchless hand gestures are the most intuitive to select a Region Of Interest (ROI) like defective parts in a machine, through a wearable. This paper presents a hand gestural interaction method to localize the ROI in First Person View (FPV). The region selected using freehand sketching gestures is highlighted to the remote server setup for expert's advice. Novelty of the proposed method include (a) touchless fingerbased gesture recognition algorithm that runs on smartphones, which can be used with wearable frugal modality like Google Cardboard/Wearality, (b)reducing the network latency and achieving real-time performance by on-board implementation of recognition module. We conducted user studies that suggest the ease and usefulness of the proposed method. Further, we evaluated the the effectiveness of the ROI gesture using the PASCAL Visual Object Classes(VOC) criteria.},
  doi       = {10.1109/CVPRW.2017.53},
  keywords  = {Gesture recognition;Google;Image segmentation;Real-time systems;Sensors;Tracking;Trajectory},
}

@Book{9783319325507,
  title     = {Springer Handbook of Robotics (Springer Handbooks)},
  publisher = {Springer},
  year      = {2016},
  isbn      = {9783319325507},
  url       = {https://www.amazon.com/Springer-Handbook-Robotics-Handbooks-ebook/dp/B01J7C9XQ8?SubscriptionId=0JYN1NVW651KCA56C102&tag=techkie-20&linkCode=xm2&camp=2025&creative=165953&creativeASIN=B01J7C9XQ8},
}

@Article{6084753,
  author   = {Y. Otake and M. Armand and R. S. Armiger and M. D. Kutzer and E. Basafa and P. Kazanzides and R. H. Taylor},
  title    = {Intraoperative Image-based Multiview 2D/3D Registration for Image-Guided Orthopaedic Surgery: Incorporation of Fiducial-Based C-Arm Tracking and GPU-Acceleration},
  journal  = {IEEE Transactions on Medical Imaging},
  year     = {2012},
  volume   = {31},
  number   = {4},
  pages    = {948-962},
  month    = {April},
  issn     = {0278-0062},
  abstract = {Intraoperative patient registration may significantly affect the outcome of image-guided surgery (IGS). Image-based registration approaches have several advantages over the currently dominant point-based direct contact methods and are used in some industry solutions in image-guided radiation therapy with fixed X-ray gantries. However, technical challenges including geometric calibration and computational cost have precluded their use with mobile C-arms for IGS. We propose a 2D/3D registration framework for intraoperative patient registration using a conventional mobile X-ray imager combining fiducial-based C-arm tracking and graphics processing unit (GPU)-acceleration. The two-stage framework 1) acquires X-ray images and estimates relative pose between the images using a custom-made in-image fiducial, and 2) estimates the patient pose using intensity-based 2D/3D registration. Experimental validations using a publicly available gold standard dataset, a plastic bone phantom and cadaveric specimens have been conducted. The mean target registration error (mTRE) was 0.34±0.04 mm (success rate: 100%, registration time: 14.2 s) for the phantom with two images 90° apart, and 0.99±0.41 mm (81%, 16.3 s) for the cadaveric specimen with images 58.5° apart. The experimental results showed the feasibility of the proposed registration framework as a practical alternative for IGS routines.},
  doi      = {10.1109/TMI.2011.2176555},
  file     = {:ref_downloads/6084753 - Intraoperative Image-based Multiview 2D_3D Registration for Image-Guided Orthopaedic Surgery_ Incorporation of Fiducial-Based C-Arm Tracking and GPU-Acceleration.pdf:PDF},
  keywords = {bone;diagnostic radiography;graphics processing units;image registration;medical image processing;orthopaedics;phantoms;surgery;GPU-acceleration;IGS routines;cadaveric specimen;fiducial-based C-arm tracking;gold standard dataset;graphics processing unit-acceleration;image-guided orthopaedic surgery;intensity-based 2D-3D registration;intraoperative image-based multiview 2D-3D registration;intraoperative patient registration;mean target registration error;mobile X-ray imager;plastic bone phantom;Calibration;Computed tomography;Detectors;Graphics processing unit;Optimization;Three dimensional displays;X-ray imaging;C-arm pose tracking;GPU-acceleration;image-guided surgery;intraoperative 2D/3D registration;Algorithms;Femur;Humans;Imaging, Three-Dimensional;Orthopedic Procedures;Phantoms, Imaging;Reproducibility of Results;Surgery, Computer-Assisted;Tomography, X-Ray Computed},
}

@InProceedings{4649997,
  author    = {N. M. Hamming and M. J. Daly and J. C. Irish and J. H. Siewerdsen},
  title     = {Effect of fiducial configuration on target registration error in intraoperative cone-beam CT guidance of head and neck surgery},
  booktitle = {2008 30th Annual International Conference of the IEEE Engineering in Medicine and Biology Society},
  year      = {2008},
  pages     = {3643-3648},
  month     = {Aug},
  abstract  = {Advances in image-guided surgery have led to minimally-invasive, high-precision procedures that increase the efficacy of treatment, minimize surgical complications, and reduce patient recovery time. A recent advance in intraoperative 3D imaging includes cone-beam CT (CBCT) implemented on a mobile C-arm. This paper investigates the effect of the number and configuration of fiducials on target registration error (TRE) and identifies fiducial configurations that minimize TRE for rigid point-based registration in CBCT-guided head and neck surgery. Best configurations were those that minimized the distance between the centroid of fiducials and the surgical target while maximizing fiducial separation (distance from principal axes). Configurations with as few as 4 fiducials could be identified that minimized TRE (e.g., TRE < 0.3 mm for the pituitary, cochlea, and nasion), with more fiducials (6 or more) providing improved TRE uniformity throughout the volume of clinical interest. If possible, fiducials affixed to the skin or cranium (e.g., 4–6 markers) should include a majority about the target (to minimize centroid-to-target distance) with others at a distance (to maximize separation). A greater number of fiducials distributed evenly can provide low, uniform TRE for all targets - e.g., 8 markers, TRE ∼0.2–0.6 mm throughout the volume of interest. Such work helps guide the implementation of C-arm CBCT in head and neck surgery in a manner that maximizes surgical precision and exploits intraoperative image guidance to its full potential.},
  doi       = {10.1109/IEMBS.2008.4649997},
  file      = {:ref_downloads/4649997 - Effect of fiducial configuration on target registration error in intraoperative cone-beam CT guidance of head and neck surgery.pdf:PDF},
  issn      = {1094-687X},
  keywords  = {Computed tomography;Guidelines;Head;Image reconstruction;Imaging phantoms;Medical treatment;Navigation;Neck;Skin;Surgery;Algorithms;Brain;Cone-Beam Computed Tomography;Head and Neck Neoplasms;Humans;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Models, Statistical;Numerical Analysis, Computer-Assisted;Pattern Recognition, Automated;Phantoms, Imaging;Reproducibility of Results;Surgery, Computer-Assisted;Surgical Procedures, Operative},
}

@InProceedings{100000,
  author    = {J. K. Parker and A. R. Khoogar and D. E. Goldberg},
  title     = {Inverse kinematics of redundant robots using genetic algorithms},
  booktitle = {Proceedings, 1989 International Conference on Robotics and Automation},
  year      = {1989},
  pages     = {271-276 vol.1},
  month     = {May},
  abstract  = {Genetic algorithms, which are robust general-purpose optimization techniques, have been used to solve the inverse kinematics problem for redundant robots. A genetic algorithm (GA) was used to position a robot at a target location while minimizing the largest joint displacement from the initial position. As currently implemented, GAs are suitable for offline programming of a redundant robot in point-to-point positioning tasks. The GA solution needs only the forward kinematic equations (which are easily developed) and does not require any artificial constraints on the joint angles. The joint rotation limits which are present in any feasible robot design are handled directly; so any solution determined by the GA is physically realizable. Finally, the GA works with joint angles represented as digital values (not continuous real numbers), which is more representative for computer-controlled robot systems},
  doi       = {10.1109/ROBOT.1989.100000},
  keywords  = {inverse problems;kinematics;optimisation;redundancy;robots;forward kinematic equations;genetic algorithms;inverse kinematics problem;joint displacement minimisation;joint rotation limits;offline programming;point-to-point positioning tasks;redundant robots;robust general-purpose optimization techniques;Differential equations;Genetic algorithms;Jacobian matrices;Lagrangian functions;Mechanical engineering;Motion planning;Nonlinear equations;Robot kinematics;Service robots;Testing},
}

@Article{Wang2014,
  author   = {J. Wang and H. Suenaga and K. Hoshi and L. Yang and E. Kobayashi and I. Sakuma and H. Liao},
  title    = {Augmented Reality Navigation With Automatic Marker-Free Image Registration Using 3-D Image Overlay for Dental Surgery},
  journal  = {IEEE Transactions on Biomedical Engineering},
  year     = {2014},
  volume   = {61},
  number   = {4},
  pages    = {1295-1304},
  month    = {April},
  issn     = {0018-9294},
  abstract = {Computer-assisted oral and maxillofacial surgery (OMS) has been rapidly evolving since the last decade. State-of-the-art surgical navigation in OMS still suffers from bulky tracking sensors, troublesome image registration procedures, patient movement, loss of depth perception in visual guidance, and low navigation accuracy. We present an augmented reality navigation system with automatic marker-free image registration using 3-D image overlay and stereo tracking for dental surgery. A customized stereo camera is designed to track both the patient and instrument. Image registration is performed by patient tracking and real-time 3-D contour matching, without requiring any fiducial and reference markers. Real-time autostereoscopic 3-D imaging is implemented with the help of a consumer-level graphics processing unit. The resulting 3-D image of the patient's anatomy is overlaid on the surgical site by a half-silvered mirror using image registration and IP-camera registration to guide the surgeon by exposing hidden critical structures. The 3-D image of the surgical instrument is also overlaid over the real one for an augmented display. The 3-D images present both stereo and motion parallax from which depth perception can be obtained. Experiments were performed to evaluate various aspects of the system; the overall image overlay error of the proposed system was 0.71 mm.},
  doi      = {10.1109/TBME.2014.2301191},
  file     = {:ref_downloads/6716056 - Augmented Reality Navigation With Automatic Marker-Free Image Registration Using 3-D Image Overlay for Dental Surgery.pdf:PDF},
  keywords = {augmented reality;biomedical optical imaging;dentistry;graphics processing units;image matching;image registration;image sensors;medical image processing;stereo image processing;surgery;3D image overlay;IP-camera registration;augmented display;augmented reality navigation;automatic marker-free image registration;bulky tracking sensors;computer-assisted oral surgery;consumer-level graphics processing unit;customized stereo camera;dental surgery;depth perception;depth perception loss;half-silvered mirror;hidden critical structures;low navigation accuracy;maxillofacial surgery;motion parallax;overall image overlay error;patient anatomy;patient movement;patient tracking;real-time 3D contour matching;real-time autostereoscopic 3D imaging;state-of-the-art surgical navigation;stereo parallax;stereo tracking;surgical instrument;surgical site;troublesome image registration procedures;visual guidance;Cameras;Dentistry;IP networks;Instruments;Navigation;Surgery;Three-dimensional displays;3-D image overlay;Augmented reality (AR);dental surgery;integral photography (IP);stereo tracking;surgical navigation;Dentistry, Operative;Head;Humans;Imaging, Three-Dimensional;Models, Biological;Phantoms, Imaging;Photography, Dental;User-Computer Interface},
}

@Article{1634325,
  author   = {A. I. Comport and E. Marchand and M. Pressigout and F. Chaumette},
  title    = {Real-time markerless tracking for augmented reality: the virtual visual servoing framework},
  journal  = {IEEE Transactions on Visualization and Computer Graphics},
  year     = {2006},
  volume   = {12},
  number   = {4},
  pages    = {615-628},
  month    = {July},
  issn     = {1077-2626},
  abstract = {Tracking is a very important research subject in a real-time augmented reality context. The main requirements for trackers are high accuracy and little latency at a reasonable cost. In order to address these issues, a real-time, robust, and efficient 3D model-based tracking algorithm is proposed for a "video see through" monocular vision system. The tracking of objects in the scene amounts to calculating the pose between the camera and the objects. Virtual objects can then be projected into the scene using the pose. In this paper, nonlinear pose estimation is formulated by means of a virtual visual servoing approach. In this context, the derivation of point-to-curves interaction matrices are given for different 3D geometrical primitives including straight lines, circles, cylinders, and spheres. A local moving edges tracker is used in order to provide real-time tracking of points normal to the object contours. Robustness is obtained by integrating an M-estimator into the visual control law via an iteratively reweighted least squares implementation. This approach is then extended to address the 3D model-free augmented reality problem. The method presented in this paper has been validated on several complex image sequences including outdoor environments. Results show the method to be robust to occlusion, changes in illumination, and mistracking.},
  doi      = {10.1109/TVCG.2006.78},
  file     = {:ref_downloads/1634325 - Real-time markerless tracking for augmented reality_ the virtual visual servoing framework.pdf:PDF},
  keywords = {augmented reality;computational geometry;image motion analysis;image sequences;least squares approximations;object detection;tracking;3D geometrical primitives;3D model-based tracking algorithm;3D model-free augmented reality;M-estimator;image sequences;monocular vision system;point-to-curves interaction matrices;real-time markerless tracking;virtual objects;virtual visual servoing framework;visual control law;Augmented reality;Cameras;Costs;Delay;Layout;Machine vision;Real time systems;Robust control;Robustness;Visual servoing;Augmented reality;model-based tracking;model-free tracking.;real-time;robust estimators;virtual visual servoing;Algorithms;Computer Graphics;Computer Systems;Feedback;Image Enhancement;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Information Storage and Retrieval;Signal Processing, Computer-Assisted;User-Computer Interface},
}

@Article{7805258,
  author   = {N. Ahmidi and L. Tao and S. Sefati and Y. Gao and C. Lea and B. B. Haro and L. Zappella and S. Khudanpur and R. Vidal and G. D. Hager},
  title    = {A Dataset and Benchmarks for Segmentation and Recognition of Gestures in Robotic Surgery},
  journal  = {IEEE Transactions on Biomedical Engineering},
  year     = {2017},
  volume   = {64},
  number   = {9},
  pages    = {2025-2041},
  month    = {Sept},
  issn     = {0018-9294},
  abstract = {Objective: State-of-the-art techniques for surgical data analysis report promising results for automated skill assessment and action recognition. The contributions of many of these techniques, however, are limited to study-specific data and validation metrics, making assessment of progress across the field extremely challenging. Methods: In this paper, we address two major problems for surgical data analysis: First, lack of uniform-shared datasets and benchmarks, and second, lack of consistent validation processes. We address the former by presenting the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS), a public dataset that we have created to support comparative research benchmarking. JIGSAWS contains synchronized video and kinematic data from multiple performances of robotic surgical tasks by operators of varying skill. We address the latter by presenting a well-documented evaluation methodology and reporting results for six techniques for automated segmentation and classification of time-series data on JIGSAWS. These techniques comprise four temporal approaches for joint segmentation and classification: hidden Markov model, sparse hidden Markov model (HMM), Markov semi-Markov conditional random field, and skip-chain conditional random field; and two feature-based ones that aim to classify fixed segments: bag of spatiotemporal features and linear dynamical systems. Results: Most methods recognize gesture activities with approximately 80% overall accuracy under both leave-one-super-trial-out and leave-one-user-out cross-validation settings. Conclusion: Current methods show promising results on this shared dataset, but room for significant progress remains, particularly for consistent prediction of gesture activities across different surgeons. Significance: The results reported in this paper provide the first systematic and uniform evaluatio- of surgical activity recognition techniques on the benchmark database.},
  doi      = {10.1109/TBME.2016.2647680},
  file     = {:ref_downloads/7805258 - A Dataset and Benchmarks for Segmentation and Recognition of Gestures in Robotic Surgery.pdf:PDF},
  keywords = {Benchmark testing;Hidden Markov models;Kinematics;Measurement;Needles;Robots;Surgery;Activity recognition;benchmark robotic dataset;kinematics and video;surgical motion},
}

@Article{7208833,
  author   = {H. Cheng and L. Yang and Z. Liu},
  title    = {Survey on 3D Hand Gesture Recognition},
  journal  = {IEEE Transactions on Circuits and Systems for Video Technology},
  year     = {2016},
  volume   = {26},
  number   = {9},
  pages    = {1659-1673},
  month    = {Sept},
  issn     = {1051-8215},
  abstract = {Three-dimensional hand gesture recognition has attracted increasing research interests in computer vision, pattern recognition, and human-computer interaction. The emerging depth sensors greatly inspired various hand gesture recognition approaches and applications, which were severely limited in the 2D domain with conventional cameras. This paper presents a survey of some recent works on hand gesture recognition using 3D depth sensors. We first review the commercial depth sensors and public data sets that are widely used in this field. Then, we review the state-of-the-art research for 3D hand gesture recognition in four aspects: 1) 3D hand modeling; 2) static hand gesture recognition; 3) hand trajectory gesture recognition; and 4) continuous hand gesture recognition. While the emphasis is on 3D hand gesture recognition approaches, the related applications and typical systems are also briefly summarized for practitioners.},
  doi      = {10.1109/TCSVT.2015.2469551},
  file     = {:7208833 - Survey on 3D Hand Gesture Recognition.pdf:PDF},
  keywords = {gesture recognition;image sensors;solid modelling;3D depth sensor;3D hand gesture recognition;3D hand modeling;computer vision;continuous hand gesture recognition;hand trajectory gesture recognition;human-computer interaction;pattern recognition;static hand gesture recognition;Assistive technology;Cameras;Gesture recognition;Sensors;Solid modeling;Three-dimensional displays;Trajectory;Begin-end gesture detection;depth sensor;dynamic time warping;hand gesture recognition;skeleton detection and tracking},
}

@Article{7509645,
  author   = {W. Lu and Z. Tong and J. Chu},
  title    = {Dynamic Hand Gesture Recognition With Leap Motion Controller},
  journal  = {IEEE Signal Processing Letters},
  year     = {2016},
  volume   = {23},
  number   = {9},
  pages    = {1188-1192},
  month    = {Sept},
  issn     = {1070-9908},
  abstract = {Dynamic hand gesture recognition is a crucial but challenging task in the pattern recognition and computer vision communities. In this paper, we propose a novel feature vector which is suitable for representing dynamic hand gestures, and presents a satisfactory solution to recognizing dynamic hand gestures with a Leap Motion controller (LMC) only. These have not been reported in other papers. The feature vector with depth information is computed and fed into the Hidden Conditional Neural Field (HCNF) classifier to recognize dynamic hand gestures. The systematic framework of the proposed method includes two main steps: feature extraction and classification with the HCNF classifier. The proposed method is evaluated on two dynamic hand gesture datasets with frames acquired with a LMC. The recognition accuracy is 89.5% for the LeapMotion-Gesture3D dataset and 95.0% for the Handicraft-Gesture dataset. Experimental results show that the proposed method is suitable for certain dynamic hand gesture recognition tasks.},
  doi      = {10.1109/LSP.2016.2590470},
  keywords = {computer vision;feature extraction;gesture recognition;image classification;image representation;interactive devices;neural nets;HCNF classifier;Handicraft-Gesture dataset;LMC;LeapMotion-Gesture3D dataset;computer vision;depth information;dynamic hand gesture recognition;feature classification;feature extraction;feature vector;gesture representation;hidden conditional neural field;leap motion controller;pattern recognition;Dynamics;Feature extraction;Gesture recognition;Hidden Markov models;Logic gates;Systematics;Training;Depth data;dynamic hand gesture recognition;hidden conditional neural field (HCNF);leap motion controller (LMC)},
}

@InBook{Ruffieux2014,
  pages     = {337--348},
  title     = {A Survey of Datasets for Human Gesture Recognition},
  publisher = {Springer International Publishing},
  year      = {2014},
  author    = {Ruffieux, Simon and Lalanne, Denis and Mugellini, Elena and Abou Khaled, Omar},
  editor    = {Kurosu, Masaaki},
  address   = {Cham},
  isbn      = {978-3-319-07230-2},
  abstract  = {This paper presents a survey on datasets created for the field of gesture recognition. The main characteristics of the datasets are presented on two tables to provide researchers a clear and rapid access to the information. This paper also provides a comprehensive description of the datasets and discusses their general strengths and limitations. Guidelines for creation and selection of datasets for gesture recognition are proposed. This survey should be a key-access point for researchers looking to create or use datasets in the field of human gesture recognition.},
  booktitle = {Human-Computer Interaction. Advanced Interaction Modalities and Techniques: 16th International Conference, HCI International 2014, Heraklion, Crete, Greece, June 22-27, 2014, Proceedings, Part II},
  doi       = {10.1007/978-3-319-07230-2_33},
  url       = {https://doi.org/10.1007/978-3-319-07230-2_33},
}

@InProceedings{6126483,
  author    = {I. Oikonomidis and N. Kyriazis and A. A. Argyros},
  title     = {Full DOF tracking of a hand interacting with an object by modeling occlusions and physical constraints},
  booktitle = {2011 International Conference on Computer Vision},
  year      = {2011},
  pages     = {2088-2095},
  month     = {Nov},
  abstract  = {Due to occlusions, the estimation of the full pose of a human hand interacting with an object is much more challenging than pose recovery of a hand observed in isolation. In this work we formulate an optimization problem whose solution is the 26-DOF hand pose together with the pose and model parameters of the manipulated object. Optimization seeks for the joint hand-object model that (a) best explains the incompleteness of observations resulting from occlusions due to hand-object interaction and (b) is physically plausible in the sense that the hand does not share the same physical space with the object. The proposed method is the first that solves efficiently the continuous, full-DOF, joint hand-object tracking problem based solely on markerless multicamera input. Additionally, it is the first to demonstrate how hand-object interaction can be exploited as a context that facilitates hand pose estimation, instead of being considered as a complicating factor. Extensive quantitative and qualitative experiments with simulated and real world image sequences as well as a comparative evaluation with a state-of-the-art method for pose estimation of isolated hands, support the above findings.},
  doi       = {10.1109/ICCV.2011.6126483},
  issn      = {1550-5499},
  keywords  = {object tracking;optimisation;pose estimation;solid modelling;full DOF tracking;hand pose estimation;hand-object interaction;hand-object tracking;occlusions modeling;optimization;physical constraints modeling;Cameras},
}

@InProceedings{6460664,
  author    = {A. D. Bagdanov and A. Del Bimbo and L. Seidenari and L. Usai},
  title     = {Real-time hand status recognition from RGB-D imagery},
  booktitle = {Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)},
  year      = {2012},
  pages     = {2456-2459},
  month     = {Nov},
  abstract  = {One of the most critical limitations of KinectTM-based interfaces is the need for persistence in order to interact with virtual objects. Indeed, a user must keep her arm still for a not-so-short span of time while pointing at an object with which she wishes to interact. The most natural way to overcome this limitation and improve interface reactivity is to employ a vision module able to recognize simple hand poses (e.g. open/closed) in order to add a state to the virtual pointer represented by the user hand. In this paper we propose a method to robustly predict the status of the user hand in real-time. We jointly exploit depth and RGB imagery to produce a robust feature for hand representation. Finally, we use temporal filtering to reduce spurious prediction errors. We have also prepared a dataset of more than 30K depth-RGB image pairs of hands that is being made publicly available. The proposed method achieves more than 98% accuracy and is highly responsive.},
  issn      = {1051-4651},
  keywords  = {computer vision;filtering theory;image colour analysis;image representation;palmprint recognition;pose estimation;virtual reality;KinectTM-based interface;RGB-D imagery;depth-RGB image;hand status recognition;robust feature;temporal filtering;user hand representation;virtual object interaction;virtual pointer representation;vision module;Accuracy;Image recognition;Image segmentation;Real-time systems;Shape;Smoothing methods;Support vector machines},
}

@Article{Marin2016,
  author   = {Marin, Giulio and Dominio, Fabio and Zanuttigh, Pietro},
  title    = {Hand gesture recognition with jointly calibrated Leap Motion and depth sensor},
  journal  = {Multimedia Tools and Applications},
  year     = {2016},
  volume   = {75},
  number   = {22},
  pages    = {14991--15015},
  month    = {Nov},
  issn     = {1573-7721},
  abstract = {Novel 3D acquisition devices like depth cameras and the Leap Motion have recently reached the market. Depth cameras allow to obtain a complete 3D description of the framed scene while the Leap Motion sensor is a device explicitly targeted for hand gesture recognition and provides only a limited set of relevant points. This paper shows how to jointly exploit the two types of sensors for accurate gesture recognition. An ad-hoc solution for the joint calibration of the two devices is firstly presented. Then a set of novel feature descriptors is introduced both for the Leap Motion and for depth data. Various schemes based on the distances of the hand samples from the centroid, on the curvature of the hand contour and on the convex hull of the hand shape are employed and the use of Leap Motion data to aid feature extraction is also considered. The proposed feature sets are fed to two different classifiers, one based on multi-class SVMs and one exploiting Random Forests. Different feature selection algorithms have also been tested in order to reduce the complexity of the approach. Experimental results show that a very high accuracy can be obtained from the proposed method. The current implementation is also able to run in real-time.},
  day      = {01},
  doi      = {10.1007/s11042-015-2451-6},
  url      = {https://doi.org/10.1007/s11042-015-2451-6},
}

@InProceedings{6333871,
  author    = {A. Kurakin and Z. Zhang and Z. Liu},
  title     = {A real time system for dynamic hand gesture recognition with a depth sensor},
  booktitle = {2012 Proceedings of the 20th European Signal Processing Conference (EUSIPCO)},
  year      = {2012},
  pages     = {1975-1979},
  month     = {Aug},
  abstract  = {Recent advances in depth sensing provide exciting opportunities for the development of new methods for human activity understanding. Yet, little work has been done in the area of hand gesture recognition which has many practical applications. In this paper we propose a real-time system for dynamic hand gesture recognition. It is fully automatic and robust to variations in speed and style as well as in hand orientations. Our approach is based on action graph, which shares similar robust properties with standard HMM but requires less training data by allowing states shared among different gestures. To deal with hand orientations, we have developed a new technique for hand segmentation and orientation normalization. The proposed system is evaluated on a challenging dataset of twelve dynamic American Sign Language (ASL) gestures.},
  issn      = {2219-5491},
  keywords  = {gesture recognition;palmprint recognition;sensors;ASL gesture;depth sensor;dynamic american sign language gesture;dynamic hand gesture recognition;hand orientation;hand segmentation;human activity understanding;real-time system;standard HMM;Cameras;Feature extraction;Gesture recognition;Hidden Markov models;Image segmentation;Maximum likelihood decoding;Shape;Gesture recognition;depth camera},
}

@Article{BetterThanYouThink,
  author     = {Plaumann, Katrin and Ehlers, Jan and Geiselhart, Florian and Yuras, Gabriel and Huckauf, Anke and Rukzio, Enrico},
  title      = {Better than you Think: Head Gestures for Mid Air Input},
  journal    = {In Proc. of Interact 2015 (IFIP TC15 Conference on Human-Computer Interaction)},
  year       = {2015},
  month      = {9},
  doi        = {10.1007/978-3-319-22698-9_36},
  event_name = {INTERACT 2015},
  extern     = {1},
  in_library = {1},
  publisher  = {Springer},
  reviewed   = {1},
  url        = {www.uni-ulm.de/fileadmin/website\_uni\_ulm/iui.inst.100/institut/Papers/Prof\_Rukzio/2015/InteractPaper.pdf},
}

@Article{4154947,
  author   = {S. Mitra and T. Acharya},
  title    = {Gesture Recognition: A Survey},
  journal  = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  year     = {2007},
  volume   = {37},
  number   = {3},
  pages    = {311-324},
  month    = {May},
  issn     = {1094-6977},
  abstract = {Gesture recognition pertains to recognizing meaningful expressions of motion by a human, involving the hands, arms, face, head, and/or body. It is of utmost importance in designing an intelligent and efficient human-computer interface. The applications of gesture recognition are manifold, ranging from sign language through medical rehabilitation to virtual reality. In this paper, we provide a survey on gesture recognition with particular emphasis on hand gestures and facial expressions. Applications involving hidden Markov models, particle filtering and condensation, finite-state machines, optical flow, skin color, and connectionist models are discussed in detail. Existing challenges and future research possibilities are also highlighted},
  doi      = {10.1109/TSMCC.2007.893280},
  keywords = {face recognition;finite state machines;gesture recognition;hidden Markov models;human computer interaction;image colour analysis;image sequences;condensation;connectionist models;facial expressions;finite-state machines;gesture recognition;hand gestures;hidden Markov models;intelligent human-computer interface;optical flow;particle filtering;skin color;Arm;Face recognition;Filtering;Handicapped aids;Hidden Markov models;Humans;Magnetic heads;Manifolds;Optical filters;Virtual reality;Face recognition;facial expressions;hand gestures;hidden Markov models (HMMs);optical flow;soft computing},
}

@InProceedings{7226047,
  author    = {Yashaswi Alva M and Nachamai M and J. Paulose},
  title     = {A comprehensive survey on features and methods for speech emotion detection},
  booktitle = {2015 IEEE International Conference on Electrical, Computer and Communication Technologies (ICECCT)},
  year      = {2015},
  pages     = {1-6},
  month     = {March},
  abstract  = {Human computer interaction will be natural and effective when the interfaces are sensitive to human emotion or stress. Previous studies were mainly focused on facial emotion recognition but speech emotion detection is gaining importance due its wide range of applications. Speech emotion recognition still remains a challenging task in the field of affective computing as no defined standards exist for emotion classification. Speech signal carries large information related to the emotions conveyed by a person. Speech recognition system fails miserably if robust techniques are not implemented to address the variations in speech due to emotion. Emotion detection from speech has two main steps. They are feature extraction and classification. The goal of this paper is to give an overview on the types of corpus, features and classification techniques that are associated with speech emotion recognition.},
  doi       = {10.1109/ICECCT.2015.7226047},
  keywords  = {emotion recognition;feature extraction;human computer interaction;speech recognition;emotion classification;facial emotion recognition;feature classification technique;feature extraction;human computer interaction;human emotion;speech emotion detection;speech recognition system;speech signal;Emotion recognition;Hidden Markov models;Mel frequency cepstral coefficient;Silicon;Speech;Speech recognition;Support vector machines;Emotion recognition;classification methods;speech corpus;speech features},
}

@Article{7737032,
  author   = {K. Zinchenko and C. Y. Wu and K. T. Song},
  title    = {A Study on Speech Recognition Control for a Surgical Robot},
  journal  = {IEEE Transactions on Industrial Informatics},
  year     = {2017},
  volume   = {13},
  number   = {2},
  pages    = {607-615},
  month    = {April},
  issn     = {1551-3203},
  abstract = {Speech recognition is common in electronic appliances and personal services, but its use for industrial and medical purposes is rare because of the presence of motion ambiguity. For minimally invasive surgical robotic assistants, this ambiguity arises because the robotic motion is not calibrated to the camera images. This paper presents a design for a speech recognition interface for an HIWIN robotic endoscope holder. A new intentional speech control is proposed to control movement over long distances. To decrease ambiguity, a method is proposed for voice-to-motion calibration that compares the degree of change in the endoscope image for a voice command. A speech recognition algorithm is implemented on Ubuntu OS, using CMU Sphinx. The control signal is sent to the robot controller using serial-port communication through a RS232 cable. The experimental results show that the proposed intentional speech control strategy has a navigation precision of up to 3.1° of angular displacement for the endoscope. The overall system processing time, including robotic motion, is 3.22 s for ~1.8-s speech duration. The reference image navigation range is from 2.5 mm for ~0.5-s speech duration up to 6 mm for ~1.8-s speech duration, using a setup with camera tip that is located at a distance of 5 cm from the remote center of motion point.},
  doi      = {10.1109/TII.2016.2625818},
  keywords = {control engineering computing;endoscopes;human-robot interaction;medical computing;medical robotics;speech recognition;speech-based user interfaces;surgery;telerobotics;CMU Sphinx;HIWIN robotic endoscope holder;RS232 cable;Ubuntu OS;intentional speech control;minimally invasive surgical robotic assistants;motion ambiguity;reference image navigation range;serial-port communication;speech recognition control;speech recognition interface;voice command;voice-to-motion calibration;Cameras;Endoscopes;Robot vision systems;Service robots;Speech;Speech recognition;Automated system;human–robot interface;motion control;robotic surgery;speech recognition control},
}

@InProceedings{7554323,
  author    = {X. Liu and S. S. Ge and R. Jiang and C. H. Goh},
  title     = {Intelligent speech control system for human-robot interaction},
  booktitle = {2016 35th Chinese Control Conference (CCC)},
  year      = {2016},
  pages     = {6154-6159},
  month     = {July},
  abstract  = {Accurately extracting subjective contents of speech signals and applying it on controlling robots remain to this day a challenging task as well as an insistent demand in human-robot interaction (HRI). A simple classification of human's intentions may limit the development of robots' natural reactions to users. Additionally, there should be a control system that can understand and translate human's intentions into control inputs. This paper proposes an intelligent speech control system for HRI. The objective is to understand human' s speech commands via recognizing, quantifying audio signals and translating speech inputs into control inputs. Aiming at this purpose, three main parts for the system are designed: a speech recognition system, a speech measurement system and a control system. Specifically, Mel Frequency Cepstral Coefficient (MFCC) and Dynamic Time Warping (DTW) techniques are utilized to recognize isolate speech commands. An energy-based feature and novelly proposed spectrum-based features are introduced to represent subjective contents of speech signals followed by Random Forest (RF) as a regressor. Several control schemes are utilized to translate quantified speech signals into control inputs. Simulation results illustrate the performance of the proposed system and the robot adaptive control system outperforms other control methods on effectiveness and controllability. The improved spectrum-based features demonstrate the capacity to extract subjective information of signals.},
  doi       = {10.1109/ChiCC.2016.7554323},
  keywords  = {adaptive control;human-robot interaction;regression analysis;speech recognition;DTW;HRI;MFCC;RF;audio signals;control inputs;dynamic time warping techniques;energy-based feature;human intention classification;human speech commands;human-robot interaction;intelligent speech control system;isolate speech command recognition;mel frequency cepstral coefficient;random forest;robot adaptive control system;robot natural reaction development;robots controlling;signal subjective information extraction;spectrum-based features demonstrate;speech measurement system;speech recognition system;speech signals;Control systems;Feature extraction;Mel frequency cepstral coefficient;Robots;Speech;Speech recognition;Training},
}

@Comment{jabref-meta: databaseType:bibtex;}
